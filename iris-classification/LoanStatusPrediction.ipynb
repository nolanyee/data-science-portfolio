{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loan Status Prediction\n",
    "\n",
    "The purpose of this project is to predict whether a loan will be approved based on the demographics of the person requesting the loan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'klaR' is in use and will not be installed\"Warning message:\n",
      "\"package 'kknn' is in use and will not be installed\"Warning message:\n",
      "\"package 'gbm' is in use and will not be installed\""
     ]
    }
   ],
   "source": [
    "library(tidyverse)\n",
    "library(ggplot2)\n",
    "library(caret)\n",
    "library(e1071)\n",
    "\n",
    "options(repos='https://cran.cnr.berkeley.edu/')\n",
    "install.packages('klaR')\n",
    "install.packages('kknn')\n",
    "install.packages('gbm')\n",
    "library(klaR)\n",
    "library(kknn)\n",
    "library(gbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Loan_ID</th><th scope=col>Gender</th><th scope=col>Married</th><th scope=col>Dependents</th><th scope=col>Education</th><th scope=col>Self_Employed</th><th scope=col>ApplicantIncome</th><th scope=col>CoapplicantIncome</th><th scope=col>LoanAmount</th><th scope=col>Loan_Amount_Term</th><th scope=col>Credit_History</th><th scope=col>Property_Area</th><th scope=col>Loan_Status</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>LP001002    </td><td>Male        </td><td>No          </td><td>0           </td><td>Graduate    </td><td>No          </td><td> 5849       </td><td>    0       </td><td> NA         </td><td>360         </td><td> 1          </td><td>Urban       </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP001003    </td><td>Male        </td><td>Yes         </td><td>1           </td><td>Graduate    </td><td>No          </td><td> 4583       </td><td> 1508       </td><td>128         </td><td>360         </td><td> 1          </td><td>Rural       </td><td>N           </td></tr>\n",
       "\t<tr><td>LP001005    </td><td>Male        </td><td>Yes         </td><td>0           </td><td>Graduate    </td><td>Yes         </td><td> 3000       </td><td>    0       </td><td> 66         </td><td>360         </td><td> 1          </td><td>Urban       </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP001006    </td><td>Male        </td><td>Yes         </td><td>0           </td><td>Not Graduate</td><td>No          </td><td> 2583       </td><td> 2358       </td><td>120         </td><td>360         </td><td> 1          </td><td>Urban       </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP001008    </td><td>Male        </td><td>No          </td><td>0           </td><td>Graduate    </td><td>No          </td><td> 6000       </td><td>    0       </td><td>141         </td><td>360         </td><td> 1          </td><td>Urban       </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP001011    </td><td>Male        </td><td>Yes         </td><td>2           </td><td>Graduate    </td><td>Yes         </td><td> 5417       </td><td> 4196       </td><td>267         </td><td>360         </td><td> 1          </td><td>Urban       </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP001013    </td><td>Male        </td><td>Yes         </td><td>0           </td><td>Not Graduate</td><td>No          </td><td> 2333       </td><td> 1516       </td><td> 95         </td><td>360         </td><td> 1          </td><td>Urban       </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP001014    </td><td>Male        </td><td>Yes         </td><td>3+          </td><td>Graduate    </td><td>No          </td><td> 3036       </td><td> 2504       </td><td>158         </td><td>360         </td><td> 0          </td><td>Semiurban   </td><td>N           </td></tr>\n",
       "\t<tr><td>LP001018    </td><td>Male        </td><td>Yes         </td><td>2           </td><td>Graduate    </td><td>No          </td><td> 4006       </td><td> 1526       </td><td>168         </td><td>360         </td><td> 1          </td><td>Urban       </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP001020    </td><td>Male        </td><td>Yes         </td><td>1           </td><td>Graduate    </td><td>No          </td><td>12841       </td><td>10968       </td><td>349         </td><td>360         </td><td> 1          </td><td>Semiurban   </td><td>N           </td></tr>\n",
       "\t<tr><td>LP001024    </td><td>Male        </td><td>Yes         </td><td>2           </td><td>Graduate    </td><td>No          </td><td> 3200       </td><td>  700       </td><td> 70         </td><td>360         </td><td> 1          </td><td>Urban       </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP001027    </td><td>Male        </td><td>Yes         </td><td>2           </td><td>Graduate    </td><td>NA          </td><td> 2500       </td><td> 1840       </td><td>109         </td><td>360         </td><td> 1          </td><td>Urban       </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP001028    </td><td>Male        </td><td>Yes         </td><td>2           </td><td>Graduate    </td><td>No          </td><td> 3073       </td><td> 8106       </td><td>200         </td><td>360         </td><td> 1          </td><td>Urban       </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP001029    </td><td>Male        </td><td>No          </td><td>0           </td><td>Graduate    </td><td>No          </td><td> 1853       </td><td> 2840       </td><td>114         </td><td>360         </td><td> 1          </td><td>Rural       </td><td>N           </td></tr>\n",
       "\t<tr><td>LP001030    </td><td>Male        </td><td>Yes         </td><td>2           </td><td>Graduate    </td><td>No          </td><td> 1299       </td><td> 1086       </td><td> 17         </td><td>120         </td><td> 1          </td><td>Urban       </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP001032    </td><td>Male        </td><td>No          </td><td>0           </td><td>Graduate    </td><td>No          </td><td> 4950       </td><td>    0       </td><td>125         </td><td>360         </td><td> 1          </td><td>Urban       </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP001034    </td><td>Male        </td><td>No          </td><td>1           </td><td>Not Graduate</td><td>No          </td><td> 3596       </td><td>    0       </td><td>100         </td><td>240         </td><td>NA          </td><td>Urban       </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP001036    </td><td>Female      </td><td>No          </td><td>0           </td><td>Graduate    </td><td>No          </td><td> 3510       </td><td>    0       </td><td> 76         </td><td>360         </td><td> 0          </td><td>Urban       </td><td>N           </td></tr>\n",
       "\t<tr><td>LP001038    </td><td>Male        </td><td>Yes         </td><td>0           </td><td>Not Graduate</td><td>No          </td><td> 4887       </td><td>    0       </td><td>133         </td><td>360         </td><td> 1          </td><td>Rural       </td><td>N           </td></tr>\n",
       "\t<tr><td>LP001041    </td><td>Male        </td><td>Yes         </td><td>0           </td><td>Graduate    </td><td>NA          </td><td> 2600       </td><td> 3500       </td><td>115         </td><td> NA         </td><td> 1          </td><td>Urban       </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP001043    </td><td>Male        </td><td>Yes         </td><td>0           </td><td>Not Graduate</td><td>No          </td><td> 7660       </td><td>    0       </td><td>104         </td><td>360         </td><td> 0          </td><td>Urban       </td><td>N           </td></tr>\n",
       "\t<tr><td>LP001046    </td><td>Male        </td><td>Yes         </td><td>1           </td><td>Graduate    </td><td>No          </td><td> 5955       </td><td> 5625       </td><td>315         </td><td>360         </td><td> 1          </td><td>Urban       </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP001047    </td><td>Male        </td><td>Yes         </td><td>0           </td><td>Not Graduate</td><td>No          </td><td> 2600       </td><td> 1911       </td><td>116         </td><td>360         </td><td> 0          </td><td>Semiurban   </td><td>N           </td></tr>\n",
       "\t<tr><td>LP001050    </td><td>NA          </td><td>Yes         </td><td>2           </td><td>Not Graduate</td><td>No          </td><td> 3365       </td><td> 1917       </td><td>112         </td><td>360         </td><td> 0          </td><td>Rural       </td><td>N           </td></tr>\n",
       "\t<tr><td>LP001052    </td><td>Male        </td><td>Yes         </td><td>1           </td><td>Graduate    </td><td>NA          </td><td> 3717       </td><td> 2925       </td><td>151         </td><td>360         </td><td>NA          </td><td>Semiurban   </td><td>N           </td></tr>\n",
       "\t<tr><td>LP001066    </td><td>Male        </td><td>Yes         </td><td>0           </td><td>Graduate    </td><td>Yes         </td><td> 9560       </td><td>    0       </td><td>191         </td><td>360         </td><td> 1          </td><td>Semiurban   </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP001068    </td><td>Male        </td><td>Yes         </td><td>0           </td><td>Graduate    </td><td>No          </td><td> 2799       </td><td> 2253       </td><td>122         </td><td>360         </td><td> 1          </td><td>Semiurban   </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP001073    </td><td>Male        </td><td>Yes         </td><td>2           </td><td>Not Graduate</td><td>No          </td><td> 4226       </td><td> 1040       </td><td>110         </td><td>360         </td><td> 1          </td><td>Urban       </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP001086    </td><td>Male        </td><td>No          </td><td>0           </td><td>Not Graduate</td><td>No          </td><td> 1442       </td><td>    0       </td><td> 35         </td><td>360         </td><td> 1          </td><td>Urban       </td><td>N           </td></tr>\n",
       "\t<tr><td>LP001087    </td><td>Female      </td><td>No          </td><td>2           </td><td>Graduate    </td><td>NA          </td><td> 3750       </td><td> 2083       </td><td>120         </td><td>360         </td><td> 1          </td><td>Semiurban   </td><td>Y           </td></tr>\n",
       "\t<tr><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr>\n",
       "\t<tr><td>LP002911    </td><td>Male        </td><td>Yes         </td><td>1           </td><td>Graduate    </td><td>No          </td><td> 2787       </td><td> 1917       </td><td>146         </td><td>360         </td><td> 0          </td><td>Rural       </td><td>N           </td></tr>\n",
       "\t<tr><td>LP002912    </td><td>Male        </td><td>Yes         </td><td>1           </td><td>Graduate    </td><td>No          </td><td> 4283       </td><td> 3000       </td><td>172         </td><td> 84         </td><td> 1          </td><td>Rural       </td><td>N           </td></tr>\n",
       "\t<tr><td>LP002916    </td><td>Male        </td><td>Yes         </td><td>0           </td><td>Graduate    </td><td>No          </td><td> 2297       </td><td> 1522       </td><td>104         </td><td>360         </td><td> 1          </td><td>Urban       </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP002917    </td><td>Female      </td><td>No          </td><td>0           </td><td>Not Graduate</td><td>No          </td><td> 2165       </td><td>    0       </td><td> 70         </td><td>360         </td><td> 1          </td><td>Semiurban   </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP002925    </td><td>NA          </td><td>No          </td><td>0           </td><td>Graduate    </td><td>No          </td><td> 4750       </td><td>    0       </td><td> 94         </td><td>360         </td><td> 1          </td><td>Semiurban   </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP002926    </td><td>Male        </td><td>Yes         </td><td>2           </td><td>Graduate    </td><td>Yes         </td><td> 2726       </td><td>    0       </td><td>106         </td><td>360         </td><td> 0          </td><td>Semiurban   </td><td>N           </td></tr>\n",
       "\t<tr><td>LP002928    </td><td>Male        </td><td>Yes         </td><td>0           </td><td>Graduate    </td><td>No          </td><td> 3000       </td><td> 3416       </td><td> 56         </td><td>180         </td><td> 1          </td><td>Semiurban   </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP002931    </td><td>Male        </td><td>Yes         </td><td>2           </td><td>Graduate    </td><td>Yes         </td><td> 6000       </td><td>    0       </td><td>205         </td><td>240         </td><td> 1          </td><td>Semiurban   </td><td>N           </td></tr>\n",
       "\t<tr><td>LP002933    </td><td>NA          </td><td>No          </td><td>3+          </td><td>Graduate    </td><td>Yes         </td><td> 9357       </td><td>    0       </td><td>292         </td><td>360         </td><td> 1          </td><td>Semiurban   </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP002936    </td><td>Male        </td><td>Yes         </td><td>0           </td><td>Graduate    </td><td>No          </td><td> 3859       </td><td> 3300       </td><td>142         </td><td>180         </td><td> 1          </td><td>Rural       </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP002938    </td><td>Male        </td><td>Yes         </td><td>0           </td><td>Graduate    </td><td>Yes         </td><td>16120       </td><td>    0       </td><td>260         </td><td>360         </td><td> 1          </td><td>Urban       </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP002940    </td><td>Male        </td><td>No          </td><td>0           </td><td>Not Graduate</td><td>No          </td><td> 3833       </td><td>    0       </td><td>110         </td><td>360         </td><td> 1          </td><td>Rural       </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP002941    </td><td>Male        </td><td>Yes         </td><td>2           </td><td>Not Graduate</td><td>Yes         </td><td> 6383       </td><td> 1000       </td><td>187         </td><td>360         </td><td> 1          </td><td>Rural       </td><td>N           </td></tr>\n",
       "\t<tr><td>LP002943    </td><td>Male        </td><td>No          </td><td>NA          </td><td>Graduate    </td><td>No          </td><td> 2987       </td><td>    0       </td><td> 88         </td><td>360         </td><td> 0          </td><td>Semiurban   </td><td>N           </td></tr>\n",
       "\t<tr><td>LP002945    </td><td>Male        </td><td>Yes         </td><td>0           </td><td>Graduate    </td><td>Yes         </td><td> 9963       </td><td>    0       </td><td>180         </td><td>360         </td><td> 1          </td><td>Rural       </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP002948    </td><td>Male        </td><td>Yes         </td><td>2           </td><td>Graduate    </td><td>No          </td><td> 5780       </td><td>    0       </td><td>192         </td><td>360         </td><td> 1          </td><td>Urban       </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP002949    </td><td>Female      </td><td>No          </td><td>3+          </td><td>Graduate    </td><td>NA          </td><td>  416       </td><td>41667       </td><td>350         </td><td>180         </td><td>NA          </td><td>Urban       </td><td>N           </td></tr>\n",
       "\t<tr><td>LP002950    </td><td>Male        </td><td>Yes         </td><td>0           </td><td>Not Graduate</td><td>NA          </td><td> 2894       </td><td> 2792       </td><td>155         </td><td>360         </td><td> 1          </td><td>Rural       </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP002953    </td><td>Male        </td><td>Yes         </td><td>3+          </td><td>Graduate    </td><td>No          </td><td> 5703       </td><td>    0       </td><td>128         </td><td>360         </td><td> 1          </td><td>Urban       </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP002958    </td><td>Male        </td><td>No          </td><td>0           </td><td>Graduate    </td><td>No          </td><td> 3676       </td><td> 4301       </td><td>172         </td><td>360         </td><td> 1          </td><td>Rural       </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP002959    </td><td>Female      </td><td>Yes         </td><td>1           </td><td>Graduate    </td><td>No          </td><td>12000       </td><td>    0       </td><td>496         </td><td>360         </td><td> 1          </td><td>Semiurban   </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP002960    </td><td>Male        </td><td>Yes         </td><td>0           </td><td>Not Graduate</td><td>No          </td><td> 2400       </td><td> 3800       </td><td> NA         </td><td>180         </td><td> 1          </td><td>Urban       </td><td>N           </td></tr>\n",
       "\t<tr><td>LP002961    </td><td>Male        </td><td>Yes         </td><td>1           </td><td>Graduate    </td><td>No          </td><td> 3400       </td><td> 2500       </td><td>173         </td><td>360         </td><td> 1          </td><td>Semiurban   </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP002964    </td><td>Male        </td><td>Yes         </td><td>2           </td><td>Not Graduate</td><td>No          </td><td> 3987       </td><td> 1411       </td><td>157         </td><td>360         </td><td> 1          </td><td>Rural       </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP002974    </td><td>Male        </td><td>Yes         </td><td>0           </td><td>Graduate    </td><td>No          </td><td> 3232       </td><td> 1950       </td><td>108         </td><td>360         </td><td> 1          </td><td>Rural       </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP002978    </td><td>Female      </td><td>No          </td><td>0           </td><td>Graduate    </td><td>No          </td><td> 2900       </td><td>    0       </td><td> 71         </td><td>360         </td><td> 1          </td><td>Rural       </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP002979    </td><td>Male        </td><td>Yes         </td><td>3+          </td><td>Graduate    </td><td>No          </td><td> 4106       </td><td>    0       </td><td> 40         </td><td>180         </td><td> 1          </td><td>Rural       </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP002983    </td><td>Male        </td><td>Yes         </td><td>1           </td><td>Graduate    </td><td>No          </td><td> 8072       </td><td>  240       </td><td>253         </td><td>360         </td><td> 1          </td><td>Urban       </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP002984    </td><td>Male        </td><td>Yes         </td><td>2           </td><td>Graduate    </td><td>No          </td><td> 7583       </td><td>    0       </td><td>187         </td><td>360         </td><td> 1          </td><td>Urban       </td><td>Y           </td></tr>\n",
       "\t<tr><td>LP002990    </td><td>Female      </td><td>No          </td><td>0           </td><td>Graduate    </td><td>Yes         </td><td> 4583       </td><td>    0       </td><td>133         </td><td>360         </td><td> 0          </td><td>Semiurban   </td><td>N           </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllllllll}\n",
       " Loan\\_ID & Gender & Married & Dependents & Education & Self\\_Employed & ApplicantIncome & CoapplicantIncome & LoanAmount & Loan\\_Amount\\_Term & Credit\\_History & Property\\_Area & Loan\\_Status\\\\\n",
       "\\hline\n",
       "\t LP001002     & Male         & No           & 0            & Graduate     & No           &  5849        &     0        &  NA          & 360          &  1           & Urban        & Y           \\\\\n",
       "\t LP001003     & Male         & Yes          & 1            & Graduate     & No           &  4583        &  1508        & 128          & 360          &  1           & Rural        & N           \\\\\n",
       "\t LP001005     & Male         & Yes          & 0            & Graduate     & Yes          &  3000        &     0        &  66          & 360          &  1           & Urban        & Y           \\\\\n",
       "\t LP001006     & Male         & Yes          & 0            & Not Graduate & No           &  2583        &  2358        & 120          & 360          &  1           & Urban        & Y           \\\\\n",
       "\t LP001008     & Male         & No           & 0            & Graduate     & No           &  6000        &     0        & 141          & 360          &  1           & Urban        & Y           \\\\\n",
       "\t LP001011     & Male         & Yes          & 2            & Graduate     & Yes          &  5417        &  4196        & 267          & 360          &  1           & Urban        & Y           \\\\\n",
       "\t LP001013     & Male         & Yes          & 0            & Not Graduate & No           &  2333        &  1516        &  95          & 360          &  1           & Urban        & Y           \\\\\n",
       "\t LP001014     & Male         & Yes          & 3+           & Graduate     & No           &  3036        &  2504        & 158          & 360          &  0           & Semiurban    & N           \\\\\n",
       "\t LP001018     & Male         & Yes          & 2            & Graduate     & No           &  4006        &  1526        & 168          & 360          &  1           & Urban        & Y           \\\\\n",
       "\t LP001020     & Male         & Yes          & 1            & Graduate     & No           & 12841        & 10968        & 349          & 360          &  1           & Semiurban    & N           \\\\\n",
       "\t LP001024     & Male         & Yes          & 2            & Graduate     & No           &  3200        &   700        &  70          & 360          &  1           & Urban        & Y           \\\\\n",
       "\t LP001027     & Male         & Yes          & 2            & Graduate     & NA           &  2500        &  1840        & 109          & 360          &  1           & Urban        & Y           \\\\\n",
       "\t LP001028     & Male         & Yes          & 2            & Graduate     & No           &  3073        &  8106        & 200          & 360          &  1           & Urban        & Y           \\\\\n",
       "\t LP001029     & Male         & No           & 0            & Graduate     & No           &  1853        &  2840        & 114          & 360          &  1           & Rural        & N           \\\\\n",
       "\t LP001030     & Male         & Yes          & 2            & Graduate     & No           &  1299        &  1086        &  17          & 120          &  1           & Urban        & Y           \\\\\n",
       "\t LP001032     & Male         & No           & 0            & Graduate     & No           &  4950        &     0        & 125          & 360          &  1           & Urban        & Y           \\\\\n",
       "\t LP001034     & Male         & No           & 1            & Not Graduate & No           &  3596        &     0        & 100          & 240          & NA           & Urban        & Y           \\\\\n",
       "\t LP001036     & Female       & No           & 0            & Graduate     & No           &  3510        &     0        &  76          & 360          &  0           & Urban        & N           \\\\\n",
       "\t LP001038     & Male         & Yes          & 0            & Not Graduate & No           &  4887        &     0        & 133          & 360          &  1           & Rural        & N           \\\\\n",
       "\t LP001041     & Male         & Yes          & 0            & Graduate     & NA           &  2600        &  3500        & 115          &  NA          &  1           & Urban        & Y           \\\\\n",
       "\t LP001043     & Male         & Yes          & 0            & Not Graduate & No           &  7660        &     0        & 104          & 360          &  0           & Urban        & N           \\\\\n",
       "\t LP001046     & Male         & Yes          & 1            & Graduate     & No           &  5955        &  5625        & 315          & 360          &  1           & Urban        & Y           \\\\\n",
       "\t LP001047     & Male         & Yes          & 0            & Not Graduate & No           &  2600        &  1911        & 116          & 360          &  0           & Semiurban    & N           \\\\\n",
       "\t LP001050     & NA           & Yes          & 2            & Not Graduate & No           &  3365        &  1917        & 112          & 360          &  0           & Rural        & N           \\\\\n",
       "\t LP001052     & Male         & Yes          & 1            & Graduate     & NA           &  3717        &  2925        & 151          & 360          & NA           & Semiurban    & N           \\\\\n",
       "\t LP001066     & Male         & Yes          & 0            & Graduate     & Yes          &  9560        &     0        & 191          & 360          &  1           & Semiurban    & Y           \\\\\n",
       "\t LP001068     & Male         & Yes          & 0            & Graduate     & No           &  2799        &  2253        & 122          & 360          &  1           & Semiurban    & Y           \\\\\n",
       "\t LP001073     & Male         & Yes          & 2            & Not Graduate & No           &  4226        &  1040        & 110          & 360          &  1           & Urban        & Y           \\\\\n",
       "\t LP001086     & Male         & No           & 0            & Not Graduate & No           &  1442        &     0        &  35          & 360          &  1           & Urban        & N           \\\\\n",
       "\t LP001087     & Female       & No           & 2            & Graduate     & NA           &  3750        &  2083        & 120          & 360          &  1           & Semiurban    & Y           \\\\\n",
       "\t ... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ...\\\\\n",
       "\t LP002911     & Male         & Yes          & 1            & Graduate     & No           &  2787        &  1917        & 146          & 360          &  0           & Rural        & N           \\\\\n",
       "\t LP002912     & Male         & Yes          & 1            & Graduate     & No           &  4283        &  3000        & 172          &  84          &  1           & Rural        & N           \\\\\n",
       "\t LP002916     & Male         & Yes          & 0            & Graduate     & No           &  2297        &  1522        & 104          & 360          &  1           & Urban        & Y           \\\\\n",
       "\t LP002917     & Female       & No           & 0            & Not Graduate & No           &  2165        &     0        &  70          & 360          &  1           & Semiurban    & Y           \\\\\n",
       "\t LP002925     & NA           & No           & 0            & Graduate     & No           &  4750        &     0        &  94          & 360          &  1           & Semiurban    & Y           \\\\\n",
       "\t LP002926     & Male         & Yes          & 2            & Graduate     & Yes          &  2726        &     0        & 106          & 360          &  0           & Semiurban    & N           \\\\\n",
       "\t LP002928     & Male         & Yes          & 0            & Graduate     & No           &  3000        &  3416        &  56          & 180          &  1           & Semiurban    & Y           \\\\\n",
       "\t LP002931     & Male         & Yes          & 2            & Graduate     & Yes          &  6000        &     0        & 205          & 240          &  1           & Semiurban    & N           \\\\\n",
       "\t LP002933     & NA           & No           & 3+           & Graduate     & Yes          &  9357        &     0        & 292          & 360          &  1           & Semiurban    & Y           \\\\\n",
       "\t LP002936     & Male         & Yes          & 0            & Graduate     & No           &  3859        &  3300        & 142          & 180          &  1           & Rural        & Y           \\\\\n",
       "\t LP002938     & Male         & Yes          & 0            & Graduate     & Yes          & 16120        &     0        & 260          & 360          &  1           & Urban        & Y           \\\\\n",
       "\t LP002940     & Male         & No           & 0            & Not Graduate & No           &  3833        &     0        & 110          & 360          &  1           & Rural        & Y           \\\\\n",
       "\t LP002941     & Male         & Yes          & 2            & Not Graduate & Yes          &  6383        &  1000        & 187          & 360          &  1           & Rural        & N           \\\\\n",
       "\t LP002943     & Male         & No           & NA           & Graduate     & No           &  2987        &     0        &  88          & 360          &  0           & Semiurban    & N           \\\\\n",
       "\t LP002945     & Male         & Yes          & 0            & Graduate     & Yes          &  9963        &     0        & 180          & 360          &  1           & Rural        & Y           \\\\\n",
       "\t LP002948     & Male         & Yes          & 2            & Graduate     & No           &  5780        &     0        & 192          & 360          &  1           & Urban        & Y           \\\\\n",
       "\t LP002949     & Female       & No           & 3+           & Graduate     & NA           &   416        & 41667        & 350          & 180          & NA           & Urban        & N           \\\\\n",
       "\t LP002950     & Male         & Yes          & 0            & Not Graduate & NA           &  2894        &  2792        & 155          & 360          &  1           & Rural        & Y           \\\\\n",
       "\t LP002953     & Male         & Yes          & 3+           & Graduate     & No           &  5703        &     0        & 128          & 360          &  1           & Urban        & Y           \\\\\n",
       "\t LP002958     & Male         & No           & 0            & Graduate     & No           &  3676        &  4301        & 172          & 360          &  1           & Rural        & Y           \\\\\n",
       "\t LP002959     & Female       & Yes          & 1            & Graduate     & No           & 12000        &     0        & 496          & 360          &  1           & Semiurban    & Y           \\\\\n",
       "\t LP002960     & Male         & Yes          & 0            & Not Graduate & No           &  2400        &  3800        &  NA          & 180          &  1           & Urban        & N           \\\\\n",
       "\t LP002961     & Male         & Yes          & 1            & Graduate     & No           &  3400        &  2500        & 173          & 360          &  1           & Semiurban    & Y           \\\\\n",
       "\t LP002964     & Male         & Yes          & 2            & Not Graduate & No           &  3987        &  1411        & 157          & 360          &  1           & Rural        & Y           \\\\\n",
       "\t LP002974     & Male         & Yes          & 0            & Graduate     & No           &  3232        &  1950        & 108          & 360          &  1           & Rural        & Y           \\\\\n",
       "\t LP002978     & Female       & No           & 0            & Graduate     & No           &  2900        &     0        &  71          & 360          &  1           & Rural        & Y           \\\\\n",
       "\t LP002979     & Male         & Yes          & 3+           & Graduate     & No           &  4106        &     0        &  40          & 180          &  1           & Rural        & Y           \\\\\n",
       "\t LP002983     & Male         & Yes          & 1            & Graduate     & No           &  8072        &   240        & 253          & 360          &  1           & Urban        & Y           \\\\\n",
       "\t LP002984     & Male         & Yes          & 2            & Graduate     & No           &  7583        &     0        & 187          & 360          &  1           & Urban        & Y           \\\\\n",
       "\t LP002990     & Female       & No           & 0            & Graduate     & Yes          &  4583        &     0        & 133          & 360          &  0           & Semiurban    & N           \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "Loan_ID | Gender | Married | Dependents | Education | Self_Employed | ApplicantIncome | CoapplicantIncome | LoanAmount | Loan_Amount_Term | Credit_History | Property_Area | Loan_Status | \n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| LP001002     | Male         | No           | 0            | Graduate     | No           |  5849        |     0        |  NA          | 360          |  1           | Urban        | Y            | \n",
       "| LP001003     | Male         | Yes          | 1            | Graduate     | No           |  4583        |  1508        | 128          | 360          |  1           | Rural        | N            | \n",
       "| LP001005     | Male         | Yes          | 0            | Graduate     | Yes          |  3000        |     0        |  66          | 360          |  1           | Urban        | Y            | \n",
       "| LP001006     | Male         | Yes          | 0            | Not Graduate | No           |  2583        |  2358        | 120          | 360          |  1           | Urban        | Y            | \n",
       "| LP001008     | Male         | No           | 0            | Graduate     | No           |  6000        |     0        | 141          | 360          |  1           | Urban        | Y            | \n",
       "| LP001011     | Male         | Yes          | 2            | Graduate     | Yes          |  5417        |  4196        | 267          | 360          |  1           | Urban        | Y            | \n",
       "| LP001013     | Male         | Yes          | 0            | Not Graduate | No           |  2333        |  1516        |  95          | 360          |  1           | Urban        | Y            | \n",
       "| LP001014     | Male         | Yes          | 3+           | Graduate     | No           |  3036        |  2504        | 158          | 360          |  0           | Semiurban    | N            | \n",
       "| LP001018     | Male         | Yes          | 2            | Graduate     | No           |  4006        |  1526        | 168          | 360          |  1           | Urban        | Y            | \n",
       "| LP001020     | Male         | Yes          | 1            | Graduate     | No           | 12841        | 10968        | 349          | 360          |  1           | Semiurban    | N            | \n",
       "| LP001024     | Male         | Yes          | 2            | Graduate     | No           |  3200        |   700        |  70          | 360          |  1           | Urban        | Y            | \n",
       "| LP001027     | Male         | Yes          | 2            | Graduate     | NA           |  2500        |  1840        | 109          | 360          |  1           | Urban        | Y            | \n",
       "| LP001028     | Male         | Yes          | 2            | Graduate     | No           |  3073        |  8106        | 200          | 360          |  1           | Urban        | Y            | \n",
       "| LP001029     | Male         | No           | 0            | Graduate     | No           |  1853        |  2840        | 114          | 360          |  1           | Rural        | N            | \n",
       "| LP001030     | Male         | Yes          | 2            | Graduate     | No           |  1299        |  1086        |  17          | 120          |  1           | Urban        | Y            | \n",
       "| LP001032     | Male         | No           | 0            | Graduate     | No           |  4950        |     0        | 125          | 360          |  1           | Urban        | Y            | \n",
       "| LP001034     | Male         | No           | 1            | Not Graduate | No           |  3596        |     0        | 100          | 240          | NA           | Urban        | Y            | \n",
       "| LP001036     | Female       | No           | 0            | Graduate     | No           |  3510        |     0        |  76          | 360          |  0           | Urban        | N            | \n",
       "| LP001038     | Male         | Yes          | 0            | Not Graduate | No           |  4887        |     0        | 133          | 360          |  1           | Rural        | N            | \n",
       "| LP001041     | Male         | Yes          | 0            | Graduate     | NA           |  2600        |  3500        | 115          |  NA          |  1           | Urban        | Y            | \n",
       "| LP001043     | Male         | Yes          | 0            | Not Graduate | No           |  7660        |     0        | 104          | 360          |  0           | Urban        | N            | \n",
       "| LP001046     | Male         | Yes          | 1            | Graduate     | No           |  5955        |  5625        | 315          | 360          |  1           | Urban        | Y            | \n",
       "| LP001047     | Male         | Yes          | 0            | Not Graduate | No           |  2600        |  1911        | 116          | 360          |  0           | Semiurban    | N            | \n",
       "| LP001050     | NA           | Yes          | 2            | Not Graduate | No           |  3365        |  1917        | 112          | 360          |  0           | Rural        | N            | \n",
       "| LP001052     | Male         | Yes          | 1            | Graduate     | NA           |  3717        |  2925        | 151          | 360          | NA           | Semiurban    | N            | \n",
       "| LP001066     | Male         | Yes          | 0            | Graduate     | Yes          |  9560        |     0        | 191          | 360          |  1           | Semiurban    | Y            | \n",
       "| LP001068     | Male         | Yes          | 0            | Graduate     | No           |  2799        |  2253        | 122          | 360          |  1           | Semiurban    | Y            | \n",
       "| LP001073     | Male         | Yes          | 2            | Not Graduate | No           |  4226        |  1040        | 110          | 360          |  1           | Urban        | Y            | \n",
       "| LP001086     | Male         | No           | 0            | Not Graduate | No           |  1442        |     0        |  35          | 360          |  1           | Urban        | N            | \n",
       "| LP001087     | Female       | No           | 2            | Graduate     | NA           |  3750        |  2083        | 120          | 360          |  1           | Semiurban    | Y            | \n",
       "| ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | \n",
       "| LP002911     | Male         | Yes          | 1            | Graduate     | No           |  2787        |  1917        | 146          | 360          |  0           | Rural        | N            | \n",
       "| LP002912     | Male         | Yes          | 1            | Graduate     | No           |  4283        |  3000        | 172          |  84          |  1           | Rural        | N            | \n",
       "| LP002916     | Male         | Yes          | 0            | Graduate     | No           |  2297        |  1522        | 104          | 360          |  1           | Urban        | Y            | \n",
       "| LP002917     | Female       | No           | 0            | Not Graduate | No           |  2165        |     0        |  70          | 360          |  1           | Semiurban    | Y            | \n",
       "| LP002925     | NA           | No           | 0            | Graduate     | No           |  4750        |     0        |  94          | 360          |  1           | Semiurban    | Y            | \n",
       "| LP002926     | Male         | Yes          | 2            | Graduate     | Yes          |  2726        |     0        | 106          | 360          |  0           | Semiurban    | N            | \n",
       "| LP002928     | Male         | Yes          | 0            | Graduate     | No           |  3000        |  3416        |  56          | 180          |  1           | Semiurban    | Y            | \n",
       "| LP002931     | Male         | Yes          | 2            | Graduate     | Yes          |  6000        |     0        | 205          | 240          |  1           | Semiurban    | N            | \n",
       "| LP002933     | NA           | No           | 3+           | Graduate     | Yes          |  9357        |     0        | 292          | 360          |  1           | Semiurban    | Y            | \n",
       "| LP002936     | Male         | Yes          | 0            | Graduate     | No           |  3859        |  3300        | 142          | 180          |  1           | Rural        | Y            | \n",
       "| LP002938     | Male         | Yes          | 0            | Graduate     | Yes          | 16120        |     0        | 260          | 360          |  1           | Urban        | Y            | \n",
       "| LP002940     | Male         | No           | 0            | Not Graduate | No           |  3833        |     0        | 110          | 360          |  1           | Rural        | Y            | \n",
       "| LP002941     | Male         | Yes          | 2            | Not Graduate | Yes          |  6383        |  1000        | 187          | 360          |  1           | Rural        | N            | \n",
       "| LP002943     | Male         | No           | NA           | Graduate     | No           |  2987        |     0        |  88          | 360          |  0           | Semiurban    | N            | \n",
       "| LP002945     | Male         | Yes          | 0            | Graduate     | Yes          |  9963        |     0        | 180          | 360          |  1           | Rural        | Y            | \n",
       "| LP002948     | Male         | Yes          | 2            | Graduate     | No           |  5780        |     0        | 192          | 360          |  1           | Urban        | Y            | \n",
       "| LP002949     | Female       | No           | 3+           | Graduate     | NA           |   416        | 41667        | 350          | 180          | NA           | Urban        | N            | \n",
       "| LP002950     | Male         | Yes          | 0            | Not Graduate | NA           |  2894        |  2792        | 155          | 360          |  1           | Rural        | Y            | \n",
       "| LP002953     | Male         | Yes          | 3+           | Graduate     | No           |  5703        |     0        | 128          | 360          |  1           | Urban        | Y            | \n",
       "| LP002958     | Male         | No           | 0            | Graduate     | No           |  3676        |  4301        | 172          | 360          |  1           | Rural        | Y            | \n",
       "| LP002959     | Female       | Yes          | 1            | Graduate     | No           | 12000        |     0        | 496          | 360          |  1           | Semiurban    | Y            | \n",
       "| LP002960     | Male         | Yes          | 0            | Not Graduate | No           |  2400        |  3800        |  NA          | 180          |  1           | Urban        | N            | \n",
       "| LP002961     | Male         | Yes          | 1            | Graduate     | No           |  3400        |  2500        | 173          | 360          |  1           | Semiurban    | Y            | \n",
       "| LP002964     | Male         | Yes          | 2            | Not Graduate | No           |  3987        |  1411        | 157          | 360          |  1           | Rural        | Y            | \n",
       "| LP002974     | Male         | Yes          | 0            | Graduate     | No           |  3232        |  1950        | 108          | 360          |  1           | Rural        | Y            | \n",
       "| LP002978     | Female       | No           | 0            | Graduate     | No           |  2900        |     0        |  71          | 360          |  1           | Rural        | Y            | \n",
       "| LP002979     | Male         | Yes          | 3+           | Graduate     | No           |  4106        |     0        |  40          | 180          |  1           | Rural        | Y            | \n",
       "| LP002983     | Male         | Yes          | 1            | Graduate     | No           |  8072        |   240        | 253          | 360          |  1           | Urban        | Y            | \n",
       "| LP002984     | Male         | Yes          | 2            | Graduate     | No           |  7583        |     0        | 187          | 360          |  1           | Urban        | Y            | \n",
       "| LP002990     | Female       | No           | 0            | Graduate     | Yes          |  4583        |     0        | 133          | 360          |  0           | Semiurban    | N            | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "    Loan_ID  Gender Married Dependents Education    Self_Employed\n",
       "1   LP001002 Male   No      0          Graduate     No           \n",
       "2   LP001003 Male   Yes     1          Graduate     No           \n",
       "3   LP001005 Male   Yes     0          Graduate     Yes          \n",
       "4   LP001006 Male   Yes     0          Not Graduate No           \n",
       "5   LP001008 Male   No      0          Graduate     No           \n",
       "6   LP001011 Male   Yes     2          Graduate     Yes          \n",
       "7   LP001013 Male   Yes     0          Not Graduate No           \n",
       "8   LP001014 Male   Yes     3+         Graduate     No           \n",
       "9   LP001018 Male   Yes     2          Graduate     No           \n",
       "10  LP001020 Male   Yes     1          Graduate     No           \n",
       "11  LP001024 Male   Yes     2          Graduate     No           \n",
       "12  LP001027 Male   Yes     2          Graduate     NA           \n",
       "13  LP001028 Male   Yes     2          Graduate     No           \n",
       "14  LP001029 Male   No      0          Graduate     No           \n",
       "15  LP001030 Male   Yes     2          Graduate     No           \n",
       "16  LP001032 Male   No      0          Graduate     No           \n",
       "17  LP001034 Male   No      1          Not Graduate No           \n",
       "18  LP001036 Female No      0          Graduate     No           \n",
       "19  LP001038 Male   Yes     0          Not Graduate No           \n",
       "20  LP001041 Male   Yes     0          Graduate     NA           \n",
       "21  LP001043 Male   Yes     0          Not Graduate No           \n",
       "22  LP001046 Male   Yes     1          Graduate     No           \n",
       "23  LP001047 Male   Yes     0          Not Graduate No           \n",
       "24  LP001050 NA     Yes     2          Not Graduate No           \n",
       "25  LP001052 Male   Yes     1          Graduate     NA           \n",
       "26  LP001066 Male   Yes     0          Graduate     Yes          \n",
       "27  LP001068 Male   Yes     0          Graduate     No           \n",
       "28  LP001073 Male   Yes     2          Not Graduate No           \n",
       "29  LP001086 Male   No      0          Not Graduate No           \n",
       "30  LP001087 Female No      2          Graduate     NA           \n",
       "... ...      ...    ...     ...        ...          ...          \n",
       "585 LP002911 Male   Yes     1          Graduate     No           \n",
       "586 LP002912 Male   Yes     1          Graduate     No           \n",
       "587 LP002916 Male   Yes     0          Graduate     No           \n",
       "588 LP002917 Female No      0          Not Graduate No           \n",
       "589 LP002925 NA     No      0          Graduate     No           \n",
       "590 LP002926 Male   Yes     2          Graduate     Yes          \n",
       "591 LP002928 Male   Yes     0          Graduate     No           \n",
       "592 LP002931 Male   Yes     2          Graduate     Yes          \n",
       "593 LP002933 NA     No      3+         Graduate     Yes          \n",
       "594 LP002936 Male   Yes     0          Graduate     No           \n",
       "595 LP002938 Male   Yes     0          Graduate     Yes          \n",
       "596 LP002940 Male   No      0          Not Graduate No           \n",
       "597 LP002941 Male   Yes     2          Not Graduate Yes          \n",
       "598 LP002943 Male   No      NA         Graduate     No           \n",
       "599 LP002945 Male   Yes     0          Graduate     Yes          \n",
       "600 LP002948 Male   Yes     2          Graduate     No           \n",
       "601 LP002949 Female No      3+         Graduate     NA           \n",
       "602 LP002950 Male   Yes     0          Not Graduate NA           \n",
       "603 LP002953 Male   Yes     3+         Graduate     No           \n",
       "604 LP002958 Male   No      0          Graduate     No           \n",
       "605 LP002959 Female Yes     1          Graduate     No           \n",
       "606 LP002960 Male   Yes     0          Not Graduate No           \n",
       "607 LP002961 Male   Yes     1          Graduate     No           \n",
       "608 LP002964 Male   Yes     2          Not Graduate No           \n",
       "609 LP002974 Male   Yes     0          Graduate     No           \n",
       "610 LP002978 Female No      0          Graduate     No           \n",
       "611 LP002979 Male   Yes     3+         Graduate     No           \n",
       "612 LP002983 Male   Yes     1          Graduate     No           \n",
       "613 LP002984 Male   Yes     2          Graduate     No           \n",
       "614 LP002990 Female No      0          Graduate     Yes          \n",
       "    ApplicantIncome CoapplicantIncome LoanAmount Loan_Amount_Term\n",
       "1    5849               0              NA        360             \n",
       "2    4583            1508             128        360             \n",
       "3    3000               0              66        360             \n",
       "4    2583            2358             120        360             \n",
       "5    6000               0             141        360             \n",
       "6    5417            4196             267        360             \n",
       "7    2333            1516              95        360             \n",
       "8    3036            2504             158        360             \n",
       "9    4006            1526             168        360             \n",
       "10  12841           10968             349        360             \n",
       "11   3200             700              70        360             \n",
       "12   2500            1840             109        360             \n",
       "13   3073            8106             200        360             \n",
       "14   1853            2840             114        360             \n",
       "15   1299            1086              17        120             \n",
       "16   4950               0             125        360             \n",
       "17   3596               0             100        240             \n",
       "18   3510               0              76        360             \n",
       "19   4887               0             133        360             \n",
       "20   2600            3500             115         NA             \n",
       "21   7660               0             104        360             \n",
       "22   5955            5625             315        360             \n",
       "23   2600            1911             116        360             \n",
       "24   3365            1917             112        360             \n",
       "25   3717            2925             151        360             \n",
       "26   9560               0             191        360             \n",
       "27   2799            2253             122        360             \n",
       "28   4226            1040             110        360             \n",
       "29   1442               0              35        360             \n",
       "30   3750            2083             120        360             \n",
       "... ...             ...               ...        ...             \n",
       "585  2787            1917             146        360             \n",
       "586  4283            3000             172         84             \n",
       "587  2297            1522             104        360             \n",
       "588  2165               0              70        360             \n",
       "589  4750               0              94        360             \n",
       "590  2726               0             106        360             \n",
       "591  3000            3416              56        180             \n",
       "592  6000               0             205        240             \n",
       "593  9357               0             292        360             \n",
       "594  3859            3300             142        180             \n",
       "595 16120               0             260        360             \n",
       "596  3833               0             110        360             \n",
       "597  6383            1000             187        360             \n",
       "598  2987               0              88        360             \n",
       "599  9963               0             180        360             \n",
       "600  5780               0             192        360             \n",
       "601   416           41667             350        180             \n",
       "602  2894            2792             155        360             \n",
       "603  5703               0             128        360             \n",
       "604  3676            4301             172        360             \n",
       "605 12000               0             496        360             \n",
       "606  2400            3800              NA        180             \n",
       "607  3400            2500             173        360             \n",
       "608  3987            1411             157        360             \n",
       "609  3232            1950             108        360             \n",
       "610  2900               0              71        360             \n",
       "611  4106               0              40        180             \n",
       "612  8072             240             253        360             \n",
       "613  7583               0             187        360             \n",
       "614  4583               0             133        360             \n",
       "    Credit_History Property_Area Loan_Status\n",
       "1    1             Urban         Y          \n",
       "2    1             Rural         N          \n",
       "3    1             Urban         Y          \n",
       "4    1             Urban         Y          \n",
       "5    1             Urban         Y          \n",
       "6    1             Urban         Y          \n",
       "7    1             Urban         Y          \n",
       "8    0             Semiurban     N          \n",
       "9    1             Urban         Y          \n",
       "10   1             Semiurban     N          \n",
       "11   1             Urban         Y          \n",
       "12   1             Urban         Y          \n",
       "13   1             Urban         Y          \n",
       "14   1             Rural         N          \n",
       "15   1             Urban         Y          \n",
       "16   1             Urban         Y          \n",
       "17  NA             Urban         Y          \n",
       "18   0             Urban         N          \n",
       "19   1             Rural         N          \n",
       "20   1             Urban         Y          \n",
       "21   0             Urban         N          \n",
       "22   1             Urban         Y          \n",
       "23   0             Semiurban     N          \n",
       "24   0             Rural         N          \n",
       "25  NA             Semiurban     N          \n",
       "26   1             Semiurban     Y          \n",
       "27   1             Semiurban     Y          \n",
       "28   1             Urban         Y          \n",
       "29   1             Urban         N          \n",
       "30   1             Semiurban     Y          \n",
       "... ...            ...           ...        \n",
       "585  0             Rural         N          \n",
       "586  1             Rural         N          \n",
       "587  1             Urban         Y          \n",
       "588  1             Semiurban     Y          \n",
       "589  1             Semiurban     Y          \n",
       "590  0             Semiurban     N          \n",
       "591  1             Semiurban     Y          \n",
       "592  1             Semiurban     N          \n",
       "593  1             Semiurban     Y          \n",
       "594  1             Rural         Y          \n",
       "595  1             Urban         Y          \n",
       "596  1             Rural         Y          \n",
       "597  1             Rural         N          \n",
       "598  0             Semiurban     N          \n",
       "599  1             Rural         Y          \n",
       "600  1             Urban         Y          \n",
       "601 NA             Urban         N          \n",
       "602  1             Rural         Y          \n",
       "603  1             Urban         Y          \n",
       "604  1             Rural         Y          \n",
       "605  1             Semiurban     Y          \n",
       "606  1             Urban         N          \n",
       "607  1             Semiurban     Y          \n",
       "608  1             Rural         Y          \n",
       "609  1             Rural         Y          \n",
       "610  1             Rural         Y          \n",
       "611  1             Rural         Y          \n",
       "612  1             Urban         Y          \n",
       "613  1             Urban         Y          \n",
       "614  0             Semiurban     N          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df <- read.csv('C:/Datasets/LoanTrain.csv',na.strings='')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0"
      ],
      "text/latex": [
       "0"
      ],
      "text/markdown": [
       "0"
      ],
      "text/plain": [
       "[1] 0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sum(duplicated(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     Loan_ID       Gender    Married    Dependents        Education  \n",
       " LP001002:  1   Female:112   No  :213   0   :345   Graduate    :480  \n",
       " LP001003:  1   Male  :489   Yes :398   1   :102   Not Graduate:134  \n",
       " LP001005:  1   NA's  : 13   NA's:  3   2   :101                     \n",
       " LP001006:  1                           3+  : 51                     \n",
       " LP001008:  1                           NA's: 15                     \n",
       " LP001011:  1                                                        \n",
       " (Other) :608                                                        \n",
       " Self_Employed ApplicantIncome CoapplicantIncome   LoanAmount   \n",
       " No  :500      Min.   :  150   Min.   :    0     Min.   :  9.0  \n",
       " Yes : 82      1st Qu.: 2878   1st Qu.:    0     1st Qu.:100.0  \n",
       " NA's: 32      Median : 3812   Median : 1188     Median :128.0  \n",
       "               Mean   : 5403   Mean   : 1621     Mean   :146.4  \n",
       "               3rd Qu.: 5795   3rd Qu.: 2297     3rd Qu.:168.0  \n",
       "               Max.   :81000   Max.   :41667     Max.   :700.0  \n",
       "                                                 NA's   :22     \n",
       " Loan_Amount_Term Credit_History     Property_Area Loan_Status\n",
       " Min.   : 12      Min.   :0.0000   Rural    :179   N:192      \n",
       " 1st Qu.:360      1st Qu.:1.0000   Semiurban:233   Y:422      \n",
       " Median :360      Median :1.0000   Urban    :202              \n",
       " Mean   :342      Mean   :0.8422                              \n",
       " 3rd Qu.:360      3rd Qu.:1.0000                              \n",
       " Max.   :480      Max.   :1.0000                              \n",
       " NA's   :14       NA's   :50                                  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Credit_History and Loan_Status to factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     Loan_ID       Gender    Married    Dependents        Education  \n",
       " LP001002:  1   Female:112   No  :213   0   :345   Graduate    :480  \n",
       " LP001003:  1   Male  :489   Yes :398   1   :102   Not Graduate:134  \n",
       " LP001005:  1   NA's  : 13   NA's:  3   2   :101                     \n",
       " LP001006:  1                           3+  : 51                     \n",
       " LP001008:  1                           NA's: 15                     \n",
       " LP001011:  1                                                        \n",
       " (Other) :608                                                        \n",
       " Self_Employed ApplicantIncome CoapplicantIncome   LoanAmount   \n",
       " No  :500      Min.   :  150   Min.   :    0     Min.   :  9.0  \n",
       " Yes : 82      1st Qu.: 2878   1st Qu.:    0     1st Qu.:100.0  \n",
       " NA's: 32      Median : 3812   Median : 1188     Median :128.0  \n",
       "               Mean   : 5403   Mean   : 1621     Mean   :146.4  \n",
       "               3rd Qu.: 5795   3rd Qu.: 2297     3rd Qu.:168.0  \n",
       "               Max.   :81000   Max.   :41667     Max.   :700.0  \n",
       "                                                 NA's   :22     \n",
       " Loan_Amount_Term Credit_History   Property_Area Loan_Status\n",
       " Min.   : 12      0   : 89       Rural    :179   N:192      \n",
       " 1st Qu.:360      1   :475       Semiurban:233   Y:422      \n",
       " Median :360      NA's: 50       Urban    :202              \n",
       " Mean   :342                                                \n",
       " 3rd Qu.:360                                                \n",
       " Max.   :480                                                \n",
       " NA's   :14                                                 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df$Credit_History<-as.factor(df$Credit_History)\n",
    "summary(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>freqRatio</th><th scope=col>percentUnique</th><th scope=col>zeroVar</th><th scope=col>nzv</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Loan_ID</th><td> 1.000000  </td><td>100.0000000</td><td>FALSE      </td><td>FALSE      </td></tr>\n",
       "\t<tr><th scope=row>Gender</th><td> 4.366071  </td><td>  0.3257329</td><td>FALSE      </td><td>FALSE      </td></tr>\n",
       "\t<tr><th scope=row>Married</th><td> 1.868545  </td><td>  0.3257329</td><td>FALSE      </td><td>FALSE      </td></tr>\n",
       "\t<tr><th scope=row>Dependents</th><td> 3.382353  </td><td>  0.6514658</td><td>FALSE      </td><td>FALSE      </td></tr>\n",
       "\t<tr><th scope=row>Education</th><td> 3.582090  </td><td>  0.3257329</td><td>FALSE      </td><td>FALSE      </td></tr>\n",
       "\t<tr><th scope=row>Self_Employed</th><td> 6.097561  </td><td>  0.3257329</td><td>FALSE      </td><td>FALSE      </td></tr>\n",
       "\t<tr><th scope=row>ApplicantIncome</th><td> 1.500000  </td><td> 82.2475570</td><td>FALSE      </td><td>FALSE      </td></tr>\n",
       "\t<tr><th scope=row>CoapplicantIncome</th><td>54.600000  </td><td> 46.7426710</td><td>FALSE      </td><td>FALSE      </td></tr>\n",
       "\t<tr><th scope=row>LoanAmount</th><td> 1.176471  </td><td> 33.0618893</td><td>FALSE      </td><td>FALSE      </td></tr>\n",
       "\t<tr><th scope=row>Loan_Amount_Term</th><td>11.636364  </td><td>  1.6286645</td><td>FALSE      </td><td>FALSE      </td></tr>\n",
       "\t<tr><th scope=row>Credit_History</th><td> 5.337079  </td><td>  0.3257329</td><td>FALSE      </td><td>FALSE      </td></tr>\n",
       "\t<tr><th scope=row>Property_Area</th><td> 1.153465  </td><td>  0.4885993</td><td>FALSE      </td><td>FALSE      </td></tr>\n",
       "\t<tr><th scope=row>Loan_Status</th><td> 2.197917  </td><td>  0.3257329</td><td>FALSE      </td><td>FALSE      </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       "  & freqRatio & percentUnique & zeroVar & nzv\\\\\n",
       "\\hline\n",
       "\tLoan\\_ID &  1.000000   & 100.0000000 & FALSE       & FALSE      \\\\\n",
       "\tGender &  4.366071   &   0.3257329 & FALSE       & FALSE      \\\\\n",
       "\tMarried &  1.868545   &   0.3257329 & FALSE       & FALSE      \\\\\n",
       "\tDependents &  3.382353   &   0.6514658 & FALSE       & FALSE      \\\\\n",
       "\tEducation &  3.582090   &   0.3257329 & FALSE       & FALSE      \\\\\n",
       "\tSelf\\_Employed &  6.097561   &   0.3257329 & FALSE       & FALSE      \\\\\n",
       "\tApplicantIncome &  1.500000   &  82.2475570 & FALSE       & FALSE      \\\\\n",
       "\tCoapplicantIncome & 54.600000   &  46.7426710 & FALSE       & FALSE      \\\\\n",
       "\tLoanAmount &  1.176471   &  33.0618893 & FALSE       & FALSE      \\\\\n",
       "\tLoan\\_Amount\\_Term & 11.636364   &   1.6286645 & FALSE       & FALSE      \\\\\n",
       "\tCredit\\_History &  5.337079   &   0.3257329 & FALSE       & FALSE      \\\\\n",
       "\tProperty\\_Area &  1.153465   &   0.4885993 & FALSE       & FALSE      \\\\\n",
       "\tLoan\\_Status &  2.197917   &   0.3257329 & FALSE       & FALSE      \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | freqRatio | percentUnique | zeroVar | nzv | \n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| Loan_ID |  1.000000   | 100.0000000 | FALSE       | FALSE       | \n",
       "| Gender |  4.366071   |   0.3257329 | FALSE       | FALSE       | \n",
       "| Married |  1.868545   |   0.3257329 | FALSE       | FALSE       | \n",
       "| Dependents |  3.382353   |   0.6514658 | FALSE       | FALSE       | \n",
       "| Education |  3.582090   |   0.3257329 | FALSE       | FALSE       | \n",
       "| Self_Employed |  6.097561   |   0.3257329 | FALSE       | FALSE       | \n",
       "| ApplicantIncome |  1.500000   |  82.2475570 | FALSE       | FALSE       | \n",
       "| CoapplicantIncome | 54.600000   |  46.7426710 | FALSE       | FALSE       | \n",
       "| LoanAmount |  1.176471   |  33.0618893 | FALSE       | FALSE       | \n",
       "| Loan_Amount_Term | 11.636364   |   1.6286645 | FALSE       | FALSE       | \n",
       "| Credit_History |  5.337079   |   0.3257329 | FALSE       | FALSE       | \n",
       "| Property_Area |  1.153465   |   0.4885993 | FALSE       | FALSE       | \n",
       "| Loan_Status |  2.197917   |   0.3257329 | FALSE       | FALSE       | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "                  freqRatio percentUnique zeroVar nzv  \n",
       "Loan_ID            1.000000 100.0000000   FALSE   FALSE\n",
       "Gender             4.366071   0.3257329   FALSE   FALSE\n",
       "Married            1.868545   0.3257329   FALSE   FALSE\n",
       "Dependents         3.382353   0.6514658   FALSE   FALSE\n",
       "Education          3.582090   0.3257329   FALSE   FALSE\n",
       "Self_Employed      6.097561   0.3257329   FALSE   FALSE\n",
       "ApplicantIncome    1.500000  82.2475570   FALSE   FALSE\n",
       "CoapplicantIncome 54.600000  46.7426710   FALSE   FALSE\n",
       "LoanAmount         1.176471  33.0618893   FALSE   FALSE\n",
       "Loan_Amount_Term  11.636364   1.6286645   FALSE   FALSE\n",
       "Credit_History     5.337079   0.3257329   FALSE   FALSE\n",
       "Property_Area      1.153465   0.4885993   FALSE   FALSE\n",
       "Loan_Status        2.197917   0.3257329   FALSE   FALSE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nearZeroVar(df, saveMetrics=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the summary above, some factors levels are unbalanced. However, it is not so unbalanced as to warrant exclusion of any factor. \n",
    "\n",
    "Next, check for outliers in continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM\njIyampqnp6eysrK9vb3Hx8fQ0NDZ2dnh4eHp6enw8PD////QFLu4AAAACXBIWXMAABJ0AAAS\ndAHeZh94AAAdWElEQVR4nO3da0OqTBhG4TE77Q76///tFlRCDRUC576fZ10ffMuoaKalMNh+\nyxbAn5XaOwBEQEjADCRCeimlPF/dYrfB4eaur3f2iZjmYzcxq5evCZ+Zb74kdruUW+M3ZmI+\nV73NbCdGwGvZex3/qfnmS2G3P9vp+ry2yZjxPdnWdmLqeytHH6M/N998Kez27pHv+cbjXr6J\nqe57N3Tv2+3XupSn0Z+cb74UdvuplM3uYLx5czeO3y+HqM7e7gb5Y1fd/sD9qzm5Wn8ePv66\nKk/vh+PE7sfqPvHw0ZPP336uD5/fbLJ5Kav35hdntX8E3n3G6nXzmCEQ9HYY+u+nt+/mvz9j\ndTruzFdLIKTvZqVhN1jNdO3GZ9WM6/ry7ePEHI7cv46HhO1BYbNZ+/b7wMQcP9r//OObb71v\n9twdy7TvllXaktb7GTnqjdXpuDNf+x2v9H173kv5t/23H7VmCDfNHP67ePtw89He+9pM11Pz\nkX/tzJXDvc1RSLk4VOh/tPf5X82bm3U7SbtNXpqP7W7/tdu9NRP+vp+1lE6G8WSsLsad+ZII\nqX3o23SPZJ/tQ9fzxduHm+d2HDfP3aLs8UNf2/5mgx/tff7L8Ru87I9LerftXrWfsn70cKg4\nDak/Vr2PM1/djlf6vv1daE9mmxOlblD7A/zz9uWwb95f1r9v+/PFhz+6Ot69Ot7duy1HC//0\nsk5/9P5YXR/3pPNV//fkoxuCj7ET89yN3aSJubxbaGKq2x9CHfXH6vq4J52v+r8nL90QvIyc\nmN1B9PrfZrlHuOV/dmWvhwW4r3bVrj9W18c96XzV/21ZdSHtx+f2Mffn4Zj5chjvmJje558d\nc29Pbs8WrfLZndq315Ge2vOOgbFivrodr/nNG1/H89d2xMrdq0BPbYKf7ZLomInpff7n6SrQ\n9uT2rdmvz1uvAYzs+Aqh469vN1an48587Xe80vftvLUDv23XRd+aMXlqpu6wgtd/+zhsr93k\n7l/DsmoeivpD3xwrXqwtdW/3Pv/45uv2l4nZPO2/euKnpeNBd3uE1xur03FnvvY7Xun7dvar\nddv2uuxTOyYvZdVdHe+/fRzfj/XxSvd7c/V700xDf+i/n3uPS5cT0/v89s31x8/H+rfbt92u\nvSTuaP+agu7V3z9jdTbuzFe7Q7W+8e/6T/MnT/mQxHwd/fazF1R057yhontDGhHizEr6R7h7\nQ1p2L+6Vc74IyQAh6TMICV4h5URIBghJHyEZICR9hGSAkPQRkgFC0kdIBg6Df+WKRX8z1EBI\nBkrvdjs8F8xRRYRkoHQ3vfcHNkMdKUJyvyqYIaQQczTxPhfFe/dThBRjjibeZ+L819BP/HOk\nKHM07T4TYSYp8KpdmDmadJ+LkIcN0zeTFHKOooUU8kR2+maaIs5RuJDcxT+080dIBuIvNvi7\nHs31v6TFgyRY/rbHM5IBQtJHSAYISR8hGeAcSR8hGWDVTh8hGchwHckdIRkgJH2EZIBDO32E\nZIDFBn2EZIDlb32EZOBqSLz6RAIhGeAZSR8hGeAcSR8hGWDVTh8hGeA6kj5CMkBI+gjJAId2\n+gjJAIsN+gjJAMvf+gjJACHpIyQDhKSPkAxwjqSPkAywaqePkAxwHUkfIRkgJH2EZKAMvjO8\nGR6LkAwQkj5CMrBf/r75F3zMUUWEZOBk+ZtnJEmEZKBb/u69M7wZaiAkAz+XYgshiSIkAyev\nDiIkSYRkoD/4V/6tIOaoIkIywAVZfYRkgJD0EZIBQtJHSAYISR8hGSAkfYRkgJD0EZIBQtJH\nSAYISR8hGSAkfYRkgJD0EZIBQtJHSAYISR8hGSAkfYRkgJD0EZIBQtJHSAYISR8hGSAkfYRk\ngJD0EZIBQtJHSAYISR8hGSAkfYRkgJD0EZIBQtJHSAYISR8hGSAkfYRkgJD0EZKBDCFd+aeY\nLRCSgQQhFe/dJyQL8UMq3Y0rQjJASPoIyUD8kDi0wwMkCInFBiwvQ0juCMkAIekjJAOEpI+Q\nDBCSvhQhRTyRnb4ZlpAhpJBLq9M3wxIShBTzYt/0zbAEQjJASPoShBTm0K4c3NgMNWQIKchi\nQzl9d2gzVJEiJHfnB6cDk8EcVURIBghJHyEZICR9hGSAcyR9hGSAVTt9hGSA60j6CMkAIekj\nJAMc2ukjJAMsNugjJAMsf+sjJAOEpI+QDBCSvuvRlB+P2yWc4xxJH89IBli100dIBriOpI+Q\nDBCSPkIywKGdPkIywGKDPkIywPK3PkIycDUkLlFIICQDPCPpIyQDnCPpIyQDrNrpIyQDXEfS\nR0gGCEkfIRng0E4fIRlgsUEfIRlg+VsfIRkgJH2EZICQ9BGSAc6R9BGSAVbt9BGSAa4j6SMk\nA4Skj5AM9AefQztNhGTgsNjQJFS2LDZIIiQD3fJ3OVu/+2Uz1EFIBsrpDSEJIiQDhKSPkAyc\nX5AlJD2EZKCc/ofFBkGEZIDrSPoIyQAh6SMkA4Skj5AMEJK+FCG5/xukhKQvQ0jFe/cJyUGC\nkM7/vtQPIekjJAOEpC9BSBzaYXkZQmKxAYtLEZI7QtJHSAYISR8hGSAkfYRkgJD0EZIBQtJH\nSAYISR8hGSAkfYRkgJD0EZIBQtJHSAYISR8hGSAkfYRkgJD0EZIBQtJHSAYISR8hGSAkfYRk\ngJD0EZIBQtJHSAYISR8hGSAkfYRkgJD0EZIBQtJHSAYISR8hGSAkfYRkgJD0EZIBQtJHSAYI\nSR8hGSAkfYRkgJD0EZIBQtJHSAYISR8hGSAkfYRkgJD0EZKBw+CXgxuboQZCMlB6t9vhuWCO\nKiIkA6W76b0/sBnqICQDhKSPkAwQkj5CMsA5kj5CMsCqnT5CMsB1JH2EZICQ9BGSAQ7t9BGS\nARYb9BGSAZa/9RGSAULSlyKk4dMKD4SkL0NIxXv3OUdykCCk88dzP6za6bseTfnxuF2aW5iQ\nZtoMS0jwjLT1fhzYEpIDQjKQ4dDOfIoyhBTl0C70YkOQBaFp95kIElLo5e8gczTxPhcxHu2G\nQmJBSEKGkGIcf4d+RgryYDfxPhshQop9jhRkjqbd5yLKo13oVTt3CUKKefw9fTMsgZAMEJK+\nBCFxaIflZQgpyIls7MUGdylCcpdg+dseIRkgJH2EZICQ9BGSAc6R9BGSAVbt9BGSAa4j6SMk\nA4Skj5AM/Bzanbw/sBlqICQDP4sN5drrnZijigjJQL+esiUkRYRk4ORpaPiVg8xRRSlCCvFa\nu95lJELSkyGkIK/+/imJkPQkCCnM3yPdemmD9c/ojpAMcB1JX4KQohzazbUZlpAhpBiLDbNt\nhiWkCMkdIekjJAOEpI+QDBCSPkIyQEj6CMkAIekjJAOEpI+QDBCSPkIyQEj6CMkAIekjJAOE\npI+QDBCSPkIyQEj6CMkAIekjJAOEpI+QDBCSPkIyQEj6CMkAIekjJAOEpI+QDBCSPkIyQEj6\nCMkAIekjJAOEpI+QDBCSPkIyQEj6CMkAIekjJAOEpI+QDBCSPkIyQEj6CMkAIekjJAOEpI+Q\nDBCSPkIyQEj6CMkAIekjJAOEpI+QDBCSPkIyQEj6CMkAIekjJAOEpC9FSMV677vBLwc3NkMN\nGUIq3rt/3Pty+u7QZqgiQUilu3F1/hMM/DDWP6M7QjJASPoShBTk0I6QpGUIKchiA+dIylKE\n5I5VO32EZCDDdaQYRw3T7sODJAgpxnnsxPvwIPEP7YKsrE68Dw8Sf7GhNId2xvtPSBYSLH9z\naOcgxIls6JB4RnJw7bzCQoaQttb7nyKkpiLvkuKfI3FoZyDMYUPgVTv3h7ocIQV5RpprMywh\nRUjuJ0mEpI+QDGQ4tHN3PZry43G7NLcoh3ahFxvsZXhGirHYEHr521+GkGI8Iw2FFOKowV+C\nkLggi+VlCMn8+YhzJAcZQorxjMSqnbQEIQU5R5ptMywhQ0jdjStC0pchJPsVrQyHdt4zlCKk\nKOdIoRcbePW3viDPSKGXv2MefkcLiVc2yCMkAzwjGeDQTl+QkGKfI7HYoC9KSKFX7XhG0scF\nWX1BzmMn3mcizDPSTJspivlgR0hiMoQUcY4ISQwh6csQUozj794f8MVb/ubQzoH7g9358vf1\nzSwFebCbeJ+JMIcNt34G45+RZyQDcSbpxg/h/DO6P9YRkoP4iw2EZCDMod1MmymKOUfBQor5\naDd9M0UxjxoISQwh6UsQUsxJmr6ZIg7tDMS8RjF9M0nuHaUIiWckeTEf7MKF5P5wlyGkrfX+\nE5KF+CFxaGeAkAy4TxEhOYgfEod2BghJHyEZICQDxXv3CclCgpDML1AQkoUMIbkjJAP+IZWZ\n1P45hhGSAf+Q7uC994TkgJD0EZIBQtJHSAYISV+AkHKeyE7fDEsIENJtypHcg5D0pQjJfO8J\nyQAhGUgRkvfeE5IDQtKXIyRzhKSPkAwQkj5CMkBI+nKE5L33OUIyR0gGCEkfIRkgJH2EZCBF\nSN57T0gOCElfjpDMEZI+QjJASPoIyQAh6csRkvfe5wjJHCEZICR9hGSAkPSphzTX35HPo9Yg\nzLqZKO+91w+p0vf9lfggSI3VaN57T0hjiA+C1FiN5r33hDSG+CBIjdVo3ntPSGOID4LUWI3m\nvfeENIb4ILAg1Kg1CH+47xEI6f7vK757j6E0CIQ0QHwQxHfvMZQGQSokKbUGYdbNZkdIA9+X\nkIbUGoRZN5sdIQ18X0IaUmsQZt1sdoQ08H2lQqr0fX8lPgjiu/cYSoNASAPEB0F89x5DaRCk\nQpJSaxBm3Wx2tWflVK1B+MN9Prz3npDGqDUIf7jPh/feE9IYtQbhD/f58N57+ZAqfd9fKQ1C\nvJDMEdL9lAaBkMQQ0v2UBoGQxBDS/ZQGIV5I3ntPSCMoDQIhiTns/q11KaXfoWqUBoGQxJTe\n7Xb4p1H6HapGaRAISUzpbnrvD2xWgdToKg2CWUjmV/LucDWk+j/BXBMwj1qDcPW++vuHrfwz\nErYhnpHiEz9HwpaQLIiv2mFLSBbEryNhS0gWCEkfIRkgJH2EZICQ9BGSAULSR0gGCEkfIRkg\nJH2EZICQ9BGSAULSR0gGCEkfIRkgJH13h4SK7pxLVHRnSABGIiRgBoQEzICQgBkQEjADQgJm\nQEjADAgJmAEhATMgJGAGhATMgJCAGRASMANCAmZASMAMCAmYASEBMyAkYAaEBMyAkIAZEBIw\nA0ICZqAY0u//3tFtL7tPfP7LN375yyfHozAP1/8NLCGKuzd10P443J8rxcGoSGEeCOkPJg7a\nZzvcn4/+tnEpzAMh/cHEQXvdHVCU8vrobxuXyjxYTIziLp4O3Oe6lPX+8e2rOfpu3242eV2V\np/fehk+lbEpZHb7C5qWs3rdf67L6OPsyhy/f/qf3dRwe9h5LZR567+62Wb1u9vd9PpX1b9+h\nDsXfnJNxfN0P7Nv2eMzQHjU0E9m+/TOD380Z7u6h8Hv/FVbNR5/bbT5Ov8zZBB6/DiGdU5mH\nn3fbL1ZWm+NXfv7lO1Si+JvTH8ev3QhvNrtB/moe6v5tt/92d+wHftPMylO35XvZffjffkp3\nH37ZfrS3/9pt+l/mfAKPX4eMzqjMQ3fHWxPve5thab/95vI71KL4u9MfyZf2ge+zGaj+R0s7\nFSdbrpsHwU07vs39373b0y9zNoFfvbcf9iNaUJmH7o51+8ax4OPx4el3qEXxd6c/IKvjaLfH\n3Jv3l3V/sPtb7h+PmgP04/292/6XOZvA87dxpDIP3R3l6Oe+i+9Qi+Lvzum09Gbq+Xwce1t+\ndKP88cvw3pg0QvqNyjwQ0kRDj4S7Q+j1v83ABL50o/zyy/DyjDSByjz0Qzq/j5Cu6A/I5UH1\nwASuuglc/TK8l1/mm5BuUZmH7o71fi2wfx8hXdFNRWnHvFvmWTWz8Pr7BH4dT4Ofm40uhrf/\nZdbN1cLN5TE+IZ1RmYfujrfmi3+2L+QjpDv0J/B44aG5UP62f6BrHpYuJvCtWXRt/GtWRy+H\nt/dl3tu3ni4msDko4WWrPSrz0H31zdP+O38T0l1OJnD70VwK319pe2+ufW+aYb6YwP0q0bY9\nVHj6bXj7X+apuTx+MYHfz3980XI0KvPQC+Rt9/VfvreEBMT0W0gFFd05b6jo3pBGhIiZ3RvS\nsnuBawjJACHpIyQDhKSPkAwQkj5CMkBI+gjJACHpIyQDhKSPkAwQkj5CMkBI+gjJACHpSxGS\n+99CZAgp4hxFC6l4736KkELOUbCQSnfjKn5IMefo577rL241EXOSpm+mKOYcBXtGinnYMH0z\nSSHnKFpIIU9kp2+mKeIchQvJXYaQ3BGSAULSR0gGCEkfIRkgJH2EZICQ9BGSAULSR0gGCEkf\nIRkgJH2EZICQ9BGSAULSR0gGCEkfIRkgJH2EZICQ9BGSAULSR0gGCEkfIRkgJH2EZICQ9BGS\nAULSR0gGCEkfIRkgJH0pQor4D2tM3wxLyBBSyH/qafpmWEKCkEp344qQ9BGSAULSlyAkDu2w\nvAwhsdiAxaUIyR0h6SMkA4Skj5AMEJI+QjJASPoIyQAh6SMkA4Skj5AMEJI+QjJASPoIyQAh\n6SMkA4Skj5AMEJI+QjJASPoIyQAh6SMkA4Skj5AMEJI+QjJASPoIyQAh6SMkA4Skj5AMEJK+\nFCHxbzZgaRlCivKvCJWDG5uhhgQhle7GVendbod/Fuuf0d31aMqPx+3S3IKEVM7eH9gMdSR4\nRgpyaEdI0jKEFGOxgZCkpQjJHedI+gjJAKt2+gjJANeR9BGSAULSR0gGOLTTR0gGWGzQR0gG\nWP7WR0gGroYU4tUn/gjJAM9I+gjJAOdI+lKE5H7Qw6qdvgwhxXjR6mybYQkJQgryZxSzbYYl\nEJKBMvjO8GZ4rAQhBTu0IyRJGUKKsdhQbl4wMv8pvaUIyd3J8jfPSJIIyUC3/N17Z3gz1EBI\nBn4uxV493WOOKiIkAyevDiIkSYRkoD/4VxZOmKOKCMkAF2T1EZIBQtJHSAYISR8hGSAkfYRk\ngJD0EZIBQtJHSAYISR8hGSAkfYRkgJD0EZIBQtJHSAYISR8hGSAkfYRkgJD0EZIBQtJHSAYI\nSR8hGRAPqSipNAaE5ICQRqg1CH+4Dw9CSPerNAaE5EA8JGwJyQIh6SMkA4Skj5AMEJI+QjJA\nSPoIyQAh6UsRUoj/G8Vsm2EJGUIq3rtPSA4ShFSaZyTj/SckB9ejEbhi/Hfu+09IDjI8IxX3\nsyRC0pchJA7tsLgMIfGMhMWlCIlzJCwtQ0gc2mFxCULiOhKWlyEk8zMkQnJASAYISV+GkDi0\nw+IShFS6G1eEpI+QDBCSvgQhcWiH5WUIicUGLC5FSO4ISR8hGSAkfYRkgJD0EZIBQtJHSAYI\nSR8hGSAkfYRkgJD0EZIBQtJHSAYOg3/rH3RijioiJAOld7sdngvmqCJCMnD+stuByWCOKiIk\nA4Skj5AMEJI+QjLAOZI+QjLAqp0+QjLAdSR9KULiD/uwtAwhRflT89CHdhEf7IKFFOUfPwm9\n2BDlwW7SfSaChBR6+TvIHE28z0WMR7uhkGL8XxW7G1cZQopx/B36GSnIg93E+/AgCc6RYjzY\nTbwPD5Jh1c4dIRngOpI+QjJASPoIyUB/8Dm000RIBg6LDeWwtBVyscEdIRnolr/L2frdL5uh\nDkIyUE5vCEkQIRkgJH2EZOD8giwh6UkRUpCr5t112euboYYMIYV8Hdf0zbCEBCGV5jHceP8J\nyUGGkIr7wR0h6SMkA4Sk73o0Qf5ojEM7LC3BMxKLDVhehpDMD+wIyUGKkNwRkj5CMkBI+gjJ\nACHpIyQDhKSPkAwQkr4UIbFqh6VlCInrSFhcgpB4ZQOWlyEkXmuHxRGSAULSlyEkDu2wuAQh\nsdiA5SUIiWckLC9FSFvr/SckBwlC4tAOy8sQkvmaHSE5yBASz0hYXIKQOEfC8gjJACHpSxAS\nh3ZYXoaQWGzA4jKExDMSFpcgJM6RHEQ8aiAkMQlCCnnUECykmJM0fTNFMR/sooUU8rBh+maK\nCAkPED+kmEcNhCQmQUghjxoISUyGkNwRkgFC0pcipIiHDdM3wxIyhBTyRHb6ZlhCgpBiLq1O\n3wxLSBES//gJlpYgJA7tsLwEIfGMhOVlCKm4r9sRkj5CMkBI+jKExKEdFpcgpF1H3k9IhGQg\nQUiluJdESPoIyQAh6UsREosNWFqKkII8I5Vy/Uex/hndEZKB0rvdDs+F9c/ojpAMnL/sduCH\nsf4Z3RGSAULSR0gGCEnf9WjKj8ft0tzc959zJAc8Ixlg1U4fIRngOpK+FCFxQRZLyxBSlFd/\nc2gnLENIQZ6RWGxQliKkEOdILH9LyxBSjEO7oZBCXKLwlyCkIH/YxzOStAQh+T9ac46kL0NI\nMQ7tWLWTliGkIKt2c22GJaQIKcah3VybYQkZQgp0aHfy/sBmqCFBSM2+O+9+f7GhnC/gXW6G\nKjKEZH6GdLr8XbaEpChFSO5OnoaGn1+Zo4oChFRmUvvnGHZ6PDd4wif8E8QXIKQ7eO/9xQVZ\nQtJDSAbOF+sihqR8RHAPQjKQ4DpSkJXVaff58N77BCFdW9X3kCMkc4Skj5AMxA+JQzs8QIKQ\nWGyw4L33KUJyR0gGCEkfIRkgJH2EZICQ9BGSAULSlyMkc4Skj5AMEJI+QjJASPpyhOS994Rk\ngJAMEJI+QjJASPoIyQAh6SMkA4SkL0dI5ghJHyEZICR9hGSAkPTlCMl77wnJACEZICR9hGSA\nkPQRkgFC0kdIBghJX46QzBGSPkIyQEj6CMkAIenLEZL33hOSAUIyQEj6rkdj8f+zu4P33hOS\nAZ6RDBCSPkIyQEj6coRkjpD0EZIBQtJHSAYISV+OkLz3npAMEJIBQtJHSAYISR8hGSAkfYRk\ngJD05QjJHCHpIyQDhKSPkAxkCMn8ZdFJQvLe+wwhFe/dJyQL8UMq3Y0rQjJASPrUQypSag3C\nrJtJ4tBuYVKjKz4IUmM1FosNy5Ia3sqDcOt5UWqssiGkEeoOQjl9d2gzVEFII1QdhHL2/sBm\nqIOQRiAkDCGkEQgJQwhpBM6RMISQRmDVDkMIaQTxQZAaq2wIaQTxQZAaq2wIaQQO7TCEkEZg\nsQFDCGkElr8xhJBGkAyp/ovTsSWkUSRDunk3HoGQRuAcCUMIaQRW7TCEkEYQHwSpscpGPiQp\ntQZh1s2wBEIao9YgzLoZlkBIY9QahFk3wxLkQ6r0fX9Vd/n7ZtBSY5UNIY0gsvx9fTNT7peT\nCWmE2svf921mqXjvPiGNUnsQbnx/qbEa5/y1G34IaQTxQZAaq3EIaWlSgys+CFJjNRKHdguT\nGl3xQZAaq7FYbFiW1PCKD4LUWGVDSCOID4LUWGVDSCOID4LUWGVDSCOID4LUWGVDSCOID4LU\nWGVzPRqB12tKqTUIs26GJag/I2FLSA5yhOS994RkgJAMEJI+QjJASPoIyQAh6SMkA4SkL0dI\n5jKExItWsbgEIfFnFFhe/JD4wz4P3ntPSAYIyUD8kDi08+C99xlCYrHBgvfepwjJHSEZICR9\nOUIyR0j6CMkAIekjJAOEpC9HSN57T0gGCMkAIekjJAOEpI+QDBCSPkIyQEj6coRkjpD0EZIB\nQtJHSAYyhMSLVh14732GkPgzCgvee58gJP6wz4P33hOSAUIyED8kDu08eO99hpBYbMDyMoTk\njpAMEJI+QjJASPpyhOS994RkgJAMEJI+QjJASPoIyQAh6SMkA4SkL0dI5ghJHyEZICR9hGSA\nkPTlCMl77wnJACEZyBASL1p14L33GULizygseO99gpD4wz4BZSa1f45hhKQvQEjxxQ+JQzs8\nQIKQWGzA8jKE5I6QDBCSvuvRWJyKx0dI+nhGMkBI+gjJwGHwbx0cMEcVEZKB0rvdDs8Fc1QR\nIRk4v1w5MBnMUUWEZOBqSCwISSAkAzwj6SMkA5wj6SMkA6za6SMkA1xH0nd3SKjozrlERXeG\nBGAkQgJmQEjADAgJmAEhATMgJGAGhATMgJCAGRASMANCAmZASMAMCAmYASEBMyAkYAaEBMyA\nkIAZEBIwA0ICZkBIwAz+AwJVFNNdgkIEAAAAAElFTkSuQmCC",
      "text/plain": [
       "Plot with title \"Loan Amount Term\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "par(mfrow=c(2,2), mar=c(1,1,1,1))\n",
    "boxplot(df$ApplicantIncome,main='Applicant Income')\n",
    "boxplot(df$CoapplicantIncome, main = 'Coapplicant Income')\n",
    "boxplot(df$LoanAmount, main = 'Loan Amount')\n",
    "boxplot(df$Loan_Amount_Term, main = 'Loan Amount Term')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM\njIyampqnp6eysrK9vb3Hx8fQ0NDZ2dnh4eHp6enw8PD////QFLu4AAAACXBIWXMAABJ0AAAS\ndAHeZh94AAAgAElEQVR4nO2d62KqOhBG03tP213f/22PaEEuggQmmWFmrR9utCHgNywI0d2m\nEwDsJmnvAIAHEAlAAEQCEACRAARAJAABEAlAAEQCEACRAARAJAABEAlAAEQCEACRAAQwIdJb\nSul1scW5wd/Dqv5GK8I2vs6FeXr72bBmvHqZ2O2UHuWXU5jvp16zwxbGAO/pynv+qvHqZWG3\nvy/l+l5qkpPvoO1hC6PPR2r5yl43Xr0s7Pb5zPf64LwXrzDq/DtH93k6/byk9Jy9crx6Wdjt\n55R+z4PxZvGc47+3P6lGy13IX2frrgP3n+bm6uX77+fvT+n582+c2L2tbsW/nw7WP32//K3f\nNPl9S0+fzYHzdD0Dn9d4ev+tE4FBPv6i//f88a/595bVMHfqdcGASP+amYZzWE25zvk8Nbm+\nTJfbwvyN3H/aIeFlUNg0uyx/zhSm/Wl//Xbxo7ex124sc3mansKa9HKtSEsvq2Hu1Ou640rb\n7fGZ0n+n/66pNRH+NjX8b7L89/B1efW9Kddz85P/LpVLf682o5A0GSr0f9pb/6dZ/H25FOnc\n5K352fnxv0u7j6bgn9eqhWQQ4yCrSe7Uy4RIl1Pfb3cm+76cul4ny38Pr5ccf1+7Sdn2Rz+n\nfrPZn/bWf2s38HYdl/QeL3t1WeWldhxWGIrUz6r3c+rV7bjSdvu7cLmZbW6UulD7Ad+Wp7H/\nfr693G9763z+p0/ty0/ty73H1FL43Ztl+Nb7WS3nHrRe+sfJVxfBV25hXrvsNhVm+rKhwqhz\nHUK19LNazj1ovfSPk7cugrfMwpwH0S///ZY7w5V/75Z5/5uA+7nM2vWzWs49aL30j5anTqRr\nPo/H3N9/Y+ZpjCsK01t/NOY+DR5Hk1bxON/aXz5Her7cd8xkRb26HdfceMNPe/96SSytngV6\nvij4fZkSzSlMb/3v4SzQafD40ezX96PvAHqm/YZQe/h2WQ1zp17XHVfabsfHJfjTZV70o8nk\nuSnd3wxef7mN7b0r7vU7LE/NqagffTNWnMwtdcu99dvF99Odwvw+X3sPfFlqB92XEV4vq2Hu\n1Ou640rb7bjO1p0un8s+XzJ5S0/dp+P95Tbfr5f2k+7P5tPv36YM/ej/vfbOS9PC9Na/LL58\n3X7Wfzx9nHftLbBH1+8UdN/+vmU1yp16XXZIa8P36V/mB5d8MAn1ajH23inMsaBeLcbeO4U5\nFtSrxdh7pzDHgnq1/L33v/mZyEkA7CD1Hk/mLlAAByF1D73nAJAHIh0Ght+WQaSjwPDbNNwj\nHQROdrZh1u4gIJJtKMpBQCTbUJSjwPDbNAztDgM1sgyTDQACMP0NIMCySOlG1b2CO1AJy6y9\nIlE+bRh+m2btPRLVU4bht23WztpRPGUYfttmbfgUSRmG37ZBpKPA8Ns0/fAZ2pmG4bdl/iYb\nLr9N7MTZ7tBQI0W66e80mr+7127yIje5lhjUgdrUJQ0fMkVa+BkUYfXQjtrUBZGOQubwm9rU\nZfyBLCIZJXf4TW3qMsx7ftyASMrkjhqoTV12fY5EseqBSLZBpKOQOfymNnVBpMOQN/ymNnVB\nJD8gkiKI5AdEUgSR/IBIiiCSHxBJEUTyAyIpgkh+QCRFEMkPiKQIIvkBkRRBJD8gkiKI5AdE\nUgSR/IBIiiCSHxBJEUTyAyIpgkh+QCRFEMkPiKQIIvkBkRRBJD8gkiKI5AdEUgSR/IBIiiCS\nHxBJEUTyAyIpgkh+QCRFEMkPiKQIIvkBkRRBJD8gkiKI5AdEUqTNe9PfJ6VYpkAkRVLv8bSQ\nOyLZB5EUSd1D7/lcu5kXKVYV8kYN1KYuiHQUMkcN1KYuiHQQcmtEberCPdJBQCTbMGt3EBDJ\nNnyOdBS4RzINIh0GZu0s05tsYGh3cBBJkXTnYbbdzIsUywaIpEinz4PgEUmZ3FEDtanLskjp\nxuzKFKsKuaMGalMXrkgHIbdG1KYuV5HOF5zRJ7N32828SLEqkDtqoDZ1aYO+loFZO7NwRbIN\nnyMdhNxRA7WpCyIdh6xRA7Wpy+27doPnc+3uryy8T7ANRFLkNlpIi3OriGQfRFKkb8/SwAGR\n1MkbNVCbugwuQwmR7JI5aqA2dRlWJiGSVXJHDdSmLqMZ1dlvciGSMrmjBmpTl3HeiGSU3FED\ntakLnyMdhcxRA7WpCyIdhrxRA7WpCyL5AZEUQSQ/IJIiiOQHRFIEkfyASIogkh8QSRFE8gMi\nKYJIfkAkRRDJD4ikCCL5AZEUQSQ/IJIiiOQHRFIEkfyASIogkh8QSRFE8gMiKYJIfkAkRRDJ\nD4ikCCL5AZEUQSQ/IJIiiOQHRFIEkfyASIogkh8QSRFE8gMiKYJIfkAkRRDJD4ikCCL5AZEU\nQSQ/IJIiiOQHRFKkzfuPR+3uryy+V7AFRFIk9R5PC7kjkjp5JztqU5fUPfSez7WbeZFi1SDz\nZEdt6oJIByG3RtSmLoh0EBDJNtwjHQREsg2zdkeBeyTT8DnSYWDWzjKI5AdEUoShnR8QSREm\nGw4DQzvLMP19FJhsMM2ySOnG7MoUqwpMf9uGK9JByD3ZUZu6cI90ELgi2YZZu6PAPZJp+Bzp\nMDBrZxlE8gMiKXIb2g2ez7W7v7LwPsE2EEmR22RDGk/g3Wl3/0WKVYW8kx21qUvfnnRCJLtk\nnuyoTV0GlUmIZJbckx21qcvwFJcQySq5JztqU5fxB7KIZJTckx21qcs4b0SySubJjtrUhc+R\nDkPeyY7a1AWR/IBIiiCSHxBJEUTyAyIpgkh+QCRFEMkPiKQIIvkBkRRBJD8gkiKI5AdEUgSR\n/IBIiiCSHxBJEUTyAyIpgkh+QCRFEMkPiKQIIvkBkRRBJD8gkiKI5AdEUgSR/IBIiiCSHxBJ\nEUTyAyIpgkh+QCRFEMkPiKQIIvkBkRRBJD8gkiKI5AdEUgSR/IBIiiCSHxBJEUTyAyIpgkh+\nQCRF2ryz/mL2eGXxvYItIJIiqfd4WsgdkdTJO9lRm7qk7qH3fK7dzIsUqwaZJztqUxdEOgi5\nNaI2dUGkg4BItuEe6SAgkm2YtTsK3COZhs+RDgOzdpZBJD8gkiK9yQaGdgcHkRRJdx5m2828\nSLEqkHuyozZ16fR5EDwiKZN7sqM2dVkWKd2YXZliVSH3ZEdt6sIV6SDknuyoTV2uIp1rMPpk\n9m67mRcpVgW4ItmmDfp6PmPWziy5JztqUxc+RzoOWSc7alMXRPIDIimCSH5AJEVERHr4LTCo\nASIp0s7aPZJhzRWJoumCSIo8mAgatbv/IiJVIPdkh0h1WZs3ImmTebJDpLqslQCR1Mk72SFS\nXSRn7ShaWbJOdohUF0TyAyIpgkh+QCRFEMkPiKQIIvkBkRRBJD8gkiKI5AdEUgSR/IBIiiCS\nHxBJEUTyAyIpgkh+QCRFEMkPiKQIIvkBkRRBJD8gkiKI5AdEUgSR/IBIiiCSHxBJEUTyAyIp\ngkh+QCRFEMkPd0Xidw7WAZH8sHRFojaFQSQ/IJIiiOQHRFIEkfyASIogkh8QSRFE8gMiKYJI\nfkAkRRDJD4ikCCL5AZEUQSQ/IJIio2+SPGp3f+WlNiBFXo2oTV1S7/G0kDciaZNZI2pTl9Q9\n9J7PtZt5kWJVILdG1KYuiHQQEMk2iHQQEMk23CMdBe6RTMOs3WFg1s4yfI7kB0RSBJH8gEiK\nMLQ7DAztLMNkw1FgssE0TH8fBKa/bbMsUroxuzLFqkJujca14bdylYUr0kGQuSJRolJwj3QU\nRO6RKFEpmLU7DBKzdpSoFHyO5AdEUgSR/IBIiiCSHxBJEUTyAyIpcp3+fvB50QmR1MmtESLV\nZTT9/aDd/RcRqQaZNUKkuqwNGJHUyasRItVlrQSIpE9WjRCpLkw2+AGRFEEkPyCSIojkB0RS\nBJH8gEiKIJIfEEkRRPIDIimCSH5AJEUQyQ+IpAgi+QGRFEEkPyCSIojkB0RSBJH8gEiKIJIf\nEEkRRPIDIimCSH5AJEUQyQ+IpAgi+QGRFEEkPyCSIojkB0RSBJH8gEiKIJIfEEkRRPIDIimC\nSH5AJEUQyQ+IpAgi+QGRFEEkPyCSIojkB0RSBJH8gEiKIJIfEEkRRPIDIimCSH5AJEUQyQ+I\npAgi+QGRFGkDXv4zv4h0BBBJkdR7PC0EjUjq5J3sEKkuqXvoPZ9rN/MiItUg82SHSHVBpIOQ\nWyNEqouoSA8HH7AZRLJNiXskqlUARLJNiVk7qlUC7pFMU+JzJKpVBGbtLLOcbEqL9z2IZApE\nUqQ32cDQ7pDMnOwQqS7pzsNsu5kXEakCuSc7RKpLp8+DpBFJmdyTHSLVBZEOQm6NEKkuiHQQ\nEMk2V5HOA+90ezrbbuZFRKoAItmmCzr1KrXQ7t6LiFSB3JMdItVlbbKIpE/WyQ6R6oJIflgj\nEt8rLsQmkUblQCQbZFyRKJE020Qa/otINkAkRRDJD4ikCCL5AZEUQSQ/IJIiiOQHRFIEkfyA\nSIogkh8QSRFE8gMiKYJIfkAkRRDJD4ikSBGR+D6XCoikSMErEsWqDCIpgkh+QCRFEMkPiKQI\nIvkBkRRBJD8gkiKI5IcckZhYFQaR/JB/RaJEYiCSHxBJEUTyAyIpgkh+QCRFEMkPiKQIIvkB\nkRRBJD8gkiKI5AdEUgSR/IBIiiCSHxBJkZIi8T2UuiCSIhWuSFSrEoikCCL5AZEUQSQ/bBCJ\n0bcUiOSH7VckSrQbRPIDIimCSH5AJEUqipRG5O8sLIJIitQU6f7LIAUiKYJIfkAkRRDJD7tF\nYtS9nRoiDcuDSKXYL9K9jmAVilck5h6EQSRFRhf1R+2GT0SGdqOXYY5NNVppzrBrzm35pN7j\naeFwNi5SgOvathrtuSIJhum/Pql76D2fazd+IirSuqjHN1zjc+ny2vc7EW1dqpONNZIQSWAQ\nXm/QuCfnXet2D73nk55HnY+ThfVsKRI1qsvm+qw424Ey1Mg2a++RQBtqZJq1s3agDjWyDGUB\nEACRAARAJAABEAlAAEQCEACRAARAJAABEAlAAEQCEGDXt2RhPdKFo0aCSORdbaXYndUXia6q\ndoVIdTpDJOddIVKdzhDJeVeIVKczRHLeFSLV6QyRnHeFSHU6QyTnXSFSnc4QyXlXiFSnM0Ry\n3hUi1ekMkZx3hUh1OkMk513xXTsAARAJQABEAhAAkQAEQCQAARAJQABEAhAAkQAEQCQAARAJ\nQABEAhAAkQAEQCQAARAJQIANIu34hXrdL+Pr+lhYWNHdwz5Wd7amj7WdSfa1kT01GvWwed8F\niy2aaJnibFtjY5W6VdcsrOgure511Z61rXd2Jvwut7CnRlKhCsYgmqjgUTPexw0rbKpSt+qa\nhTXddW97b2eSeyb8Lrewq0ZCoQrGMDq695Za7KiZ7mT+CjuOAbsiyXR2aJHafZPad7kYEOlO\nByLvQrLmqR0WS1kpVfZt7OrepkhSeyV9phjuYP4K248BsUNMVCTRg19Qym2YEUkqUrlEEWm2\nH6nOHvWRvWeIJBmDTFeiR82k5w0rbD4GBM8sRkUSlXIbVkQSPWQlEpU9aqZd56+w9RhIw8cd\nh2v3J6IQaX4XNq8ssu9ixV7Rw0qRRI+a8e5tWWFPkUSzRaSlXdi8slyoAl0JJyp3ppjuZO4a\n+zzq9bGwsLo/ic7W9LG2M8m+NrKne6lQ5YqdZBOVO2qmveatsvXrJ70/2MlXhLx/RUi02C6/\nIgQAExAJQABEAhAAkQAEQCQAARAJQABEAhAAkQAEQCQAARAJQABEAhAAkQAEQCQAARAJQABE\nAhAAkQAEsCjS1v+V9nZe8XXPht/2rOwPC3VIN/b0WB6Lu7c1tJ1xfz9ZDEMRC3VApB1sDO37\nEvd37c36xUIdEGkHG0N7Pw8oUnqvvVm/WKnDIQpjcReHwX2/pPRyPb/9NKPvy3LT5P0pPX/2\nGj6n9JvS018Pv2/p6fP085Kevkbd9H8lR6+fI5z26mKlDr2n5zZP77/X176f08u9Lehg8cgZ\n5Ph+Dfbj1I4ZLqOGppCX5VsF/zV3uOdT4b9rD0/NT18vbb6G3YwK2PaDSGOs1OH29NJZevpt\ne369swUlLB45/Rx/zgn//p5D/mlOdf+dTv+dX7gG/9tU5blr+ZnOP/7vWtLzj99OX5fH/y5t\n+t2MC9j2g0YjrNShe+GjkffzomG6bP53ugUtLB47/STfLie+7yao/k/TpRSDli/NSfD3km/z\n+r/e47CbUQF/esvV3uIhsFKH7oWX66+K/DO4HR8Ot6CFxWOnH8hTm/ZlzP37+fbSD7vf8no+\nagbo7eu9x343owKOl6HFSh26F1LL7bXJFrSweOwMy9Kr1Os4x17Lry7lrzvxPigaIt3DSh0Q\naSNzZ8LzEPrlv9+ZAr51Kb/diZcr0gas1KEv0vg1RFqgH8h0UD1TwKeugE934p128w+RHmGl\nDt0LL9e5wP5riLRAV4p0ybyb5nlqqvB+v4A/7W3wa9NoEm+/m5fm08Lf6RgfkUZYqUP3wkfT\n+ffli3yItIJ+AdsPHpoPyj+uJ7rmtDQp4Ecz6drwXzM7Oo23183nZel5UsBmUMLXVntYqUPX\n++/zdcv/EGkVgwKevpqPwq+ftH02n33/NjFPCnidJTpdhgrP9+Ltd/PcfDw+KeC/151fWvaG\nlTr0BPk49//274RIAD5BJAABEAlAAEQCEACRAARAJAABEAlAAEQCEACRAARAJAABEAlAAEQC\nEACRAARAJAABEAlAAEQCEACRAARAJAABEAlAAEQ6DP1foADWoCxHIU0WwBBU5SCku4tgBYpy\nEBDJNhTlICCSbeoUhbvk/XCPNMHS9Eslkeptyi+WDhsbpME/uiASHBZEAhAAkWADDO3GIBLk\nw2TDBESCbJj+noJIkM2sSLe/YhQuYESCbFZckcIFjEiQz+N7pHABIxJs4OEALlzAiAQlCBcw\nIkEJwgWMSJBN90e8Gdp1IBJkk4YPs00igUiQTTo9PHDCBWxQpMIf6SHSbhBpij2RSn+PC5F2\ng0hTzIlU/HtciLSbZrgwOutNmlTbGSMgEmzhOvRm1q4DkaAE4QI2JxL3SC4IF7A9kZi180C4\ngA2KVGMrJt6vZ8IFfByRpP7TGCLVIFzA9kS6TgeV+x4XItUgXMAGRTo9OtgRyT7hAjYnUvFP\nzRGpBuECRiQoQbiAEQlKEC5gcyJxj+SCcAHbE6n0L9ZApBqEC9igSGLtFtY28X49Ey5gRIIS\nhAsYkaAE4QJGJChBuIARCUoQLmBEghKECxiRoAThAkYkKEG4gBEJShAuYESCEoQLGJGgBOEC\nRiQoQbiAEQlKEC5gRIIShAsYkaAE4QJGJChBuIARCUoQLmBEghKECxiRoAThAkYkKEG4gBEJ\nShAuYESCEoQLGJGgBOECRiQoQbiAEQlKEC5gRIINFP5tuAcEkSCfNFmYbREFRIJs0t3FuSYx\nQCTIBpGmIBJkg0hTEAny4R5pQlSRFqec4BHM2o2JKtLubmCRcOEaFKnWX+wz8Z69Ei5ceyKV\nHn8jkgAM7caYE6n4jBAi7YfJhgmRRWLGYSNMf0+JLNL+voIyW6N0o/IuqWNOpMr3SCbe+NHg\nijTFnkh1Z+1MvPHDwT3SBIMiibVbWBuR9sGs3ZjjiLRz/N2tiUg1CJerOZH+bpTEz3ZTfxCp\nIOFytSnS8jGOSEZYCC9criZFerBTiGQERLqBSJBNeny/Gi5XRIJ8Hh814XK1J9L5JDf6ZPZu\nu22d312AbNKj8MLlak6k09WlpV1CJAM0NUKkGxZFkmo3XQmRBEmI1AORYCtLn4yHyxWRoATh\nckUkKEG4XBEJShAuV0SCEoTLNZhI7WfxiFSYcLkGE+nuAsgTLldEghKEyxWRoAThckUkKEG4\nXBEJShAuV0SCEoTLFZGgBOFyRSQoQbhcEQlKEC5XRIIShMsVkaAE4XJFJChBuFwRCUoQLldE\nghKEyxWRoAThckUkKEG4XBEJShAuV0SCEoTLFZGgBOFyRSQoQbhcEQlKEC5XRIIShMsVkaAE\n4XJFJChBuFwRCUoQLleDIj3466SIdATC5WpPpDR8OttuS+eIVItwuZoTKY2ez7Xb3Dki1SBc\nrogEJQiXKyJBCcLlak4k7pGOQJkJoSNjTyRm7exT6GR3ZAyKJNZuuhIiiVBq+H1kjiNSurG5\n8/sibe80KIg0xaxIC3skLtLmToOCSFMQCZHy4R5pgjmR0uMhHCKpw6zdGHMirdgjRLJPuDjt\niXRKjw5sRLJPuDgNinRVCZEsw9BujEmRmkVEMgyTDROMinRa+lwHkZRh+nuKVZEk2k1XQiQR\nZkXa+aH5kUEkRMqGK9IUREKkfLhHmoBIiLQBZu3GIBIilSBcnIiESCUIFyciIdJWGNr1QCRE\nyie13z1hsqEFkRApm0toqVucaxIKREKkbHqBIdIfiIRI2SDSFERCpHy6sGT/8+WRQSRE2kB7\n2DDZ0IJIiFSCcHEiEiKVIFyciIRIJQgXJyIhUgnCxYlIiFSCcHEiEiKVIFyciIRIJQgXJyIh\nUgnCxYlIiFSCcHEiEiKVIFyciIRIJQgXJyIhUgnCxYlIiFSCcHEiEiKVIFyciIRIJQgXJyIh\nUgnCxYlIiFSCcHEiEiKVIFyciIRIJQgXp0GRyvyCdkSqSrg47YmUhk9n223pHJFqES5OcyKl\n0fO5dps7R6QahIsTkRCpBOHiRCREKkG4OM2JxD2SC8LFaU8kZu08EC5OgyKJtZuuhEi1CBfn\ncURKNzZ3vkakzVuAHuEStCqS3tAunYzEcWjCJWhPpEJ/VhGRqhIuQXMiXY7j0fzdvXbbO59f\nOA1egR2ES9CkSOOJ8HvtNneOSDUIlyAiIVIJwiVoTqT+9Nliuy2dI1ItwiVoT6TuHyYbDky4\nBA2KJNZuuhIi1SJcgoiESCUIlyAiIdIGynwf8sggEiLlkyYLsy2igEiIlE26uzjXJAaIhEjZ\nINIUREKkbBBpCiIhUj7cI01AJETaALN2YxAJkUoQLkFEQqQShEswhEjdKASRhGBoNyaGSCeu\nSKIw2TABkRApG6a/pyASImUzK9LO3/R0ZBAJkbLhijQFkRApH+6RJiASIm2AWbsxiIRIJQiX\nICIhUgnCJYhIiLSBblTHZMMfiIRI+VwyGsZ2p0UoEAmRsukdNIj0ByIhUjbp9i8i/YFIiJRN\nLytE+gORECmfW1iI9Ed4kfr/w2LYGOZJk4W5BlEIL1J/YdgYdhAuQURCpBKESxCREKkE4RL0\nLlL33co1Ig0bww7CJehepPYhfwF2EC5BgyKJfkUfkXQIl6A9kR7OrCLSAQiXoDmR0uj5XLvc\nPhGpKuESRCREKkG4BBEJkUoQLkFzInGP5IJwCdoTiVk7D4RL0KBIYu26tohUnXAJHkekbb/F\nE5F0CJegQZFEf7EGIukQLkF7Il0O5PEE3p12GX0iUnXCJWhOpN4OIdJxCZegVZGafxHpuIRL\n0KxIp/2/WGPtH+pDJHnCJWhOpJ5Ju0U6bfEHkSQIl6A9kfrXpMV2q/pDJB3CJWhQJNF2iKRD\nuAQRCZFKEC5BREKkEoRLEJEQqQThEkQkRCpBuAQRCZFKEC5BREKkEoRLEJHahcnXIGAH4RJE\nJK5IJQiXICIhUgnCJYhIiFSCcAkiEiKVIFyCiIRIJQiXICIhUgnCJYhIiFSCcAkiEiKVIFyC\niIRIGxD9bbguQCREyidNFmZbRAGRECmbdHdxrkkMEAmRskGkKYiESNkg0hSnInW3wohUAu6R\nJngVqX1ApCIwazcGkRCpBOESRCREKkG4BBEJkTbA0G4MIiFSPkw2TEAkRMom3V2caxIDREKk\nbGZF2vZ3fl2ASIiUDVekKYiESPlwjzTBoUg7/1AfIq2AWbsxHkU67fIHkSQIl6BBkfae7RDJ\nAOEStCfS7vE3IhkgXILmRNo/I4RIBgiXICLdXZgfW8b8lGRI7+OijTXyByItLjzeyZA8fv/h\nEjInkp17JESa52EA4RKyJ5KZWTtEWuBRAuESMijS3naIZIBwCR1HpNVfiEQkA4RL6DgirW6H\nSAYIlxAiIVIJwiVkTqT9n1EgkgHCJWROpBW7Ukukuy4j0irCJWRPpMf7UvGKhEgbCZeQQZEe\n7gwi2SdcQhZF2tkOkQwQLiFEQqQShEsIkRCpBOESQiREKkG4hBAJkUoQLiFEQqQShEsIkRCp\nBOESQiREKkG4hBAJkUoQLiFEQqQShEsIkRCpBOESQiREKkG4hBAJkUoQLiFEQqQShEsIkRCp\nBFoJqf3BQERCpBKoiaS1eURCpBIgkiL7RJL6Q33DhcEvYhlugt+mvwAiKbJTpJOgPzkLcAdE\nUgSR/IBIiiCSHxBJEUTyAyIpIiBSOwEgr818z7c9YPqhA5EUOfoViWvUDURSBJH8gEiKIJIf\nEEkRRPIDIilyUJGm8xB7mX6V4nAgkiJHFWmysJdpr4cDkRRBpNm3cjgQSRFEmn0rhwORFDmQ\nSNPvg88vnKbPVrzBfJFM3VAhkiIHEiln4TR9tuINbhApp3FpEGlvfzv+qy8izb6VrLUs4Eak\ntcezuEg7+kOk2beStZYF/Ii0sj9EQqQSIJLIdiuKNPjP4Aa0GS7c2bs0Hv1Ohw63VZdTGfVz\nt/HMyKT0zAQiiWy3pkinlce06sLcy4NXFt7TQhCPRDrNvoxIqzs8pEgP5ysQqR+Eikh5NaoI\nIp0m+zzbByL1g9AQKbNGub3fsXTtbPDa42/17PJhRUqj5/PbuD0xYksYkbJr1D9qh4fwveP5\n3jG0WhDhdg5FGt6433sZMtlSJGpUl831WXG2A2WokW1GV1FqZBZqZJrhsJkaGYYaWYayAAiA\nSAACIBKAAIgEIAAiAQiASAACIBKAAIgEIAAiAQiw7Qt6sBnpAlKcJYqnfYu9+Eo03tx4EwJb\noItKmzJynLlvvAkbx5+bLopuyshx5r7xJmwcf266KLopI8eZ+8absHH8uemi6KaMHGfuG+0R\nV7UAAAI1SURBVG/CxvHnpouimzJynLlvvAkbx5+bLopuyshx5r7xJmwcf266KLopI8eZ+8ab\nsHH8uemi6KaMHGfuG2/CxvHnpouimzJynLlvvAkbx5+bLgxuCsAviAQgACIBCIBIAAIgEoAA\niAQgACIBCIBIAAIgEoAAiAQgACIBCIBIAAIgEoAAiAQgwAaR1v3evTRqvLBW95v8VjZe33PO\nbtx+o6DwbqS8nvexp+OsYGc2vr6Qs13s3QuBN7KB/C2kVWu1ibaNF9aatpFqnL0befucVjY+\nTdqsy3ADezrOyOrBxlW7EHgjW9ho+6PV0u0tXB4X1pq2edQ43V1r5250r4rvc/eT1Y03s6fj\nnKyWN76zi71R7X4jmygjUjrlvpkyB2W2zyUadz8wLVJ+yZb72d4FIk3bFREp6yKzXqTuPmZN\n41NG47x93sW+jm2ItHcvpM4ImRgRKeMik3EEZ4mUsxvD26PHbzANHx2LtPtykneCur8HiCTb\nuGu3PtOV59NpG0S69aHaxYaii2BDpCKHexo+INKqtfftpMghvD7XpR2IKFLucbZSpO4DHERa\nv/aunZR5n7tE2lB0ESyIlEY9Cx7B63cjq+fM3ch4g/tQFmn3+8wu70I31kW6rLFirdvup8HC\nwi6saJymbZb3J283Vvectxup9++Kfd7Bro7XZ7W0/p4ussu7tCPl055uM28V2a8IZX6DpthX\nhPJ6zms8v5Y0il8REvkqVKCvCAHABEQCEACRAAT4H8J3XW1S4SjvAAAAAElFTkSuQmCC",
      "text/plain": [
       "Plot with title \"Loan Amount Term\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "par(mfrow=c(2,2), mar=c(2,2,2,2))\n",
    "hist(df$ApplicantIncome,main='Applicant Income',breaks=50)\n",
    "hist(df$CoapplicantIncome, main = 'Coapplicant Income',breaks=50)\n",
    "hist(df$LoanAmount, main = 'Loan Amount',breaks=50)\n",
    "hist(df$Loan_Amount_Term, main = 'Loan Amount Term',breaks=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For applicant income, co-applicant income, and loan amount, there are many extreme values. However the histograms show a skewed possibly log-normal distribution consistent with having a lower bound of 0 on possible values for these features. The extreme values are expected at the tail of such distributions, and thus data points will be excluded based on value of these features. Loan Amount Term appears to be 1 year (360 days) for almost all data points, suggesting that the data was entered in years most of the time. Because of this, it cannot be determined if the extreme values are actually out of the ordinary or not, therefore no data points will be excluded based on loan amount term.\n",
    "\n",
    "Next, check for correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>ApplicantIncome</th><th scope=col>CoapplicantIncome</th><th scope=col>LoanAmount</th><th scope=col>Loan_Amount_Term</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>ApplicantIncome</th><td> 1.00000000</td><td>-0.11363997</td><td>0.57129807 </td><td>-0.04734816</td></tr>\n",
       "\t<tr><th scope=row>CoapplicantIncome</th><td>-0.11363997</td><td> 1.00000000</td><td>0.18885511 </td><td>-0.05979733</td></tr>\n",
       "\t<tr><th scope=row>LoanAmount</th><td> 0.57129807</td><td> 0.18885511</td><td>1.00000000 </td><td> 0.03944725</td></tr>\n",
       "\t<tr><th scope=row>Loan_Amount_Term</th><td>-0.04734816</td><td>-0.05979733</td><td>0.03944725 </td><td> 1.00000000</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       "  & ApplicantIncome & CoapplicantIncome & LoanAmount & Loan\\_Amount\\_Term\\\\\n",
       "\\hline\n",
       "\tApplicantIncome &  1.00000000 & -0.11363997 & 0.57129807  & -0.04734816\\\\\n",
       "\tCoapplicantIncome & -0.11363997 &  1.00000000 & 0.18885511  & -0.05979733\\\\\n",
       "\tLoanAmount &  0.57129807 &  0.18885511 & 1.00000000  &  0.03944725\\\\\n",
       "\tLoan\\_Amount\\_Term & -0.04734816 & -0.05979733 & 0.03944725  &  1.00000000\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | ApplicantIncome | CoapplicantIncome | LoanAmount | Loan_Amount_Term | \n",
       "|---|---|---|---|\n",
       "| ApplicantIncome |  1.00000000 | -0.11363997 | 0.57129807  | -0.04734816 | \n",
       "| CoapplicantIncome | -0.11363997 |  1.00000000 | 0.18885511  | -0.05979733 | \n",
       "| LoanAmount |  0.57129807 |  0.18885511 | 1.00000000  |  0.03944725 | \n",
       "| Loan_Amount_Term | -0.04734816 | -0.05979733 | 0.03944725  |  1.00000000 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "                  ApplicantIncome CoapplicantIncome LoanAmount Loan_Amount_Term\n",
       "ApplicantIncome    1.00000000     -0.11363997       0.57129807 -0.04734816     \n",
       "CoapplicantIncome -0.11363997      1.00000000       0.18885511 -0.05979733     \n",
       "LoanAmount         0.57129807      0.18885511       1.00000000  0.03944725     \n",
       "Loan_Amount_Term  -0.04734816     -0.05979733       0.03944725  1.00000000     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cor(df[7:10], use='complete.obs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Gender</th><th scope=col>Married</th><th scope=col>Dependents</th><th scope=col>Education</th><th scope=col>Self_Employed</th><th scope=col>Credit_History</th><th scope=col>Property_Area</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Gender</th><td>0.00</td><td>0.00</td><td>0.00</td><td>0.28</td><td>0.94</td><td>0.82</td><td>0.02</td></tr>\n",
       "\t<tr><th scope=row>Married</th><td>0.00</td><td>0.00</td><td>0.00</td><td>0.80</td><td>1.00</td><td>1.00</td><td>0.99</td></tr>\n",
       "\t<tr><th scope=row>Dependents</th><td>0.00</td><td>0.00</td><td>0.00</td><td>0.47</td><td>0.10</td><td>0.48</td><td>0.31</td></tr>\n",
       "\t<tr><th scope=row>Education</th><td>0.28</td><td>0.80</td><td>0.47</td><td>0.00</td><td>0.88</td><td>0.07</td><td>0.16</td></tr>\n",
       "\t<tr><th scope=row>Self_Employed</th><td>0.94</td><td>1.00</td><td>0.10</td><td>0.88</td><td>0.00</td><td>1.00</td><td>0.75</td></tr>\n",
       "\t<tr><th scope=row>Credit_History</th><td>0.82</td><td>1.00</td><td>0.48</td><td>0.07</td><td>1.00</td><td>0.00</td><td>0.60</td></tr>\n",
       "\t<tr><th scope=row>Property_Area</th><td>0.02</td><td>0.99</td><td>0.31</td><td>0.16</td><td>0.75</td><td>0.60</td><td>0.00</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllll}\n",
       "  & Gender & Married & Dependents & Education & Self\\_Employed & Credit\\_History & Property\\_Area\\\\\n",
       "\\hline\n",
       "\tGender & 0.00 & 0.00 & 0.00 & 0.28 & 0.94 & 0.82 & 0.02\\\\\n",
       "\tMarried & 0.00 & 0.00 & 0.00 & 0.80 & 1.00 & 1.00 & 0.99\\\\\n",
       "\tDependents & 0.00 & 0.00 & 0.00 & 0.47 & 0.10 & 0.48 & 0.31\\\\\n",
       "\tEducation & 0.28 & 0.80 & 0.47 & 0.00 & 0.88 & 0.07 & 0.16\\\\\n",
       "\tSelf\\_Employed & 0.94 & 1.00 & 0.10 & 0.88 & 0.00 & 1.00 & 0.75\\\\\n",
       "\tCredit\\_History & 0.82 & 1.00 & 0.48 & 0.07 & 1.00 & 0.00 & 0.60\\\\\n",
       "\tProperty\\_Area & 0.02 & 0.99 & 0.31 & 0.16 & 0.75 & 0.60 & 0.00\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Gender | Married | Dependents | Education | Self_Employed | Credit_History | Property_Area | \n",
       "|---|---|---|---|---|---|---|\n",
       "| Gender | 0.00 | 0.00 | 0.00 | 0.28 | 0.94 | 0.82 | 0.02 | \n",
       "| Married | 0.00 | 0.00 | 0.00 | 0.80 | 1.00 | 1.00 | 0.99 | \n",
       "| Dependents | 0.00 | 0.00 | 0.00 | 0.47 | 0.10 | 0.48 | 0.31 | \n",
       "| Education | 0.28 | 0.80 | 0.47 | 0.00 | 0.88 | 0.07 | 0.16 | \n",
       "| Self_Employed | 0.94 | 1.00 | 0.10 | 0.88 | 0.00 | 1.00 | 0.75 | \n",
       "| Credit_History | 0.82 | 1.00 | 0.48 | 0.07 | 1.00 | 0.00 | 0.60 | \n",
       "| Property_Area | 0.02 | 0.99 | 0.31 | 0.16 | 0.75 | 0.60 | 0.00 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "               Gender Married Dependents Education Self_Employed Credit_History\n",
       "Gender         0.00   0.00    0.00       0.28      0.94          0.82          \n",
       "Married        0.00   0.00    0.00       0.80      1.00          1.00          \n",
       "Dependents     0.00   0.00    0.00       0.47      0.10          0.48          \n",
       "Education      0.28   0.80    0.47       0.00      0.88          0.07          \n",
       "Self_Employed  0.94   1.00    0.10       0.88      0.00          1.00          \n",
       "Credit_History 0.82   1.00    0.48       0.07      1.00          0.00          \n",
       "Property_Area  0.02   0.99    0.31       0.16      0.75          0.60          \n",
       "               Property_Area\n",
       "Gender         0.02         \n",
       "Married        0.99         \n",
       "Dependents     0.31         \n",
       "Education      0.16         \n",
       "Self_Employed  0.75         \n",
       "Credit_History 0.60         \n",
       "Property_Area  0.00         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chisqfun <- function(a,b){\n",
    "    if(a==b) return(0)\n",
    "    else return(round(chisq.test(table(df[,a],df[,b]))$p.value,2))\n",
    "}\n",
    "matrix(mapply(chisqfun,rep(c(2:6,11:12),7),rep(c(2:6,11:12),rep(7,7))),7,7,dimnames=list(names(df[c(2:6,11:12)]),names(df[c(2:6,11:12)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the continuous variables are strongly correlated with each other, though loan amount is moderately correlated with income. The only factors that are not independent are gender, marital status, and dependents.\n",
    "\n",
    "A count table for these 3 variables is then generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ", ,  = 0\n",
       "\n",
       "        \n",
       "          No Yes\n",
       "  Female  60  20\n",
       "  Male   109 149\n",
       "\n",
       ", ,  = 1\n",
       "\n",
       "        \n",
       "          No Yes\n",
       "  Female  13   6\n",
       "  Male    10  72\n",
       "\n",
       ", ,  = 2\n",
       "\n",
       "        \n",
       "          No Yes\n",
       "  Female   2   5\n",
       "  Male     6  86\n",
       "\n",
       ", ,  = 3+\n",
       "\n",
       "        \n",
       "          No Yes\n",
       "  Female   3   0\n",
       "  Male     3  42\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table(df[,2],df[,3],df[,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though the distribution is heavily skewed, all combinations of the 3 levels are represented except married females with 3+ dependents. Therefore, none of these three factors will be dropped at this time.\n",
    "\n",
    "The imbalance comes apparently from the fact that all members of a family are considered dependents of the father rather than the mother. In addition, from this data set husbands seem to manage finances more than wives, which is why the count of married females is much lower than the count of unmarried females, who manage their own finances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAANlBMVEUAAAAAgP9NTU1oaGh8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\nAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "featurePlot(df[,7:10],df[,13],plot='pairs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bar plots of % of loans approved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEUAAABNTU1oaGh8fHyM\njIyampqnp6eysrK9vb2+vr7Hx8fQ0NDZ2dnh4eHp6enw8PD////ojgWfAAAACXBIWXMAABJ0\nAAASdAHeZh94AAAgAElEQVR4nO2di3qjuBIG5STOzNmJE97/aY9vgMASQk0jpE7VfptxjJH6\nRypuyTCuA4DNuKMLALAAIgEogEgACiASgAKIBKAAIgEogEgACiASgAKIBKAAIgEogEgACiAS\ngAKIBKAAIgEogEgACiASgAKIBKAAIgEooCbS5ezc6XxZ7Ksha51z39c/vl206MCCigK6O+//\n8tcLvn352FyRFq5nfGN5hXvtuw+NVvufz3RfS33VM8+SXKP8d/3jv7ZFcu4ze72ct48gW6T7\n8lZEurjTded3eXdvS33VMxpJbrvz6x/vcZGCK+1WTy6PUq77gcVzhNh6a98+gpdSVom0O0qd\nnN39JOLycRu3rzf3djsyXUfx/Nwn3t+6JxoX/jstWXcwzp3vO7Lzvei/p+v3333R/dduTOMF\nrINnKX/dufO3+bTeLjlGz4XPI8DnyZ1yD3Hq+Bt5KPnx5qP4687v43Za/hy0R+33Rd/nxzD6\nqdXK0mnm5H6G15d76ZfhIHzu37qFmSys58T7heuZ3XXf8O/61d2m47Paxx/9Vy/NGLASnqX8\nuNPrgFxyxui+8LHgQ3SyuFOyG2PJo0iXPk8/aKNIP6fby9OPn1qvLKVmvHbO1zG43Iq8Vvpz\ny3N76/N+GTVZ+Kf7iTd4NM79XKs8X7+6227i63bboS+6/+qlGQNWQl/KbJvfqzyvH6N+4fM6\n46c7/qjrXSONJY8i3d67zAbtuehzGKkxmF5ZSs08inWPBHfxHxv+sej+6iewsFqutb6duv4M\nrvv68z7k8HJN0/wcPslGfJECVeaM0TAZr+dRf7JvA6rjiTSWPIrkjcEwaF2/HX6ex+gxmF5Z\nOs28uZ9BpMDx1n81WVgt1/L+uM/roedW59fpJVEwTUWZ/FO74ICsH6Ph1ffbza+jVfI2crj4\n58LpoMWy6pWl08zzZsM0yljuabbr6KqadCGu5V3PYq5nB7c639yfy898ACZpTpUekf49ztCG\nN8dd8uRzi2PkvfX993xb+1C8jTwreVr8dNDmR6RZSwpl6TTzuP39PZxsf93ug09PXPvz78nC\nannumfsX391/IZGGNJVeI/073e4ZeNv8w7tSWDtG3mR8XnUcileAf430t/s5z4r3B+32xb9G\nmrWkUJZSO88fyL796++bfPnlzu8IfTUh0rnfnd9vV932Z3ORhjQV3rV7cLvH5m3zO995Y/TY\nmd9vvfQtHol3jTSWfJ6999cbtNPjvuN1P+/dteuD6ZWl1dDtxvzb3/vLrzd3uv1agFfu5GcU\n3sJquZX33+2XG+51Xk9oPr+vU2gu0pCmwp8j3Xj+itC4zS9v7v0yfavrEmN0+3o9tH1U83Ok\ncac1lnzdkb9fhuLv87AftHvtj4u88edIXVerSFA/FYluDjbtLwKR9oNN+4tApP1g0wIogEgA\nCiASgAKIBKAAIgEogEgACiASgAKIBKAAIgEogEhwEN4vnxrASAxoDvfyomlspHjB1t7OJIjU\nAMYGySTGxqhQCidE2t3LC3UKJ9qb8iUnx6itLVxKpP+JqFmkson2ZiHOXiWnJn5bWxiRpF00\nNcxJDhApVIUvV1tb2KZIBW42tDXMScqLtNQsIkW7OX6jKJ9DV5BIkwNEWhgJRIp2U3ajJPd2\n22lrmJMcckSKqoRI0W7KbpTk3m47bQ1zkoNO7SLjZFWk7Tciyx+Rlvd222lrmJMcdo0U3OeZ\nFWlzpCNO7Zb2dtsxdvuEmw0bMSzS0t5uO8Zu6Ndx+3uoJlHSHltYo+TkhzZH4maDViJE2mkL\na5Sc/NDmSBVtFEQKtl92EFLVJEoqPWfWlpz80OZIFW0URAq2X3YQUtUkSio9Z9aWnPzQ5kgV\nbZQ2RbJ3s2GxmkRJpefM2pKTH9ocqaKN0qhIoRIU5UIkjZKTH9ocqaKN0qZIe98+QSSNkpMf\n2hypoo3SqEg7/64GImmUnPzQ5kgVbZRGRUr+ZtqaXzARxUGk1SUnP7Q5UkUbpVmRpL+ZhkjF\nSk5+aHOkijZKwyLJfjMNkYqVnPzQ5kgVbZSmRYovQ6Tjyh1LTpxMI1KgmcoSIdJx5fYlJ39q\njkiBZipLhEjHlduXjEiSZipLhEjHlduX/AtFau6vKqYTbeuyrsdxIVIsUkUbRWmQ2klUqUhL\ns65RkQzebNh/b1fRMCNSgXJTRA9C/lRsbtoVGKSKhhmRIuVuP4HPjOOenUaXb4+0uYX8VJMX\nrSdKpk2JlJg3KZGUp11nboyc97+RSJ25QVqTNinS8vKkSPqJbJ1+I1IbiZJp2xMplKTdCwqb\nIhnb26XCLtdTq0i2LigmEtkRKZS03b1dKtlyPZWKZGz3PbmSjC5vKlJftq1EibCIFKinuEip\n5W1F6swN0r0/8c4OkeQt5MVJLm8rUmdukPwg+YkqFsnQBQUitZNo+mJ1okpFMnZBYVckQ3u7\nzqRIqbRtjZFNkYzt7TpEWg8iaUWymcjczYbFsImSlkAkrUi/JtHKn4whkryFzJJtRbKZ6H7F\nxxEpDSJpRTKZaMN9SESSt5BZsq1IJhMh0loQSSuSyUSItBZE0opkMtEgESIlQCStSDYTcft7\nJYikFYlEoS4RSdJCZsm2IpEo1CUiSVrILdnaL9SkE/+iRIgkbyGvZOd/036kFYl/VSJEkreQ\nVzIimU6ESPIW8kpGJNOJahXJ1gUFIplPVKlIxmad0ZsN9hIthV2uB5HkLQjizN9v+uFVxgZp\nTVpEeq3nkFM7U/tvY4O0Ji0ivdZzhEjOTqTO3CCtSducSMZOvxGpjUTJtO2JFErS7gWFTZGM\n7e1SYZfrqVQkYxcU7pHGmkihpO3u7VLJluupWCRDs+650n1+Gdk3dOb2dn0aU3+NwqZIy8vb\nitSZGyQ/SH4iRJK3kBcnubytSJ25QepMimTrggKR2kk0fbE6UaUidbYuKIyKZGtv1xkVaTlt\nW2NkU6TO1t7umcfWzYbFsImSlkAkrUi/JhGPLE7Ug0ibIplM9KiaI1IaRNKKZDLRhtsniCRv\nIbNkW5FMJkKktSCSViSTiRBpLYikFclkog039BFJ3kJmybYiWU0kvaGPSPIWMku2FYlEoS4R\nSdJCZsm2IpEo1CUiSVrILNlWJBKFukQkSQuZJduKRKJQl4gkaSG3ZP5ituFEiCRvIa9k53/T\nfqQViX9VIkSSt5BXMiKZToRI8hbySkYk04lqFcnWBYVRkWwNUirscj2VimRs1tm82WBskNak\nRaTXemq4/d32U+CMDdKatIj0Ws9BIgXXbzBSZ26Q1qRFpNd6il8jufi5XYOROnODtCZtcyIZ\nu6B4XiMNLw1EeiSyNEjbEtUq0kLIBsfI9QlsiRRK0u4gdZuOsXWLZGXWjQMU3Nm1GCkQrv1E\n9kQydkFh9GaDrUHqLIpk7IIitVKDke79mRqkzqRIti4ojIpka5AeeSzebLBzQWFWJEuDFA/Y\n/pNWrYyRYZHsDNK2RHWLFI/Z1hiZFslQog23TxBJ3kJmybYi2Uwkv32CSPIWMku2FclqIunt\nE0SSt5BZsq1IdhPJbp8gkryFzJJtRbKcCJGSIJJWJBKFukQkSQuZJduKRKJQl4gkaSGzZFuR\nSBTqEpEkLWSWbCsSiUJdIpKkhcySbUUiUahLRJK0kFmyrUgkCnWJSJIWMku2FYlEoS4RSdJC\nbsnmnnCQTPyLEiGSvIW8kp3/TfuRViT+VYkQSd5CXsmIZDpRrSLZOg8yKpKtQUqFXa6nUpGM\nzTqbItlLlEyLSK/1cLNhU6TO3CCtSYtIr/XUcPv71z5OsdJEybSI9FrPEUekjiNS1YkMPkXI\n1nmQ8/5n2tWbaMOuoVaRQinbPQ8yKlIoabuD1JkUydZ5kFGRbA1SZ1EkY7NuiGMnUmdukDpE\nWs+hNxu81+1H6swN0r0/azcbjI1RaqUGI3XmBimWsulHFhs7D7IrUixem4k2XPVVKpKx8yCb\nIhkbpG7TMbZWkRJpGxsjoyKZS4RIa0EkrUgmEyHSWhBJK5LJRBuu+hBJ3kJmybYi2Uwkv+pD\nJHkLmSXbikSiUJeIJGkhs2RbkUgU6hKRJC1klmwrEolCXSKSpIXMkm1FIlGoS0SStJBZsq1I\nJAp1iUiSFjJLthWJRKEuEUnSQmbJtiKRKNQlIklayCzZViQShbpEJEkLuSVbe8JBOvEvSoRI\n8hbySnb+N+1HWpH4VyVCJHkLeSUbFIlj7LzL+kSyNUY2RbKXKJm2OZGMjREitZEomRaRXuvh\nZsOmSJ25Qbr3Z/HhJ9MXqxMtcqRIofd/6+MU7SVCJHkLmXEGYUxE6hNxjPW7rE4kY2M0XCNN\nLpaajhRL+luPsbWKFErZ7hiN/jhLIpk7xhoUydYYeQciZyTSUHVnZZDu/Zm82WBnjPwkoVFq\nMNKQyNQxNpTSwJNWzYzR5OBqI9IYxNAxdsOJUNUimRmj1EoNRvJSGTvGyk6E6hbJyhjZFMnq\nMVZ0IlSpSMbGyKhI5hJtOBGqVaRU2rbGCJHaSLThRAiR5C1klmwrks1E8hMhRJK3kFmyrUgk\nCnWJSJIWMku2FYlEoS4RSdJCZsm2IpEo1CUiSVrILNlWJBKFukQkSQuZJduKRKJQl4gkaSGz\nZFuRSBTqEpEkLWSWbCsSiUJdIpKkhcySbUUiUahLRJK0kFmyrUgkCnWJSJIWMku2FYlEoS4R\nSdJCZsm2IpEo1CUiSVrILdnU81xWJf5FiRBJ3kJeyc7/pv1I9/7YNcy6rE8kW2NkUyR7iYw+\n/GTyYnWiRRBJKxKJQl0ikqSFvDjGIpEo1CUiSVrIjGPrbLUzN0jbEiGSvAVBnPn77T481i8/\nlri5RPZEMjZGqZUajBRL8lt3DbWKtBCywTH6PSLZS9T2k1YXkyVKWuLImw0uvrtrMNKKxL8q\nESLJW8gt2VSkjl1DoMvqRDI2Rs77Gl3eVqTlQI0m2jDtKhXJ2BhZvUZi1zDrsj6RbI2RVZHs\nJRJPu2pFSmRta4wQyXwiRJK3kFmyrUgkCnWJSJIWMku2FYlEoS4RSdJCZsm2IpEo1CUiSVrI\nLNlWJBKFukQkSQuZJduKRKJQl4gkaSGzZFuRSBTqEpEkLWSWbCsSiUJdIpKkhcySbUUiUahL\nRJK0kFmyrUgkCnWJSJIWMku2FYlEoS4RSdJCZsm2IpEo1CUiSVrILNlWJBKFukQkSQuZJduK\nRKJQl4gkaSGzZFuRSBTqEpEkLWSWbCsSiUJdIpKkhcySbUUiUahLRJK0kFmyrUgkCnWJSJIW\nMku2FYlEoS4RSdJCbsmmHh5LolCX9Ylka4xcX3bnv2g6EolCXVYnkrExiok0fRyukEBz4hay\nU5Fo0mViearkuhItEmhO3EJenKV9Q5uQqH6MJUKkNiBR5aRuNjQKierHViIjMQCOBZEAFEAk\nAAUQCUABRAJQAJEAFEAkAAUQCUABRAJQAJEAFEAkAAUQCUABRAJQAJEAFEAkAAUQCUABRAJQ\nAJEAFEAkAAUQCUCBfJEiz6vIbci9fK2AyTOViq+uRfJ5IsvLl1c/5GEl8ym3tobAgEibWtOV\nyhrZIrluGJUKJt+DWz1binE1RElmSJiyuPrGDSTjxQcXef2y4liu94eoqRRSke4lPp+mdP/q\nMndWT5FE6+7Gc8NveHrosHs47jlT6bm+8Yi1ogVdJs+KfWzae0jvnciKw3i6yT5b0NS6GrPW\n6Kvr3Dhobvh2fceun3fZ6+6G21rMc5IdemTaLFLao/IiuX6Tus6be7NZGFjReStsampFjZI1\nhgPJszT3lDqnmed/gnV3Y9jMm0TatGfbzs4i9bvvvKI20e9u/SOIN/His8cbT7exqTU1Ctbo\nv4yV5O6Ee40k6+6Gjkje6yPYKtLiUnfUqZ2bT7f5ASWyYkyk3KbW1ChZwz8ijdc5WUekfgeQ\nv+5u9Gdm2zan887JDyDd9bIqy+u7A07txsuJ4cKm38iucwuzxxvP/hPSppIlCqMBgAciASiA\nSAAKIBKAAogEoAAiASiASAAKIBKAAogEoAAiASiASAAKIBKAAogEoAAiASiASAAKIBKAAogE\noAAiASiASAAKIBKAAogEoAAiASiwXqSvD+dO5+/Jyte1v96d+/DeGZ79Gu1xVZd7P83q5E6L\nfaf6v3x0+xeZ5t9tUD5/1q/QUDaf9dUsDWyMy0f6MylWF/jvIcjJN+kW73R98+y904ZItzT/\nlvpO9e//GwDH8RkYlGXayTZhdTWLA7u59aU21n7wzf3tup8PT5rQqKwoqQqRzu5zkiS77yom\n28XdBuXy7t5Xr9JMtimrq1kc2M2tL7WxvrPbbu/n3ufXm3v7ehQwO/x4r527fuzj9tmP7+d3\n/Upd932+Hsdubz+Owyc3Nvp4sfM4Xvs9uaHKZ11+hY+ct9PWW+y/p2e57nIt/HN4Qqzz3vI+\n3qebbYMxohLnm0fX7j4ufqdetQ1nm+JNh1DOoczZwI7leWX37c0Cb61w7QdvfT431OXe8yUp\n0o3325eP4ZzvsVL3c7qfkfxcz01ux+F/1zhDo5fkyeF2bh0+uvbr8iu8/f+o5Lp/++umKc6T\nydZ/avj4kG62DYaIWry58epo7HRebZvZpozTIZ7zdiCaD2xfnl92394s8NYKV3/yVtPjuvZ8\n3WCXW92hU7tBA/ew47P76r/rPvuV7i/O9w+8dbcJcRkbPT8+uatIZ/f16GxSl1/h7f9Hgffr\nwK/u+/Hu+ec2gN7pz/jW8PEh3WwbDBG18DfS2KlXbcPZYkmDOfsyZwM7lueX3bc3C7y1wvUf\n/Xe+GfL1uL/gbidlCZHu3//0H/t5nhg+Ru/+3enh0OV2ju812n9yN+49P/bnfl1+hbNoX3/e\nh890s8k2eevGmG66DYaIWvgbaex0Wm2r2WJJYzkfZzr+wE7Lm5QdS7elwqxPf33cjiATV+Yi\nTV96Y9J5r8bv/l53FefbsXgqoEq2KP89O/tvXte0wqGGr9NLbZFg3kaYfMo/l9DLcfJO7cZO\nY9W2lW3K2PJizvnARpNE0m2pcO0Hn4P2slVXi+TvEsYdxHU3ebnvRub7vD1Fen9u7/dpXX6F\nk2hv7s/lJ7DtI8Pxsteef0CL582Gy+1mg3+QH6ptONuU0BEpUPlkYP3yZmWHAm+tcO0HP937\ntZY/tyPS+X7m+ZYp0kfoGul+E+P557PR/a+Rfu4XZo+t69f1EbiOeOb8vu3sEpNt+PjshHz4\nMnxAi8t933t5Gyp/djpU23C2KeN0COZ8fmQ6sH55/lp/u5/zgSI97nvcr5Eu/YuQSPNrpKHW\nO9+P776Hmyi3a8H7jfXLtPU9RfrP/bn/+Xmdhn5dfoW3/x+VXHf6H/dq+9F57uLut3v84Rg+\n/j29RTR8GSKq4f1AduzUq7blbBPGaRXM+SxzOrB+eeNa5+kEHQNvrXD1J38+r/199D/pOf3X\nZYp03XG+X/q3xh8GXI/Qjx8n9o3u/3Okd/fo+fvas1+XX+G9/1tJ95Onszt9fl/3Z+O2/3e6\n7uSnwzF+fPpDi/HLEFGNf+/DrwiNm3SstulsPuO0CuZ8ljkd2El541rXnc/7ZZruHnhrhVsb\nWNtPrKPrkS77VzoUGevaU91jsJytOlS2sHccin8kuupu90zXYHmyWc5WHUeLdHLv63/jcgcs\nTzbL2aqDLQygACIBKIBIAAogEoACRkVK3/0A0MTmVHMvLwB2xeZMQyQojM2ZhkgNYOv020iM\nGYhUP8bGyEaKF2zt7UyCSAAKIFILcESqHkRqAGODZBNbO7tVMZyQvYuPF/zyYtWvqMv6qiN6\nJWVso3jdelttnUj/ExGaxWVUXDoiqYuUiF6ISsrI4Da8kTEuJ5LaVisl0uYWZKkQqVrc8/8S\nYxQvQm2r2RRp6fwbkeoAkSQalBZpa+Kc9koV3kQZ60EkiQaItDeVlLEeRJJogEh7U0kZGXCz\nQaBB6ZsNC7f9EKl+EClWUPGbDRsT53SlWnjrZeiASLGCip/axddEpPpBpFhBXCPtTSVl6IBI\nsYIQaW8qKUMHRIoVhEh7U0kZOiBSrCBE2ptKytABkWIFIdLeVFKGDogUKwiR9qaSMnRApFhB\niLQ3lZShAyLFCkKkvamkDB0QKVYQIu1NJWXo0KpIqb+PikhL7ZUqvIkydGhUJOd/s7q7LA0Q\naXM3kp0dIqU6UttqiLS5vSKFy8aoaZFUn+IR60htqyHS5vaKFP4bRSqQCJHkIFL9NCoSNxs2\ntVekcETaJdH+t78n56SItNRemcJ/382GBkW6n98t/O15RFpqr1Thrz0nd3aIlOpIrQ/n/R99\nngsiLbVXpnDRzg6RUh2p9YFIm9srUrhsjBAp1ZFaH4i0ub0ihSPSLol2uP09uQ8+XY5Ii+0V\nKRyRdklU7vY3IiXbK1K4bGeHSKmO1PpIrYRIyfbKFC7a2SFSqiO1PhBpc3ulCo9XUEcZiiBS\nrKDSIv2qf9blt4mk+NusiLQyVeyKQpHjZ/CvE0kvKiKtTIVITYJIsYIQaUcQSRwVkVamQqQm\nQaRYQdxs2BFEEkdFJEHSff6ycgUzGJHEUREpVXa5f1bx+BmMSOKoiJSu2kXiIVL9IFKsIETa\nEUQSR0WkdNWIVLlI4t8ezI8a/V0IREpXjUh1i5T8EYWmSHpNJeMsLW9OJG42HFBGJoi0mrpu\nf69NnN1eqcLjFdRRRiYWReIv9m1pr1Th8QrqKCMTgyLJIqVBJM1uzP3FvpI3G/SainbRIdK6\n1fa/8bPY/cuL2bL2RAow+e0TRAoUZECkWH2IJOS+8TgirQCR9LAn0jDz7FwjcbPBW5p9BodI\nMiyKFHo/ebaapkmRYmUcLJK9mw0GRRqOSpHliCRasDvrLs3Tq88PvmUYJLIlUv6+IQ0iKVet\ne0Q6+hhm8/Y3Ii0UWolIshOh/KjHg0ixghBJoWpEql2k2294DvWvjZQGkXSrRqTKReoeLgWn\nGCL9rw6RRDs7RHq2t//gpVZCpP9VIVIn2tkh0rO9/Qfvl4qk+HPX4+cjIomjIpIc4SAhUkEQ\nKVYQIu0IIomjIpIcROpXO7zwKIgUKwiRdgSRxFERSQ4i9asdXngURIoVhEg7gkjiqIgkB5H6\n1Q4vPIp0jA78y2SINO0NkWqg2BghkhxE6lc7vPAoiBQrCJF2BJEqGDxEmvaGSDWASLGCSouk\n/7cvEakgiBQrqLBI7uXFmkRHj4UURKpg8BBp2hsi1UCrIpl7rh0irSjj+MKjNCqSbNqlQSRF\n9J9rh0iIlI5l7WaDfNcQb7JI4SIQKVZQBbe/1z1OEZFqAJFiBRU/InWd8r90gEgFaVQkmzcb\n3FD+2kRHj0Uq0fTF2kTxJosULqJVkULvJ0+E0iCSItxs2GOMdhEpuD4iiRfszrqrvvjqhxWe\npFGRlv4pgnZFml7+rUl09FisDZaXKN5WycLzaFSkzsX23q2KZPD2t2hnh0ilT+1iD/FsVaRU\n4irHYrlsyc4OkcpfI4V334gkX6BeeP7ODpG42SCPtCZxlWORLj13Z4dIh9/+TkdKg0j7FJ+X\nKN5WycLzQKRYQYi0I4hUweAh0rQ3RKoBRIoVhEg70qhINfyIApHkIFK/2rGFu5cXs2WIFCgI\nkXYEkSoYPESa9oZIpUCk1SBSGRCpgsFDpGlviFQMbjasBZHK0KhIoZJKPw4AkeQgUr/a4YXf\nq4i+iUiBghBpR9oUSf4XQ6ocPESa9oZIxRD/xZAqBw+Rpr3ZEyn0j9gt/zN25Qp3iLQGRCqD\ndqKSpUv+YkiVg4dI095anI8Ni2TsiGTuuXbpxLHeWpyPTYsUpFGRnP/N6khpEEmz7HI/vjwe\nRIoVhEgaVU9e7JfoeBApVhAiaVQ9ebFfouNBpFhBiKRR9eTFfomOp1GRuNkw9lbrfESkXcZo\n/9vfbT9EP5m4yrFYLpubDTuMkf4RKf9fE0pzoEg1/Ir+7uz1u9LH06hIw3WSmWukkidC5Y5I\npf7ptONBpFhBiLQR2RhVULgIRIoVhEgbQaRdxojb3+lU+YmOHotUIkSqXKR+cLjZUO98LLlr\nOJ5WRRJFSnOgSKEkpZ8HoItoZ1dD4RIQKVbQMSIFV29UpAUQSX+BrGRRpDQHXiPV8DyAYiCS\n/gJZyaJIaQ69RoqGQ6RqCo+CSLGCyp/aHf88gGIgkv4CWcmiSGkOvkY6+nkAxUAk/QWykkWR\n0nCzoQyIpL9AVrIoUprDRcpMdPRY1JLoeBApVhAi7Qgi6S+QlSyKlAaRyoBI+gtkJYsipUGk\nMiCS/gJZyaJIaRCpDIikv0BWsihSGkQqAyLpL5CVLIqUBpHKgEj6C2QliyKlQaQyIJL+AlnJ\nokhpEKkMiKS/QFgyz7Xre6t2PvI4rj3GSFck53+zOlIaRFKuevJiv0THg0ixghBJo+rJi/0S\nHQ8ixQpCJI2qJy/2S3Q8iBQrCJE0qp682C/R8TQqEjcbxt6qnY/cbNhjjPa//c1D9Dcv2B2e\n/b3fAmHJLvqMA0QSL1AuWzJGNRQuoVGR3OTF2khpEEm76s6SSDU8xHOPmw0OkXQX6FedPUYV\nFJ5I1BVJVPaunUOkeuejbIwqKDyVyKBIXd7jFNMgknLVXe4YVVB4MpEhkcZRyomUBpG0y+6K\nTLsyWBQptRyR6p2POyQqVba5mw3J5YhU8XxsVqRQktL/YggiyUGkSgov+JMxRJJEEic6eixq\nSVSy6g6R0iBSGRoWqdBPxhBJEuneXwUXssVoWaQyPxlDJEkkP9WRg1SMpkUq8pMxRJJE6hCp\nicLd7M/5IkQKFIRIO9KoSAsgUqwgRNoRRNJfICtZFCkNNxvKgEj6C2QliyKlOVCkUJLSPzUv\nBiLpL5CVLIqU5uAjUiQeIlVTeBREihV0xDWSC8dDpGoKj4JIsYIOudngEKnuwqMgUqygY+7a\nHftT82Igkv4CYcnWnmvXr3foT8114bl2e4yRrkjO/2Z1pDRH3myQJDp6LNYEsvM3ZJdApFhB\nFWxd7scAABXeSURBVNz+Tic6eiySVU9e7JfoeBApVhAiaVQ9ebFfouNBpFhBiKRR9eTFfomO\np1GR7N1sSCeO9VbtfORmwx5jtP/tbx6iv3nB7vAQ/f0WyEoWRUqDSGVAJP0FspJFkdIgUhkQ\nSX+BqGTn4ifgiCReoFu1aIwqKFxEoyItHZYQSbxgh7LLJDqeVkVaGCVEEi/Yo+4iiY6nWZEk\nkdIgUhkQSX+BrGRRpDSIVAZE0l8gK1kUKQ0ilQGR9BfIShZFSoNIZUAk/QWykkWR0iBSGRBJ\nf4GsZFGkNIhUBkTSXyArWRQpDSKVAZH0F8hKFkVKg0hlQCT9BbKSRZHSIFIZEEl/gaxkUaQ0\niFQGRNJfICtZFCkNIpUBkfQXyEoWRUqDSGVAJP0FspJFkdIgUhkQSX+BrGRRpDSIVAZE0l8g\nK1kUKQ0ilQGR9BfIShZFSoNIZUAk/QWykkWR0iBSGRBJf4GsZFGkNIhUBkTSXyArWRQpDSKV\nAZH0F8hKFkVKg0hlQCT9BbKSRZHSIFIZEEl/gbBkc8/+Lvik7ELz0d6zv2sYI12RnP/N6khp\njhNJlujosaglURmqGKP9RZo+oF1IoDlxC9mpBIkKLag9UbyIVhMpboOlOEv7hjYhUf0YS4RI\nbUCiykndbGgUEtWPrURGYgAcCyIBKIBIAAogEoACiASgACIBKIBIAAogEoACiASgACIBKIBI\nAAogEoACiASgACIBKIBIAAogEoACiASgACIBKIBIAAogEoACW0SaPIGo+Orru3k+wez5rRve\nH7+fPYHj+a2bvx9pum8u1sprt7LuZoV7bbtpJ9FM7rXKedvJNsbqx0LmzzDJTLRUwqw39+hr\nXphXrvM3dbQQ9/icn9KN7wRGM122nHu/W9Yv5NHwpeuGP71JMS4cVumGWbei6b65eCvzbmez\nbmV348f7hiYad9OMoWpcN387miixXbrxz5ldgkRLJcytmm7N6TqzeOOQvBbSL5ildH6nxUXa\n8KzNwG5EGfdse7K1vD+cJ5Q/fIF5FGv68XoyBrNW/O77b7O7m1fvj7W3U5j2MuskIdJUiUgb\nbviYUqJ5CbMY09du9q5f9qNPf0i6SCHDrJjuBcbOC4vkzR9hA5Oh2YPXMQ1OPvfyajqiS02P\nL2OtdN7eZjaz13cXKdvPlOokNUEGJZKF+tVuTfRSQvg0OB5iYvKsPteFCpmFSXexrnApj7mz\nVSTvzHQHxg057qn8P8ft/jz3ftbzeLVapMlqL60Ep11ud16vQ7vTSG4W7qWa6WeCbc83UDzR\neGXW/y9M9BJvWoI/y8cmQyJ5GTtvCwUK6QufiuQNVIsiKRSy3MVkRzr244nUf/FX6LpUsvmY\nx1sJipTb3aTs7mUGdLOGQtW4RCfzYpYSTXufFpOXaLGEeW/zGmYFzK545ltm3ktQ20NEesq0\noYHuuR/bUEeqC++OTjfspybVe9dpruu8XW1CJO+q1C234rwu+1XzuvN6Hbv3VvM6iWZKXET7\nH1mxXeaTTphouYTZvI6I5J3X+EPSHzUDIo0fHg9dQ4bCIjVBMODq1Isf1Gkl94Oxz2h0ckyi\n9Aqvb754kbWCYI0U5kXadsxdXll74607Lm/sdbGTYxLll/Dyc6TcQvLXSH0869MAEASRABRA\nJAAFEAlAAUQCUACRABRAJAAFEAlAAUQCUACRABRAJAAFEAlAAUQCUACRABRAJAAFEAlAAUQC\nUACRABRAJAAFEAlAgfBDW/L1OrlT9jqXj0mfPdntbOHfh3Onz5/Y4v4hT5NSJ8+jntQ7/dgR\nPDbh29+cdeJVS0b1l6Ik0r/r6P3L7nvycLJDRPp8dHn6jizvVZk99j4mUuG9QIB+I+aYFK1a\nNKq/FCWRzu7TnbP7nndTfB5e7jPu8u7eIx+Y/csLi++G3ijOo4I/7i17nQCiUf2lpET6Pjt3\nvu+v/54er5y7XN/7nK1xup4GPFb9enMf3e3Lt7/68wGWzlv/9fDTf3td++3r/v2/09u8SU3O\njz3398el72zo+/HiecyZlfpyRPq8ngONibzQ1yafZ0enMo71zzh1QyCvmmm2ocDHQTlQpTeq\n023TzwUYSIj0c7pv5Os1xN/75v4YTh4me6p/VzE+72cBj4Xvz8+Oq/siPdePinS5L7k8Pvox\na1KVNzdeHT3aH/q+DCeaaZE+7ss/nx/zQ1+bfGyXf/Ndz054R6RHoGk1frZ+Az+KD1Q5HVVv\n2wxzAQYSIt0P7efb5j25r+77MXPOP7dN6a9wvi683N26zafL7ct9Zz6u7ovUrx87MzpfB+ve\nmnN/up9Zk6pM7bh1NvR9r/ozdo00Xs89/v/pvoaP+aGvTV7up1lvtwlYgL6wf30gv5rPobjZ\nBr659VqlP6qTbTPMBRhIiHS67bJ/Hsf9rz/v/ayZzazHB+679+ejyZ+vxtV9kX660OwcGz31\n5xpeV0OTqkxF+nnp+2edSNcznj//hvb80D/dY3Zeoldh6onuB4svL9Ckmp/HsLxu4NcqJ6M6\n2TbDXICBhEjj/P86eTNnNrP+e06r/4ZZ3/nzb/nVa7+TSTpvUpXT5NQu0nf6rt337Trj9G9S\n5bjW3+tO/Fzq7pdXqD9Q80SvG/i1yvmojqsNcwEGVh+R3tyfy89s4/e8Pzfx+3zWz45IP68z\nNNTv8P7uIj1vNlweNxsmfZ8Wj0jjn4/X33/PQ8r5YfjayKXYz2NeRHo5Ip3CG/i1yvmojqsN\ncwEGEiL5Z9jft31USKSf583W25hNZ72/+t/u57xOpPP9euitgEiX+/728va4iTLpe/kaafzz\n8f9w/di9XhheX7/c5dyNF5H8aj68b+cb+KXKl1H1V3vOBRiIiNQfxL+Hez73mzvjRvU343/X\nS9Ebn9dZOZ314+rnwOnEaXbvr2/0cXfoq4BI/g9kH40PfU/u2s1KnYv0CPf5/Nj37Fbl7T6E\nK3Wv+EWk7/ldu++XDfwIN6/yZVTH1Ya5AAMJkbyfI13PXT6/7/d4Hx8ZP/7+HIDv61nAbNaP\nq1+n7PtlKtK/0+wWat/o15s7/deVEKn7997/ilA/6599T36ONCv15dTu+XOk58dmPzy7nQoV\nutUQEMmv5nrofb/flptu4Ge4WZUvo+ptm34uwADH5/35OVXxizaJ3VAlVbbKBpG8e8AHN1IG\naanO1fGrn8uF91U2NCBVgUirkZZ6cu9V/DbNcuF9lQ0NSFWwvQAUQCQABRAJQAFEAlAAkQAU\nQCQABRAJQAFEAlAAkQAUQCQABRAJQAFEAlAAkQAU2EkkF2Sfvo4hnDDN0XV360o/usbm2Euk\n/wUwNTrBhGlq2AYrSq+hzLZAJCGIBD6IJASRwAeRhCAS+CCSEEQCH0QSgkjg83xynPY9T0Sq\neIYi0g64zjsq6W0+RKp4hiLSDiCSEEQCH0QSgkjgg0hCEAl8uNkgpLRIimOESDsQ22Ibf3sR\nkZRnqOZZAyLtwOTULrJc0mxsdMz8rjEigc9zci8ulzQbFcnKsCES+DyPSFGVECkCIoHPcGoX\nUQmRInCzAXy8a6TgKC1v0egVDyIVmKHSS0xE2oFtNxvyfUEkYXfCZcLSWxyRY0ltMUSKUFok\nxRtCiLQDiCSk/BFJ7YYQIu3AKpEUL4UQSdjd/YvODSFE2oF1ImVrgUjKM1R+Q0hUeosjciyI\nJISbDeCDSEIquP09lKJfeosjciyIJASRwAeRhCAS+CCSkN8h0tLjwRv+1f0dQCQhv0SkYxI1\nCCIJQaSGB28HEEkIIjU8eDuASEIQqeHB2wFEEoJIDQ/eDkx+ly66PLYBESkXRDKK67yBiP0e\nFyK9gkgND94OIJIQRGp48HYAkYQgUsODtwOIJKS0SKnrWN3SESkXbjYIOeqvUcR2drqlI1Iu\nsc0w+U0qRHqlNpHW/FacW106IuXi+i8ckfKoTqT1nSPSDjjvf66RMkCkjYmMYVSk/X/Fv7ab\nDYh0LDZFWtp/a3WRnmJ7R01fx4Y7R6QdmEiESBldpKeYalTRdWy4c0TaAZu3v+2JJDtrCHeO\nSDuQ2gyIFOsiPcU0oyJS5dgUyd7NBkSqHKMihZLoynXE7e/s69hw54i0A0ZFSl2aK/SQnmKq\nUbn9XTc2RUqeCCl0kZ5ipaYdIlUAIkm7SE+xUtMOkSoAkaRdpKdYqWmHSBVgV6TFS3OFLtJT\nrNS0Q6QKsCmS5l+Di/WQnmKlph0iVYBRkVKJFJpJT7FSURGpAhBJ2kx6ipWKikgVgEjSZtJT\nrFRURKoARJI2k55ipaIiUgU8BsHab38vgUiItAOTe8Sxm8WIFGgmPcVKRUWkCkAkaTPpKVYq\nKiJVACJJm0lPMdWo/NJq3SCStJn0FNOMKhujcOeItAPcbJA2k55imlERqXJim8Huk1YRCZF2\nYDi144iU2Ux6imlGRaTKcZ0vU3g5IoWaSU8x1ajcbKgbRJI2k55ie0flAZEVgUjSZtJTTDOq\n7PQ73Dki7cB9cl+HB5Fym0lPMc2osp1duHNE2oF+2952ddxsyGkmPcU0oyJS5aQ2AyLFmklP\nMc2oiFQ5iCRtJj3FNKPKTr/DnSPSDiCStJn0FFOOKjj9DneOSDuASNJm0lOsVFREqgBEkjaT\nnmKloiJSBSCStJn0FCsVFZEqAJGkzaSnWKmoiFQBiCRtJj3FSkVFpApAJGkz6SlWKioiVQAi\nSZtJT7FSURGpAhBJ2kx6ipWKikgVgEjSZtJTrFRURKoARJI2k55ipaIiUgUgkrSZ9BQrFRWR\nKuAxCDxFKL+Z9BQrFRWRKsB13tQKbBNEijWTnmKloiJSBSCStJn0FFONysNP6gaRpM2kp5hm\nVNkYhTtHpB1AJGkz6SmmGRWRKoebDdJm0lNMMyoiVU5sMxR/ZLELIo5l7V81R6TKccGXszeL\niKQ5SMlptx1uNmxNZIvpwSC8HJFCXaSnmGaiYAk8abUentdIw8vw8tgGRKRcNvaeOUbhzhFp\nB/pt6xAps4v0FFNNJDprCHeOSDswTrnwlX2bItm72SA7awh3jkg7YPNmQzDJ1juBs+bSU0w5\nkeCsIdw5Iu1AajM0LVJ82m3niGuk7LOGcOeItAM2RUpeUWyHmw06iaxgU6TkFcV2Krj9PZSy\nth5E2g+jIqWuKLaDSDsmahCzIi1fUWwHkXZM1CCGReKIFOkckXbAtEiixGubSU+xookQ6VgQ\nSdpMeooVTYRIx4JI0mbSU6xoIkQ6FkSSNpOeYkUTIdKxIJK0mfQUK5oIkY4FkaTNpKdY0USI\ndCyIJG0mPcWKJkKkY3kMQgUPP0EkeSlr60Gk/XCdN7UC2wSRYs2kp1jRRIh0LIgkbSY9xVQT\n8fCTukEkaTPpKaaZSDZG4c4RaQcQSdpMeoppJkKkyuFmg7SZ9BTTTIRIlRPbDOWftFpqkBAJ\nkXagPyJ1HJEym0lPMdVE3GyoG+f9zzVSTjPpKbZ3Ip60WhGIJG0mPcVUE4nOGsKdI9IOIJK0\nmfQU00wkG6Nw54i0A9z+ljaTnmKaiRCpcvzB4WZDTjPpKaaZCJEqJ7UZECnWTHqKaSbi9nfl\nIJK0mfQUU00kOmsId45IO4BI0mbSU6xoIkQ6FkSSNpOeYkUTIdKxIJK0mfQUK5oIkY4FkaTN\npKdY0USIdCyIJG0mPcWKJkKkY0EkaTPpKVY0ESIdCyJJm0lPsaKJEOlYEEnaTHqKFU2ESMeC\nSNJm0lOsaCJEOhZEkjaTnmJFEyHSsSCStJn0FCuaCJGOBZGkzaSnWNFEiHQsj0HgKUL5zaSn\nWNFEiHQsrvOmVmCbIFKsmfQUU03Ew0/qBpGkzaSnmGYi2RiFO0ekHTAqUmr/vR1E2pjIGDZF\nSibaDiJtTGQMmzcbEGmpc0TagdhmaPuRxfZE4mZD5QxHpOGb0PLYBkSkXDSnHU9arYjhGmly\nsTRd3p5I9m42yHZ24c4RaQdGf5wlkUJJF+SSNJeeYpqJZDu7cOeItAPe2DhDIqX239s55GZD\n7s4u3Dki7YC/kwvtrNsUKbn/3s4xd+0yd3bhzhFpByb7bVsiLe2/t3PQ7e+8nV24c0TagdRm\naFmkhf33dorfbIiXj0gVYFqk+P57OxXc/h5KWVsPIu2HTZGS++/tINKOiRrEqEgbEq9tJj3F\niiZCpGNBJGkz6SlWNBEiHQsiSZtJT7GiiRDpWBBJ2kx6ihVNhEjHgkjSZtJTrGgiRDoWRJI2\nk55iRRMh0rEgkrSZ9BQrmgiRjgWRpM2kp1jRRIh0LIgkbSY9xYomQqRjQSRpM+kpVjQRIh3L\nYxCsPfwkmVihmfQUK5oIkY7Fdd7UCmwTRIo1k55iqol4+EndIJK0mfQU00wkG6Nw54i0A4gk\nbSY9xTQTIVLlIJK0mfQU00yESJXDzQZpM+kpppkIkSonthnaftKqJHFuM+kpppqImw11w8+R\npM2kp9jeiXjSakUgkrSZ9BQrmgiRjuU+hxceQopIsWbSU6xoIkQ6Fjd8WVoe24CIlIv4ZoNo\nZxfuHJF2wHlfo8sRKdBMeoqpJhLt7MKdI9IOcI0kbSY9xXQTSXZ24c4RaQd+qUhOiNdMeoqV\nTbS+c0Tagd8qUnqCJOpBpB0TNQgi5YBIZRI1CCLlgEhlEjUIIuWASGUSNQgi5YBIZRI1CCLl\ngEhlEjUIIuWASGUSNQgi5YBIZRI1CCLlgEhlEjUIIuWASGUSNQgi5YBIZRI1CCLlgEhlEjUI\nIuWASGUSNQgi5YBIZRI1CCLlgEhlEjUIIuWASGUSNQgi5YBIZRI1yGMQzD1pVeNxisv1lBaJ\nB0TWjeu8o1Jgm7QpkixRmuNEUkmESPsRE2n6FM/Y8wtKLJCmEiRKE2hO3MIBidaXnpFSmMgY\nqSNSm5AICoNIbWAvkTFSNxsahURQFgYGQAFEAlAAkQAUQCQABRAJQAFEAlAAkQAUQCQABRAJ\nQAFEAlAAkQAUQCQABRAJQAFEAlAAkQAUQCQABRAJQAFEAlAAkQAUQCQABZZEcv2zUZ7fDs9F\nu3/z/DJ7HIebLe4m3wRWuH7cvazxaCe2xq4odHbozinwgJTZCMUXRwofnjTFXjfOwrZxcx/6\n993Tl/7P13W8xf43gRUmi8aFbmGNXVHo7dgn/Uyerza+5X8TrQ+R5MS3jZt+GQ9Iw+x+vOMC\n68y0mFv10r/zG3x+36xIBx+ROn9z3o9PoQ3u+gd8zT79PKRNDmvPJqPLocsSaTxcRI9IEZHG\nD77sL4cOnNeamx7aECmnc+efEnTjn+MHZsP3us9y083guRlcDl2OSOOLtEhdv+8a335+s3xE\n6luY7isRKa9zX6SXI1LXH12iIo3LxiYXl0NX5ojkuugK0w97Czm129D5bOO9bvDZOM0/vSjS\n63Lo1ovkum64IhoONtFrJP9GXH+yEVxh/mHnNTIe1EqK1Pxdu25y2Hg9IvX/9sfEjOHTnX+0\nGlcZj0Wh5bC4OYKLXt988SJrBcEaAPWxNEdXzd/XnyPlrSBYA6A6mKMACvwf8w/74vl49CAA\nAAAASUVORK5CYII=",
      "text/plain": [
       "Plot with title \"Property_Area\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "par(mfrow=c(3,4),mar=c(2,2,3,3))\n",
    "for(i in 2:12){\n",
    "    if(i %in% c(2:6,11:12)) {\n",
    "        barplot(table(df[df[,13]=='Y',i])/table(df[,i])*100,beside=TRUE,main=names(df[i]),cex.names=0.7,ylab='% Paid')\n",
    "    } else {\n",
    "        cutdf <- df\n",
    "        cutdf$cutdf <-cut(df[,i],5)\n",
    "        barplot(table(cutdf[cutdf[,13]=='Y',14])/table(cutdf[,14])*100,beside=TRUE,main=names(df[i]),cex.names=0.7,ylab='% Paid')\n",
    "    }      \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no clear grouping of the loan status based on any pairs of continuous features. For categorical features, credit history seems to have a large effect on the loan status."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing\n",
    "\n",
    "Next, create dummy variables and perform pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$lvls):\n",
      "\"variable 'Loan_Status' is not a factor\""
     ]
    }
   ],
   "source": [
    "createdummy <- function(olddf){\n",
    "    dummies <- as.data.frame(predict(dummyVars(Loan_Status~.,df[,c(2:6,11:13)]),olddf[,c(2:6,11:13)]))\n",
    "    newdf <- olddf[,c(7:10)]\n",
    "    newdf$Male <- dummies$Gender.Male\n",
    "    newdf$Marital <- dummies$Married.Yes\n",
    "    newdf$Dependents.0 <- dummies$Dependents.0\n",
    "    newdf$Dependents.1 <- dummies$Dependents.1\n",
    "    newdf$Dependents.2 <- dummies$Dependents.2\n",
    "    newdf$Graduate <- dummies$Education.Graduate\n",
    "    newdf$SelfEmployed <- dummies$Self_Employed.Yes\n",
    "    newdf$CreditHistory <- dummies$Credit_History.1\n",
    "    newdf$Rural <- dummies$Property_Area.Rural\n",
    "    newdf$Urban <- dummies$Property_Area.Urban\n",
    "    return(newdf)\n",
    "}\n",
    "\n",
    "X = createdummy(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>ApplicantIncome</th><th scope=col>CoapplicantIncome</th><th scope=col>LoanAmount</th><th scope=col>Loan_Amount_Term</th><th scope=col>Male</th><th scope=col>Marital</th><th scope=col>Dependents.0</th><th scope=col>Dependents.1</th><th scope=col>Dependents.2</th><th scope=col>Graduate</th><th scope=col>SelfEmployed</th><th scope=col>CreditHistory</th><th scope=col>Rural</th><th scope=col>Urban</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.9574333</td><td>0.0000000</td><td>1.7251291</td><td>5.528221 </td><td>2.565957 </td><td>0.000000 </td><td>2.021797 </td><td>0.000000 </td><td>0.000000 </td><td>2.419031 </td><td>0.0000000</td><td>2.740640 </td><td>0.000000 </td><td>2.126621 </td></tr>\n",
       "\t<tr><td>0.7501995</td><td>0.5153356</td><td>1.4955485</td><td>5.528221 </td><td>2.565957 </td><td>2.096786 </td><td>0.000000 </td><td>2.658188 </td><td>0.000000 </td><td>2.419031 </td><td>0.0000000</td><td>2.740640 </td><td>2.198585 </td><td>0.000000 </td></tr>\n",
       "\t<tr><td>0.4910754</td><td>0.0000000</td><td>0.7711422</td><td>5.528221 </td><td>2.565957 </td><td>2.096786 </td><td>2.021797 </td><td>0.000000 </td><td>0.000000 </td><td>2.419031 </td><td>2.8718227</td><td>2.740640 </td><td>0.000000 </td><td>2.126621 </td></tr>\n",
       "\t<tr><td>0.4228159</td><td>0.8058099</td><td>1.4020768</td><td>5.528221 </td><td>2.565957 </td><td>2.096786 </td><td>2.021797 </td><td>0.000000 </td><td>0.000000 </td><td>0.000000 </td><td>0.0000000</td><td>2.740640 </td><td>0.000000 </td><td>2.126621 </td></tr>\n",
       "\t<tr><td>0.9821508</td><td>0.0000000</td><td>1.6474402</td><td>5.528221 </td><td>2.565957 </td><td>0.000000 </td><td>2.021797 </td><td>0.000000 </td><td>0.000000 </td><td>2.419031 </td><td>0.0000000</td><td>2.740640 </td><td>0.000000 </td><td>2.126621 </td></tr>\n",
       "\t<tr><td>0.8867185</td><td>1.4339179</td><td>3.1196208</td><td>5.528221 </td><td>2.565957 </td><td>2.096786 </td><td>0.000000 </td><td>0.000000 </td><td>2.668632 </td><td>2.419031 </td><td>2.8718227</td><td>2.740640 </td><td>0.000000 </td><td>2.126621 </td></tr>\n",
       "\t<tr><td>0.3818930</td><td>0.5180695</td><td>1.1099774</td><td>5.528221 </td><td>2.565957 </td><td>2.096786 </td><td>2.021797 </td><td>0.000000 </td><td>0.000000 </td><td>0.000000 </td><td>0.0000000</td><td>2.740640 </td><td>0.000000 </td><td>2.126621 </td></tr>\n",
       "\t<tr><td>0.4969683</td><td>0.8557032</td><td>1.8460677</td><td>5.528221 </td><td>2.565957 </td><td>2.096786 </td><td>0.000000 </td><td>0.000000 </td><td>0.000000 </td><td>2.419031 </td><td>0.0000000</td><td>0.000000 </td><td>0.000000 </td><td>0.000000 </td></tr>\n",
       "\t<tr><td>0.6557493</td><td>0.5214868</td><td>1.9629075</td><td>5.528221 </td><td>2.565957 </td><td>2.096786 </td><td>0.000000 </td><td>0.000000 </td><td>2.668632 </td><td>2.419031 </td><td>0.0000000</td><td>2.740640 </td><td>0.000000 </td><td>2.126621 </td></tr>\n",
       "\t<tr><td>2.1019663</td><td>3.7481439</td><td>4.0777066</td><td>5.528221 </td><td>2.565957 </td><td>2.096786 </td><td>0.000000 </td><td>2.658188 </td><td>0.000000 </td><td>2.419031 </td><td>0.0000000</td><td>2.740640 </td><td>0.000000 </td><td>0.000000 </td></tr>\n",
       "\t<tr><td>0.5238137</td><td>0.2392141</td><td>0.8178781</td><td>5.528221 </td><td>2.565957 </td><td>2.096786 </td><td>0.000000 </td><td>0.000000 </td><td>2.668632 </td><td>2.419031 </td><td>0.0000000</td><td>2.740640 </td><td>0.000000 </td><td>2.126621 </td></tr>\n",
       "\t<tr><td>0.4092295</td><td>0.6287915</td><td>1.2735531</td><td>5.528221 </td><td>2.565957 </td><td>2.096786 </td><td>0.000000 </td><td>0.000000 </td><td>2.668632 </td><td>2.419031 </td><td>0.1046593</td><td>2.740640 </td><td>0.000000 </td><td>2.126621 </td></tr>\n",
       "\t<tr><td>0.5030249</td><td>2.7700998</td><td>2.3367946</td><td>5.528221 </td><td>2.565957 </td><td>2.096786 </td><td>0.000000 </td><td>0.000000 </td><td>2.668632 </td><td>2.419031 </td><td>0.0000000</td><td>2.740640 </td><td>0.000000 </td><td>2.126621 </td></tr>\n",
       "\t<tr><td>0.3033209</td><td>0.9705260</td><td>1.3319729</td><td>5.528221 </td><td>2.565957 </td><td>0.000000 </td><td>2.021797 </td><td>0.000000 </td><td>0.000000 </td><td>2.419031 </td><td>0.0000000</td><td>2.740640 </td><td>2.198585 </td><td>0.000000 </td></tr>\n",
       "\t<tr><td>0.2126356</td><td>0.3711237</td><td>0.1986275</td><td>1.842740 </td><td>2.565957 </td><td>2.096786 </td><td>0.000000 </td><td>0.000000 </td><td>2.668632 </td><td>2.419031 </td><td>0.0000000</td><td>2.740640 </td><td>0.000000 </td><td>2.126621 </td></tr>\n",
       "\t<tr><td>0.8102744</td><td>0.0000000</td><td>1.4604966</td><td>5.528221 </td><td>2.565957 </td><td>0.000000 </td><td>2.021797 </td><td>0.000000 </td><td>0.000000 </td><td>2.419031 </td><td>0.0000000</td><td>2.740640 </td><td>0.000000 </td><td>2.126621 </td></tr>\n",
       "\t<tr><td>0.5886357</td><td>0.0000000</td><td>1.1683973</td><td>3.685480 </td><td>2.565957 </td><td>0.000000 </td><td>0.000000 </td><td>2.658188 </td><td>0.000000 </td><td>0.000000 </td><td>0.0000000</td><td>2.062293 </td><td>0.000000 </td><td>2.126621 </td></tr>\n",
       "\t<tr><td>0.5745582</td><td>0.0000000</td><td>0.8879820</td><td>5.528221 </td><td>0.000000 </td><td>0.000000 </td><td>2.021797 </td><td>0.000000 </td><td>0.000000 </td><td>2.419031 </td><td>0.0000000</td><td>0.000000 </td><td>0.000000 </td><td>2.126621 </td></tr>\n",
       "\t<tr><td>0.7999618</td><td>0.0000000</td><td>1.5539684</td><td>5.528221 </td><td>2.565957 </td><td>2.096786 </td><td>2.021797 </td><td>0.000000 </td><td>0.000000 </td><td>0.000000 </td><td>0.0000000</td><td>2.740640 </td><td>2.198585 </td><td>0.000000 </td></tr>\n",
       "\t<tr><td>0.4255987</td><td>1.1960707</td><td>1.3436569</td><td>5.232267 </td><td>2.565957 </td><td>2.096786 </td><td>2.021797 </td><td>0.000000 </td><td>0.000000 </td><td>2.419031 </td><td>0.3102668</td><td>2.740640 </td><td>0.000000 </td><td>2.126621 </td></tr>\n",
       "\t<tr><td>1.2538792</td><td>0.0000000</td><td>1.2151332</td><td>5.528221 </td><td>2.565957 </td><td>2.096786 </td><td>2.021797 </td><td>0.000000 </td><td>0.000000 </td><td>0.000000 </td><td>0.0000000</td><td>0.000000 </td><td>0.000000 </td><td>2.126621 </td></tr>\n",
       "\t<tr><td>0.9747846</td><td>1.9222565</td><td>3.6804515</td><td>5.528221 </td><td>2.565957 </td><td>2.096786 </td><td>0.000000 </td><td>2.658188 </td><td>0.000000 </td><td>2.419031 </td><td>0.0000000</td><td>2.740640 </td><td>0.000000 </td><td>2.126621 </td></tr>\n",
       "\t<tr><td>0.4255987</td><td>0.6530546</td><td>1.3553409</td><td>5.528221 </td><td>2.565957 </td><td>2.096786 </td><td>2.021797 </td><td>0.000000 </td><td>0.000000 </td><td>0.000000 </td><td>0.0000000</td><td>0.000000 </td><td>0.000000 </td><td>0.000000 </td></tr>\n",
       "\t<tr><td>0.5508229</td><td>0.6551050</td><td>1.3086050</td><td>5.528221 </td><td>2.328079 </td><td>2.096786 </td><td>0.000000 </td><td>0.000000 </td><td>2.668632 </td><td>0.000000 </td><td>0.0000000</td><td>0.000000 </td><td>2.198585 </td><td>0.000000 </td></tr>\n",
       "\t<tr><td>0.6084424</td><td>0.9995734</td><td>1.7642799</td><td>5.528221 </td><td>2.565957 </td><td>2.096786 </td><td>0.000000 </td><td>2.658188 </td><td>0.000000 </td><td>2.419031 </td><td>0.4672000</td><td>2.515760 </td><td>0.000000 </td><td>0.000000 </td></tr>\n",
       "\t<tr><td>1.5648936</td><td>0.0000000</td><td>2.2316388</td><td>5.528221 </td><td>2.565957 </td><td>2.096786 </td><td>2.021797 </td><td>0.000000 </td><td>0.000000 </td><td>2.419031 </td><td>2.8718227</td><td>2.740640 </td><td>0.000000 </td><td>0.000000 </td></tr>\n",
       "\t<tr><td>0.4581733</td><td>0.7699278</td><td>1.4254447</td><td>5.528221 </td><td>2.565957 </td><td>2.096786 </td><td>2.021797 </td><td>0.000000 </td><td>0.000000 </td><td>2.419031 </td><td>0.0000000</td><td>2.740640 </td><td>0.000000 </td><td>0.000000 </td></tr>\n",
       "\t<tr><td>0.6917615</td><td>0.3554039</td><td>1.2852370</td><td>5.528221 </td><td>2.565957 </td><td>2.096786 </td><td>0.000000 </td><td>0.000000 </td><td>2.668632 </td><td>0.000000 </td><td>0.0000000</td><td>2.740640 </td><td>0.000000 </td><td>2.126621 </td></tr>\n",
       "\t<tr><td>0.2360436</td><td>0.0000000</td><td>0.4089391</td><td>5.528221 </td><td>2.565957 </td><td>0.000000 </td><td>2.021797 </td><td>0.000000 </td><td>0.000000 </td><td>0.000000 </td><td>0.0000000</td><td>2.740640 </td><td>0.000000 </td><td>2.126621 </td></tr>\n",
       "\t<tr><td>0.6138442</td><td>0.7118329</td><td>1.4020768</td><td>5.528221 </td><td>0.000000 </td><td>0.000000 </td><td>0.000000 </td><td>0.000000 </td><td>2.668632 </td><td>2.419031 </td><td>0.1046593</td><td>2.740640 </td><td>0.000000 </td><td>0.000000 </td></tr>\n",
       "\t<tr><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr>\n",
       "\t<tr><td>0.45620903 </td><td> 0.65510502</td><td>1.7058601  </td><td>5.528221   </td><td>2.565957   </td><td>2.096786   </td><td>0.000000   </td><td>2.658188   </td><td>0.000000   </td><td>2.419031   </td><td>0.0000000  </td><td>0.00000    </td><td>2.198585   </td><td>0.000000   </td></tr>\n",
       "\t<tr><td>0.70109196 </td><td> 1.02520348</td><td>2.0096434  </td><td>1.289918   </td><td>2.565957   </td><td>2.096786   </td><td>0.000000   </td><td>2.658188   </td><td>0.000000   </td><td>2.419031   </td><td>0.0000000  </td><td>2.74064    </td><td>2.198585   </td><td>0.000000   </td></tr>\n",
       "\t<tr><td>0.37600005 </td><td> 0.52011990</td><td>1.2151332  </td><td>5.528221   </td><td>2.565957   </td><td>2.096786   </td><td>2.021797   </td><td>0.000000   </td><td>0.000000   </td><td>2.419031   </td><td>0.0000000  </td><td>2.74064    </td><td>0.000000   </td><td>2.126621   </td></tr>\n",
       "\t<tr><td>0.35439274 </td><td> 0.00000000</td><td>0.8178781  </td><td>5.528221   </td><td>0.000000   </td><td>0.000000   </td><td>2.021797   </td><td>0.000000   </td><td>0.000000   </td><td>0.000000   </td><td>0.0000000  </td><td>2.74064    </td><td>0.000000   </td><td>0.000000   </td></tr>\n",
       "\t<tr><td>0.77753603 </td><td> 0.00000000</td><td>1.0982935  </td><td>5.528221   </td><td>1.584591   </td><td>0.000000   </td><td>2.021797   </td><td>0.000000   </td><td>0.000000   </td><td>2.419031   </td><td>0.0000000  </td><td>2.74064    </td><td>0.000000   </td><td>0.000000   </td></tr>\n",
       "\t<tr><td>0.44622383 </td><td> 0.00000000</td><td>1.2385011  </td><td>5.528221   </td><td>2.565957   </td><td>2.096786   </td><td>0.000000   </td><td>0.000000   </td><td>2.668632   </td><td>2.419031   </td><td>2.8718227  </td><td>0.00000    </td><td>0.000000   </td><td>0.000000   </td></tr>\n",
       "\t<tr><td>0.49107539 </td><td> 1.16736502</td><td>0.6543025  </td><td>2.764110   </td><td>2.565957   </td><td>2.096786   </td><td>2.021797   </td><td>0.000000   </td><td>0.000000   </td><td>2.419031   </td><td>0.0000000  </td><td>2.74064    </td><td>0.000000   </td><td>0.000000   </td></tr>\n",
       "\t<tr><td>0.98215077 </td><td> 0.00000000</td><td>2.3952145  </td><td>3.685480   </td><td>2.565957   </td><td>2.096786   </td><td>0.000000   </td><td>0.000000   </td><td>2.668632   </td><td>2.419031   </td><td>2.8718227  </td><td>2.74064    </td><td>0.000000   </td><td>0.000000   </td></tr>\n",
       "\t<tr><td>1.53166413 </td><td> 0.00000000</td><td>3.4117201  </td><td>5.528221   </td><td>1.514971   </td><td>0.000000   </td><td>0.000000   </td><td>0.000000   </td><td>0.000000   </td><td>2.419031   </td><td>2.8718227  </td><td>2.74064    </td><td>0.000000   </td><td>0.000000   </td></tr>\n",
       "\t<tr><td>0.63168664 </td><td> 1.12772382</td><td>1.6591242  </td><td>2.764110   </td><td>2.565957   </td><td>2.096786   </td><td>2.021797   </td><td>0.000000   </td><td>0.000000   </td><td>2.419031   </td><td>0.0000000  </td><td>2.74064    </td><td>2.198585   </td><td>0.000000   </td></tr>\n",
       "\t<tr><td>2.63871174 </td><td> 0.00000000</td><td>3.0378330  </td><td>5.528221   </td><td>2.565957   </td><td>2.096786   </td><td>2.021797   </td><td>0.000000   </td><td>0.000000   </td><td>2.419031   </td><td>2.8718227  </td><td>2.74064    </td><td>0.000000   </td><td>2.126621   </td></tr>\n",
       "\t<tr><td>0.62743065 </td><td> 0.00000000</td><td>1.2852370  </td><td>5.528221   </td><td>2.565957   </td><td>0.000000   </td><td>2.021797   </td><td>0.000000   </td><td>0.000000   </td><td>0.000000   </td><td>0.0000000  </td><td>2.74064    </td><td>2.198585   </td><td>0.000000   </td></tr>\n",
       "\t<tr><td>1.04484473 </td><td> 0.34173449</td><td>2.1849030  </td><td>5.528221   </td><td>2.565957   </td><td>2.096786   </td><td>0.000000   </td><td>0.000000   </td><td>2.668632   </td><td>0.000000   </td><td>2.8718227  </td><td>2.74064    </td><td>2.198585   </td><td>0.000000   </td></tr>\n",
       "\t<tr><td>0.48894739 </td><td> 0.00000000</td><td>1.0281896  </td><td>5.528221   </td><td>2.565957   </td><td>0.000000   </td><td>1.967588   </td><td>0.000000   </td><td>0.000000   </td><td>2.419031   </td><td>0.0000000  </td><td>0.00000    </td><td>0.000000   </td><td>0.000000   </td></tr>\n",
       "\t<tr><td>1.63086136 </td><td> 0.00000000</td><td>2.1031151  </td><td>5.528221   </td><td>2.565957   </td><td>2.096786   </td><td>2.021797   </td><td>0.000000   </td><td>0.000000   </td><td>2.419031   </td><td>2.8718227  </td><td>2.74064    </td><td>2.198585   </td><td>0.000000   </td></tr>\n",
       "\t<tr><td>0.94613858 </td><td> 0.00000000</td><td>2.2433228  </td><td>5.528221   </td><td>2.565957   </td><td>2.096786   </td><td>0.000000   </td><td>0.000000   </td><td>2.668632   </td><td>2.419031   </td><td>0.0000000  </td><td>2.74064    </td><td>0.000000   </td><td>2.126621   </td></tr>\n",
       "\t<tr><td>0.06809579 </td><td>14.23905108</td><td>4.0893906  </td><td>2.764110   </td><td>0.000000   </td><td>0.000000   </td><td>0.000000   </td><td>0.000000   </td><td>0.000000   </td><td>2.419031   </td><td>1.3759056  </td><td>1.47864    </td><td>0.000000   </td><td>2.126621   </td></tr>\n",
       "\t<tr><td>0.47372406 </td><td> 0.95412270</td><td>1.8110158  </td><td>5.528221   </td><td>2.565957   </td><td>2.096786   </td><td>2.021797   </td><td>0.000000   </td><td>0.000000   </td><td>0.000000   </td><td>0.1009888  </td><td>2.74064    </td><td>2.198585   </td><td>0.000000   </td></tr>\n",
       "\t<tr><td>0.93353431 </td><td> 0.00000000</td><td>1.4955485  </td><td>5.528221   </td><td>2.565957   </td><td>2.096786   </td><td>0.000000   </td><td>0.000000   </td><td>0.000000   </td><td>2.419031   </td><td>0.0000000  </td><td>2.74064    </td><td>0.000000   </td><td>2.126621   </td></tr>\n",
       "\t<tr><td>0.60173104 </td><td> 1.46980005</td><td>2.0096434  </td><td>5.528221   </td><td>2.565957   </td><td>0.000000   </td><td>2.021797   </td><td>0.000000   </td><td>0.000000   </td><td>2.419031   </td><td>0.0000000  </td><td>2.74064    </td><td>2.198585   </td><td>0.000000   </td></tr>\n",
       "\t<tr><td>1.96430155 </td><td> 0.00000000</td><td>5.7952506  </td><td>5.528221   </td><td>0.000000   </td><td>2.096786   </td><td>0.000000   </td><td>2.658188   </td><td>0.000000   </td><td>2.419031   </td><td>0.0000000  </td><td>2.74064    </td><td>0.000000   </td><td>0.000000   </td></tr>\n",
       "\t<tr><td>0.39286031 </td><td> 1.29859107</td><td>1.6721692  </td><td>2.764110   </td><td>2.565957   </td><td>2.096786   </td><td>2.021797   </td><td>0.000000   </td><td>0.000000   </td><td>0.000000   </td><td>0.0000000  </td><td>2.74064    </td><td>0.000000   </td><td>2.126621   </td></tr>\n",
       "\t<tr><td>0.55655210 </td><td> 0.85433623</td><td>2.0213273  </td><td>5.528221   </td><td>2.565957   </td><td>2.096786   </td><td>0.000000   </td><td>2.658188   </td><td>0.000000   </td><td>2.419031   </td><td>0.0000000  </td><td>2.74064    </td><td>0.000000   </td><td>0.000000   </td></tr>\n",
       "\t<tr><td>0.65263919 </td><td> 0.48218737</td><td>1.8343838  </td><td>5.528221   </td><td>2.565957   </td><td>2.096786   </td><td>0.000000   </td><td>0.000000   </td><td>2.668632   </td><td>0.000000   </td><td>0.0000000  </td><td>2.74064    </td><td>2.198585   </td><td>0.000000   </td></tr>\n",
       "\t<tr><td>0.52905188 </td><td> 0.66638226</td><td>1.2618691  </td><td>5.528221   </td><td>2.565957   </td><td>2.096786   </td><td>2.021797   </td><td>0.000000   </td><td>0.000000   </td><td>2.419031   </td><td>0.0000000  </td><td>2.74064    </td><td>2.198585   </td><td>0.000000   </td></tr>\n",
       "\t<tr><td>0.47470621 </td><td> 0.00000000</td><td>0.8295621  </td><td>5.528221   </td><td>0.000000   </td><td>0.000000   </td><td>2.021797   </td><td>0.000000   </td><td>0.000000   </td><td>2.419031   </td><td>0.0000000  </td><td>2.74064    </td><td>2.198585   </td><td>0.000000   </td></tr>\n",
       "\t<tr><td>0.67211851 </td><td> 0.00000000</td><td>0.4673589  </td><td>2.764110   </td><td>2.565957   </td><td>2.096786   </td><td>0.000000   </td><td>0.000000   </td><td>0.000000   </td><td>2.419031   </td><td>0.0000000  </td><td>2.74064    </td><td>2.198585   </td><td>0.000000   </td></tr>\n",
       "\t<tr><td>1.32132017 </td><td> 0.08201628</td><td>2.9560452  </td><td>5.528221   </td><td>2.565957   </td><td>2.096786   </td><td>0.000000   </td><td>2.658188   </td><td>0.000000   </td><td>2.419031   </td><td>0.0000000  </td><td>2.74064    </td><td>0.000000   </td><td>2.126621   </td></tr>\n",
       "\t<tr><td>1.24127488 </td><td> 0.00000000</td><td>2.1849030  </td><td>5.528221   </td><td>2.565957   </td><td>2.096786   </td><td>0.000000   </td><td>0.000000   </td><td>2.668632   </td><td>2.419031   </td><td>0.0000000  </td><td>2.74064    </td><td>0.000000   </td><td>2.126621   </td></tr>\n",
       "\t<tr><td>0.75019950 </td><td> 0.00000000</td><td>1.5539684  </td><td>5.528221   </td><td>0.000000   </td><td>0.000000   </td><td>2.021797   </td><td>0.000000   </td><td>0.000000   </td><td>2.419031   </td><td>2.8718227  </td><td>0.00000    </td><td>0.000000   </td><td>0.000000   </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllllll}\n",
       " ApplicantIncome & CoapplicantIncome & LoanAmount & Loan\\_Amount\\_Term & Male & Marital & Dependents.0 & Dependents.1 & Dependents.2 & Graduate & SelfEmployed & CreditHistory & Rural & Urban\\\\\n",
       "\\hline\n",
       "\t 0.9574333 & 0.0000000 & 1.7251291 & 5.528221  & 2.565957  & 0.000000  & 2.021797  & 0.000000  & 0.000000  & 2.419031  & 0.0000000 & 2.740640  & 0.000000  & 2.126621 \\\\\n",
       "\t 0.7501995 & 0.5153356 & 1.4955485 & 5.528221  & 2.565957  & 2.096786  & 0.000000  & 2.658188  & 0.000000  & 2.419031  & 0.0000000 & 2.740640  & 2.198585  & 0.000000 \\\\\n",
       "\t 0.4910754 & 0.0000000 & 0.7711422 & 5.528221  & 2.565957  & 2.096786  & 2.021797  & 0.000000  & 0.000000  & 2.419031  & 2.8718227 & 2.740640  & 0.000000  & 2.126621 \\\\\n",
       "\t 0.4228159 & 0.8058099 & 1.4020768 & 5.528221  & 2.565957  & 2.096786  & 2.021797  & 0.000000  & 0.000000  & 0.000000  & 0.0000000 & 2.740640  & 0.000000  & 2.126621 \\\\\n",
       "\t 0.9821508 & 0.0000000 & 1.6474402 & 5.528221  & 2.565957  & 0.000000  & 2.021797  & 0.000000  & 0.000000  & 2.419031  & 0.0000000 & 2.740640  & 0.000000  & 2.126621 \\\\\n",
       "\t 0.8867185 & 1.4339179 & 3.1196208 & 5.528221  & 2.565957  & 2.096786  & 0.000000  & 0.000000  & 2.668632  & 2.419031  & 2.8718227 & 2.740640  & 0.000000  & 2.126621 \\\\\n",
       "\t 0.3818930 & 0.5180695 & 1.1099774 & 5.528221  & 2.565957  & 2.096786  & 2.021797  & 0.000000  & 0.000000  & 0.000000  & 0.0000000 & 2.740640  & 0.000000  & 2.126621 \\\\\n",
       "\t 0.4969683 & 0.8557032 & 1.8460677 & 5.528221  & 2.565957  & 2.096786  & 0.000000  & 0.000000  & 0.000000  & 2.419031  & 0.0000000 & 0.000000  & 0.000000  & 0.000000 \\\\\n",
       "\t 0.6557493 & 0.5214868 & 1.9629075 & 5.528221  & 2.565957  & 2.096786  & 0.000000  & 0.000000  & 2.668632  & 2.419031  & 0.0000000 & 2.740640  & 0.000000  & 2.126621 \\\\\n",
       "\t 2.1019663 & 3.7481439 & 4.0777066 & 5.528221  & 2.565957  & 2.096786  & 0.000000  & 2.658188  & 0.000000  & 2.419031  & 0.0000000 & 2.740640  & 0.000000  & 0.000000 \\\\\n",
       "\t 0.5238137 & 0.2392141 & 0.8178781 & 5.528221  & 2.565957  & 2.096786  & 0.000000  & 0.000000  & 2.668632  & 2.419031  & 0.0000000 & 2.740640  & 0.000000  & 2.126621 \\\\\n",
       "\t 0.4092295 & 0.6287915 & 1.2735531 & 5.528221  & 2.565957  & 2.096786  & 0.000000  & 0.000000  & 2.668632  & 2.419031  & 0.1046593 & 2.740640  & 0.000000  & 2.126621 \\\\\n",
       "\t 0.5030249 & 2.7700998 & 2.3367946 & 5.528221  & 2.565957  & 2.096786  & 0.000000  & 0.000000  & 2.668632  & 2.419031  & 0.0000000 & 2.740640  & 0.000000  & 2.126621 \\\\\n",
       "\t 0.3033209 & 0.9705260 & 1.3319729 & 5.528221  & 2.565957  & 0.000000  & 2.021797  & 0.000000  & 0.000000  & 2.419031  & 0.0000000 & 2.740640  & 2.198585  & 0.000000 \\\\\n",
       "\t 0.2126356 & 0.3711237 & 0.1986275 & 1.842740  & 2.565957  & 2.096786  & 0.000000  & 0.000000  & 2.668632  & 2.419031  & 0.0000000 & 2.740640  & 0.000000  & 2.126621 \\\\\n",
       "\t 0.8102744 & 0.0000000 & 1.4604966 & 5.528221  & 2.565957  & 0.000000  & 2.021797  & 0.000000  & 0.000000  & 2.419031  & 0.0000000 & 2.740640  & 0.000000  & 2.126621 \\\\\n",
       "\t 0.5886357 & 0.0000000 & 1.1683973 & 3.685480  & 2.565957  & 0.000000  & 0.000000  & 2.658188  & 0.000000  & 0.000000  & 0.0000000 & 2.062293  & 0.000000  & 2.126621 \\\\\n",
       "\t 0.5745582 & 0.0000000 & 0.8879820 & 5.528221  & 0.000000  & 0.000000  & 2.021797  & 0.000000  & 0.000000  & 2.419031  & 0.0000000 & 0.000000  & 0.000000  & 2.126621 \\\\\n",
       "\t 0.7999618 & 0.0000000 & 1.5539684 & 5.528221  & 2.565957  & 2.096786  & 2.021797  & 0.000000  & 0.000000  & 0.000000  & 0.0000000 & 2.740640  & 2.198585  & 0.000000 \\\\\n",
       "\t 0.4255987 & 1.1960707 & 1.3436569 & 5.232267  & 2.565957  & 2.096786  & 2.021797  & 0.000000  & 0.000000  & 2.419031  & 0.3102668 & 2.740640  & 0.000000  & 2.126621 \\\\\n",
       "\t 1.2538792 & 0.0000000 & 1.2151332 & 5.528221  & 2.565957  & 2.096786  & 2.021797  & 0.000000  & 0.000000  & 0.000000  & 0.0000000 & 0.000000  & 0.000000  & 2.126621 \\\\\n",
       "\t 0.9747846 & 1.9222565 & 3.6804515 & 5.528221  & 2.565957  & 2.096786  & 0.000000  & 2.658188  & 0.000000  & 2.419031  & 0.0000000 & 2.740640  & 0.000000  & 2.126621 \\\\\n",
       "\t 0.4255987 & 0.6530546 & 1.3553409 & 5.528221  & 2.565957  & 2.096786  & 2.021797  & 0.000000  & 0.000000  & 0.000000  & 0.0000000 & 0.000000  & 0.000000  & 0.000000 \\\\\n",
       "\t 0.5508229 & 0.6551050 & 1.3086050 & 5.528221  & 2.328079  & 2.096786  & 0.000000  & 0.000000  & 2.668632  & 0.000000  & 0.0000000 & 0.000000  & 2.198585  & 0.000000 \\\\\n",
       "\t 0.6084424 & 0.9995734 & 1.7642799 & 5.528221  & 2.565957  & 2.096786  & 0.000000  & 2.658188  & 0.000000  & 2.419031  & 0.4672000 & 2.515760  & 0.000000  & 0.000000 \\\\\n",
       "\t 1.5648936 & 0.0000000 & 2.2316388 & 5.528221  & 2.565957  & 2.096786  & 2.021797  & 0.000000  & 0.000000  & 2.419031  & 2.8718227 & 2.740640  & 0.000000  & 0.000000 \\\\\n",
       "\t 0.4581733 & 0.7699278 & 1.4254447 & 5.528221  & 2.565957  & 2.096786  & 2.021797  & 0.000000  & 0.000000  & 2.419031  & 0.0000000 & 2.740640  & 0.000000  & 0.000000 \\\\\n",
       "\t 0.6917615 & 0.3554039 & 1.2852370 & 5.528221  & 2.565957  & 2.096786  & 0.000000  & 0.000000  & 2.668632  & 0.000000  & 0.0000000 & 2.740640  & 0.000000  & 2.126621 \\\\\n",
       "\t 0.2360436 & 0.0000000 & 0.4089391 & 5.528221  & 2.565957  & 0.000000  & 2.021797  & 0.000000  & 0.000000  & 0.000000  & 0.0000000 & 2.740640  & 0.000000  & 2.126621 \\\\\n",
       "\t 0.6138442 & 0.7118329 & 1.4020768 & 5.528221  & 0.000000  & 0.000000  & 0.000000  & 0.000000  & 2.668632  & 2.419031  & 0.1046593 & 2.740640  & 0.000000  & 0.000000 \\\\\n",
       "\t ... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ...\\\\\n",
       "\t 0.45620903  &  0.65510502 & 1.7058601   & 5.528221    & 2.565957    & 2.096786    & 0.000000    & 2.658188    & 0.000000    & 2.419031    & 0.0000000   & 0.00000     & 2.198585    & 0.000000   \\\\\n",
       "\t 0.70109196  &  1.02520348 & 2.0096434   & 1.289918    & 2.565957    & 2.096786    & 0.000000    & 2.658188    & 0.000000    & 2.419031    & 0.0000000   & 2.74064     & 2.198585    & 0.000000   \\\\\n",
       "\t 0.37600005  &  0.52011990 & 1.2151332   & 5.528221    & 2.565957    & 2.096786    & 2.021797    & 0.000000    & 0.000000    & 2.419031    & 0.0000000   & 2.74064     & 0.000000    & 2.126621   \\\\\n",
       "\t 0.35439274  &  0.00000000 & 0.8178781   & 5.528221    & 0.000000    & 0.000000    & 2.021797    & 0.000000    & 0.000000    & 0.000000    & 0.0000000   & 2.74064     & 0.000000    & 0.000000   \\\\\n",
       "\t 0.77753603  &  0.00000000 & 1.0982935   & 5.528221    & 1.584591    & 0.000000    & 2.021797    & 0.000000    & 0.000000    & 2.419031    & 0.0000000   & 2.74064     & 0.000000    & 0.000000   \\\\\n",
       "\t 0.44622383  &  0.00000000 & 1.2385011   & 5.528221    & 2.565957    & 2.096786    & 0.000000    & 0.000000    & 2.668632    & 2.419031    & 2.8718227   & 0.00000     & 0.000000    & 0.000000   \\\\\n",
       "\t 0.49107539  &  1.16736502 & 0.6543025   & 2.764110    & 2.565957    & 2.096786    & 2.021797    & 0.000000    & 0.000000    & 2.419031    & 0.0000000   & 2.74064     & 0.000000    & 0.000000   \\\\\n",
       "\t 0.98215077  &  0.00000000 & 2.3952145   & 3.685480    & 2.565957    & 2.096786    & 0.000000    & 0.000000    & 2.668632    & 2.419031    & 2.8718227   & 2.74064     & 0.000000    & 0.000000   \\\\\n",
       "\t 1.53166413  &  0.00000000 & 3.4117201   & 5.528221    & 1.514971    & 0.000000    & 0.000000    & 0.000000    & 0.000000    & 2.419031    & 2.8718227   & 2.74064     & 0.000000    & 0.000000   \\\\\n",
       "\t 0.63168664  &  1.12772382 & 1.6591242   & 2.764110    & 2.565957    & 2.096786    & 2.021797    & 0.000000    & 0.000000    & 2.419031    & 0.0000000   & 2.74064     & 2.198585    & 0.000000   \\\\\n",
       "\t 2.63871174  &  0.00000000 & 3.0378330   & 5.528221    & 2.565957    & 2.096786    & 2.021797    & 0.000000    & 0.000000    & 2.419031    & 2.8718227   & 2.74064     & 0.000000    & 2.126621   \\\\\n",
       "\t 0.62743065  &  0.00000000 & 1.2852370   & 5.528221    & 2.565957    & 0.000000    & 2.021797    & 0.000000    & 0.000000    & 0.000000    & 0.0000000   & 2.74064     & 2.198585    & 0.000000   \\\\\n",
       "\t 1.04484473  &  0.34173449 & 2.1849030   & 5.528221    & 2.565957    & 2.096786    & 0.000000    & 0.000000    & 2.668632    & 0.000000    & 2.8718227   & 2.74064     & 2.198585    & 0.000000   \\\\\n",
       "\t 0.48894739  &  0.00000000 & 1.0281896   & 5.528221    & 2.565957    & 0.000000    & 1.967588    & 0.000000    & 0.000000    & 2.419031    & 0.0000000   & 0.00000     & 0.000000    & 0.000000   \\\\\n",
       "\t 1.63086136  &  0.00000000 & 2.1031151   & 5.528221    & 2.565957    & 2.096786    & 2.021797    & 0.000000    & 0.000000    & 2.419031    & 2.8718227   & 2.74064     & 2.198585    & 0.000000   \\\\\n",
       "\t 0.94613858  &  0.00000000 & 2.2433228   & 5.528221    & 2.565957    & 2.096786    & 0.000000    & 0.000000    & 2.668632    & 2.419031    & 0.0000000   & 2.74064     & 0.000000    & 2.126621   \\\\\n",
       "\t 0.06809579  & 14.23905108 & 4.0893906   & 2.764110    & 0.000000    & 0.000000    & 0.000000    & 0.000000    & 0.000000    & 2.419031    & 1.3759056   & 1.47864     & 0.000000    & 2.126621   \\\\\n",
       "\t 0.47372406  &  0.95412270 & 1.8110158   & 5.528221    & 2.565957    & 2.096786    & 2.021797    & 0.000000    & 0.000000    & 0.000000    & 0.1009888   & 2.74064     & 2.198585    & 0.000000   \\\\\n",
       "\t 0.93353431  &  0.00000000 & 1.4955485   & 5.528221    & 2.565957    & 2.096786    & 0.000000    & 0.000000    & 0.000000    & 2.419031    & 0.0000000   & 2.74064     & 0.000000    & 2.126621   \\\\\n",
       "\t 0.60173104  &  1.46980005 & 2.0096434   & 5.528221    & 2.565957    & 0.000000    & 2.021797    & 0.000000    & 0.000000    & 2.419031    & 0.0000000   & 2.74064     & 2.198585    & 0.000000   \\\\\n",
       "\t 1.96430155  &  0.00000000 & 5.7952506   & 5.528221    & 0.000000    & 2.096786    & 0.000000    & 2.658188    & 0.000000    & 2.419031    & 0.0000000   & 2.74064     & 0.000000    & 0.000000   \\\\\n",
       "\t 0.39286031  &  1.29859107 & 1.6721692   & 2.764110    & 2.565957    & 2.096786    & 2.021797    & 0.000000    & 0.000000    & 0.000000    & 0.0000000   & 2.74064     & 0.000000    & 2.126621   \\\\\n",
       "\t 0.55655210  &  0.85433623 & 2.0213273   & 5.528221    & 2.565957    & 2.096786    & 0.000000    & 2.658188    & 0.000000    & 2.419031    & 0.0000000   & 2.74064     & 0.000000    & 0.000000   \\\\\n",
       "\t 0.65263919  &  0.48218737 & 1.8343838   & 5.528221    & 2.565957    & 2.096786    & 0.000000    & 0.000000    & 2.668632    & 0.000000    & 0.0000000   & 2.74064     & 2.198585    & 0.000000   \\\\\n",
       "\t 0.52905188  &  0.66638226 & 1.2618691   & 5.528221    & 2.565957    & 2.096786    & 2.021797    & 0.000000    & 0.000000    & 2.419031    & 0.0000000   & 2.74064     & 2.198585    & 0.000000   \\\\\n",
       "\t 0.47470621  &  0.00000000 & 0.8295621   & 5.528221    & 0.000000    & 0.000000    & 2.021797    & 0.000000    & 0.000000    & 2.419031    & 0.0000000   & 2.74064     & 2.198585    & 0.000000   \\\\\n",
       "\t 0.67211851  &  0.00000000 & 0.4673589   & 2.764110    & 2.565957    & 2.096786    & 0.000000    & 0.000000    & 0.000000    & 2.419031    & 0.0000000   & 2.74064     & 2.198585    & 0.000000   \\\\\n",
       "\t 1.32132017  &  0.08201628 & 2.9560452   & 5.528221    & 2.565957    & 2.096786    & 0.000000    & 2.658188    & 0.000000    & 2.419031    & 0.0000000   & 2.74064     & 0.000000    & 2.126621   \\\\\n",
       "\t 1.24127488  &  0.00000000 & 2.1849030   & 5.528221    & 2.565957    & 2.096786    & 0.000000    & 0.000000    & 2.668632    & 2.419031    & 0.0000000   & 2.74064     & 0.000000    & 2.126621   \\\\\n",
       "\t 0.75019950  &  0.00000000 & 1.5539684   & 5.528221    & 0.000000    & 0.000000    & 2.021797    & 0.000000    & 0.000000    & 2.419031    & 2.8718227   & 0.00000     & 0.000000    & 0.000000   \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "ApplicantIncome | CoapplicantIncome | LoanAmount | Loan_Amount_Term | Male | Marital | Dependents.0 | Dependents.1 | Dependents.2 | Graduate | SelfEmployed | CreditHistory | Rural | Urban | \n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 0.9574333 | 0.0000000 | 1.7251291 | 5.528221  | 2.565957  | 0.000000  | 2.021797  | 0.000000  | 0.000000  | 2.419031  | 0.0000000 | 2.740640  | 0.000000  | 2.126621  | \n",
       "| 0.7501995 | 0.5153356 | 1.4955485 | 5.528221  | 2.565957  | 2.096786  | 0.000000  | 2.658188  | 0.000000  | 2.419031  | 0.0000000 | 2.740640  | 2.198585  | 0.000000  | \n",
       "| 0.4910754 | 0.0000000 | 0.7711422 | 5.528221  | 2.565957  | 2.096786  | 2.021797  | 0.000000  | 0.000000  | 2.419031  | 2.8718227 | 2.740640  | 0.000000  | 2.126621  | \n",
       "| 0.4228159 | 0.8058099 | 1.4020768 | 5.528221  | 2.565957  | 2.096786  | 2.021797  | 0.000000  | 0.000000  | 0.000000  | 0.0000000 | 2.740640  | 0.000000  | 2.126621  | \n",
       "| 0.9821508 | 0.0000000 | 1.6474402 | 5.528221  | 2.565957  | 0.000000  | 2.021797  | 0.000000  | 0.000000  | 2.419031  | 0.0000000 | 2.740640  | 0.000000  | 2.126621  | \n",
       "| 0.8867185 | 1.4339179 | 3.1196208 | 5.528221  | 2.565957  | 2.096786  | 0.000000  | 0.000000  | 2.668632  | 2.419031  | 2.8718227 | 2.740640  | 0.000000  | 2.126621  | \n",
       "| 0.3818930 | 0.5180695 | 1.1099774 | 5.528221  | 2.565957  | 2.096786  | 2.021797  | 0.000000  | 0.000000  | 0.000000  | 0.0000000 | 2.740640  | 0.000000  | 2.126621  | \n",
       "| 0.4969683 | 0.8557032 | 1.8460677 | 5.528221  | 2.565957  | 2.096786  | 0.000000  | 0.000000  | 0.000000  | 2.419031  | 0.0000000 | 0.000000  | 0.000000  | 0.000000  | \n",
       "| 0.6557493 | 0.5214868 | 1.9629075 | 5.528221  | 2.565957  | 2.096786  | 0.000000  | 0.000000  | 2.668632  | 2.419031  | 0.0000000 | 2.740640  | 0.000000  | 2.126621  | \n",
       "| 2.1019663 | 3.7481439 | 4.0777066 | 5.528221  | 2.565957  | 2.096786  | 0.000000  | 2.658188  | 0.000000  | 2.419031  | 0.0000000 | 2.740640  | 0.000000  | 0.000000  | \n",
       "| 0.5238137 | 0.2392141 | 0.8178781 | 5.528221  | 2.565957  | 2.096786  | 0.000000  | 0.000000  | 2.668632  | 2.419031  | 0.0000000 | 2.740640  | 0.000000  | 2.126621  | \n",
       "| 0.4092295 | 0.6287915 | 1.2735531 | 5.528221  | 2.565957  | 2.096786  | 0.000000  | 0.000000  | 2.668632  | 2.419031  | 0.1046593 | 2.740640  | 0.000000  | 2.126621  | \n",
       "| 0.5030249 | 2.7700998 | 2.3367946 | 5.528221  | 2.565957  | 2.096786  | 0.000000  | 0.000000  | 2.668632  | 2.419031  | 0.0000000 | 2.740640  | 0.000000  | 2.126621  | \n",
       "| 0.3033209 | 0.9705260 | 1.3319729 | 5.528221  | 2.565957  | 0.000000  | 2.021797  | 0.000000  | 0.000000  | 2.419031  | 0.0000000 | 2.740640  | 2.198585  | 0.000000  | \n",
       "| 0.2126356 | 0.3711237 | 0.1986275 | 1.842740  | 2.565957  | 2.096786  | 0.000000  | 0.000000  | 2.668632  | 2.419031  | 0.0000000 | 2.740640  | 0.000000  | 2.126621  | \n",
       "| 0.8102744 | 0.0000000 | 1.4604966 | 5.528221  | 2.565957  | 0.000000  | 2.021797  | 0.000000  | 0.000000  | 2.419031  | 0.0000000 | 2.740640  | 0.000000  | 2.126621  | \n",
       "| 0.5886357 | 0.0000000 | 1.1683973 | 3.685480  | 2.565957  | 0.000000  | 0.000000  | 2.658188  | 0.000000  | 0.000000  | 0.0000000 | 2.062293  | 0.000000  | 2.126621  | \n",
       "| 0.5745582 | 0.0000000 | 0.8879820 | 5.528221  | 0.000000  | 0.000000  | 2.021797  | 0.000000  | 0.000000  | 2.419031  | 0.0000000 | 0.000000  | 0.000000  | 2.126621  | \n",
       "| 0.7999618 | 0.0000000 | 1.5539684 | 5.528221  | 2.565957  | 2.096786  | 2.021797  | 0.000000  | 0.000000  | 0.000000  | 0.0000000 | 2.740640  | 2.198585  | 0.000000  | \n",
       "| 0.4255987 | 1.1960707 | 1.3436569 | 5.232267  | 2.565957  | 2.096786  | 2.021797  | 0.000000  | 0.000000  | 2.419031  | 0.3102668 | 2.740640  | 0.000000  | 2.126621  | \n",
       "| 1.2538792 | 0.0000000 | 1.2151332 | 5.528221  | 2.565957  | 2.096786  | 2.021797  | 0.000000  | 0.000000  | 0.000000  | 0.0000000 | 0.000000  | 0.000000  | 2.126621  | \n",
       "| 0.9747846 | 1.9222565 | 3.6804515 | 5.528221  | 2.565957  | 2.096786  | 0.000000  | 2.658188  | 0.000000  | 2.419031  | 0.0000000 | 2.740640  | 0.000000  | 2.126621  | \n",
       "| 0.4255987 | 0.6530546 | 1.3553409 | 5.528221  | 2.565957  | 2.096786  | 2.021797  | 0.000000  | 0.000000  | 0.000000  | 0.0000000 | 0.000000  | 0.000000  | 0.000000  | \n",
       "| 0.5508229 | 0.6551050 | 1.3086050 | 5.528221  | 2.328079  | 2.096786  | 0.000000  | 0.000000  | 2.668632  | 0.000000  | 0.0000000 | 0.000000  | 2.198585  | 0.000000  | \n",
       "| 0.6084424 | 0.9995734 | 1.7642799 | 5.528221  | 2.565957  | 2.096786  | 0.000000  | 2.658188  | 0.000000  | 2.419031  | 0.4672000 | 2.515760  | 0.000000  | 0.000000  | \n",
       "| 1.5648936 | 0.0000000 | 2.2316388 | 5.528221  | 2.565957  | 2.096786  | 2.021797  | 0.000000  | 0.000000  | 2.419031  | 2.8718227 | 2.740640  | 0.000000  | 0.000000  | \n",
       "| 0.4581733 | 0.7699278 | 1.4254447 | 5.528221  | 2.565957  | 2.096786  | 2.021797  | 0.000000  | 0.000000  | 2.419031  | 0.0000000 | 2.740640  | 0.000000  | 0.000000  | \n",
       "| 0.6917615 | 0.3554039 | 1.2852370 | 5.528221  | 2.565957  | 2.096786  | 0.000000  | 0.000000  | 2.668632  | 0.000000  | 0.0000000 | 2.740640  | 0.000000  | 2.126621  | \n",
       "| 0.2360436 | 0.0000000 | 0.4089391 | 5.528221  | 2.565957  | 0.000000  | 2.021797  | 0.000000  | 0.000000  | 0.000000  | 0.0000000 | 2.740640  | 0.000000  | 2.126621  | \n",
       "| 0.6138442 | 0.7118329 | 1.4020768 | 5.528221  | 0.000000  | 0.000000  | 0.000000  | 0.000000  | 2.668632  | 2.419031  | 0.1046593 | 2.740640  | 0.000000  | 0.000000  | \n",
       "| ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | \n",
       "| 0.45620903  |  0.65510502 | 1.7058601   | 5.528221    | 2.565957    | 2.096786    | 0.000000    | 2.658188    | 0.000000    | 2.419031    | 0.0000000   | 0.00000     | 2.198585    | 0.000000    | \n",
       "| 0.70109196  |  1.02520348 | 2.0096434   | 1.289918    | 2.565957    | 2.096786    | 0.000000    | 2.658188    | 0.000000    | 2.419031    | 0.0000000   | 2.74064     | 2.198585    | 0.000000    | \n",
       "| 0.37600005  |  0.52011990 | 1.2151332   | 5.528221    | 2.565957    | 2.096786    | 2.021797    | 0.000000    | 0.000000    | 2.419031    | 0.0000000   | 2.74064     | 0.000000    | 2.126621    | \n",
       "| 0.35439274  |  0.00000000 | 0.8178781   | 5.528221    | 0.000000    | 0.000000    | 2.021797    | 0.000000    | 0.000000    | 0.000000    | 0.0000000   | 2.74064     | 0.000000    | 0.000000    | \n",
       "| 0.77753603  |  0.00000000 | 1.0982935   | 5.528221    | 1.584591    | 0.000000    | 2.021797    | 0.000000    | 0.000000    | 2.419031    | 0.0000000   | 2.74064     | 0.000000    | 0.000000    | \n",
       "| 0.44622383  |  0.00000000 | 1.2385011   | 5.528221    | 2.565957    | 2.096786    | 0.000000    | 0.000000    | 2.668632    | 2.419031    | 2.8718227   | 0.00000     | 0.000000    | 0.000000    | \n",
       "| 0.49107539  |  1.16736502 | 0.6543025   | 2.764110    | 2.565957    | 2.096786    | 2.021797    | 0.000000    | 0.000000    | 2.419031    | 0.0000000   | 2.74064     | 0.000000    | 0.000000    | \n",
       "| 0.98215077  |  0.00000000 | 2.3952145   | 3.685480    | 2.565957    | 2.096786    | 0.000000    | 0.000000    | 2.668632    | 2.419031    | 2.8718227   | 2.74064     | 0.000000    | 0.000000    | \n",
       "| 1.53166413  |  0.00000000 | 3.4117201   | 5.528221    | 1.514971    | 0.000000    | 0.000000    | 0.000000    | 0.000000    | 2.419031    | 2.8718227   | 2.74064     | 0.000000    | 0.000000    | \n",
       "| 0.63168664  |  1.12772382 | 1.6591242   | 2.764110    | 2.565957    | 2.096786    | 2.021797    | 0.000000    | 0.000000    | 2.419031    | 0.0000000   | 2.74064     | 2.198585    | 0.000000    | \n",
       "| 2.63871174  |  0.00000000 | 3.0378330   | 5.528221    | 2.565957    | 2.096786    | 2.021797    | 0.000000    | 0.000000    | 2.419031    | 2.8718227   | 2.74064     | 0.000000    | 2.126621    | \n",
       "| 0.62743065  |  0.00000000 | 1.2852370   | 5.528221    | 2.565957    | 0.000000    | 2.021797    | 0.000000    | 0.000000    | 0.000000    | 0.0000000   | 2.74064     | 2.198585    | 0.000000    | \n",
       "| 1.04484473  |  0.34173449 | 2.1849030   | 5.528221    | 2.565957    | 2.096786    | 0.000000    | 0.000000    | 2.668632    | 0.000000    | 2.8718227   | 2.74064     | 2.198585    | 0.000000    | \n",
       "| 0.48894739  |  0.00000000 | 1.0281896   | 5.528221    | 2.565957    | 0.000000    | 1.967588    | 0.000000    | 0.000000    | 2.419031    | 0.0000000   | 0.00000     | 0.000000    | 0.000000    | \n",
       "| 1.63086136  |  0.00000000 | 2.1031151   | 5.528221    | 2.565957    | 2.096786    | 2.021797    | 0.000000    | 0.000000    | 2.419031    | 2.8718227   | 2.74064     | 2.198585    | 0.000000    | \n",
       "| 0.94613858  |  0.00000000 | 2.2433228   | 5.528221    | 2.565957    | 2.096786    | 0.000000    | 0.000000    | 2.668632    | 2.419031    | 0.0000000   | 2.74064     | 0.000000    | 2.126621    | \n",
       "| 0.06809579  | 14.23905108 | 4.0893906   | 2.764110    | 0.000000    | 0.000000    | 0.000000    | 0.000000    | 0.000000    | 2.419031    | 1.3759056   | 1.47864     | 0.000000    | 2.126621    | \n",
       "| 0.47372406  |  0.95412270 | 1.8110158   | 5.528221    | 2.565957    | 2.096786    | 2.021797    | 0.000000    | 0.000000    | 0.000000    | 0.1009888   | 2.74064     | 2.198585    | 0.000000    | \n",
       "| 0.93353431  |  0.00000000 | 1.4955485   | 5.528221    | 2.565957    | 2.096786    | 0.000000    | 0.000000    | 0.000000    | 2.419031    | 0.0000000   | 2.74064     | 0.000000    | 2.126621    | \n",
       "| 0.60173104  |  1.46980005 | 2.0096434   | 5.528221    | 2.565957    | 0.000000    | 2.021797    | 0.000000    | 0.000000    | 2.419031    | 0.0000000   | 2.74064     | 2.198585    | 0.000000    | \n",
       "| 1.96430155  |  0.00000000 | 5.7952506   | 5.528221    | 0.000000    | 2.096786    | 0.000000    | 2.658188    | 0.000000    | 2.419031    | 0.0000000   | 2.74064     | 0.000000    | 0.000000    | \n",
       "| 0.39286031  |  1.29859107 | 1.6721692   | 2.764110    | 2.565957    | 2.096786    | 2.021797    | 0.000000    | 0.000000    | 0.000000    | 0.0000000   | 2.74064     | 0.000000    | 2.126621    | \n",
       "| 0.55655210  |  0.85433623 | 2.0213273   | 5.528221    | 2.565957    | 2.096786    | 0.000000    | 2.658188    | 0.000000    | 2.419031    | 0.0000000   | 2.74064     | 0.000000    | 0.000000    | \n",
       "| 0.65263919  |  0.48218737 | 1.8343838   | 5.528221    | 2.565957    | 2.096786    | 0.000000    | 0.000000    | 2.668632    | 0.000000    | 0.0000000   | 2.74064     | 2.198585    | 0.000000    | \n",
       "| 0.52905188  |  0.66638226 | 1.2618691   | 5.528221    | 2.565957    | 2.096786    | 2.021797    | 0.000000    | 0.000000    | 2.419031    | 0.0000000   | 2.74064     | 2.198585    | 0.000000    | \n",
       "| 0.47470621  |  0.00000000 | 0.8295621   | 5.528221    | 0.000000    | 0.000000    | 2.021797    | 0.000000    | 0.000000    | 2.419031    | 0.0000000   | 2.74064     | 2.198585    | 0.000000    | \n",
       "| 0.67211851  |  0.00000000 | 0.4673589   | 2.764110    | 2.565957    | 2.096786    | 0.000000    | 0.000000    | 0.000000    | 2.419031    | 0.0000000   | 2.74064     | 2.198585    | 0.000000    | \n",
       "| 1.32132017  |  0.08201628 | 2.9560452   | 5.528221    | 2.565957    | 2.096786    | 0.000000    | 2.658188    | 0.000000    | 2.419031    | 0.0000000   | 2.74064     | 0.000000    | 2.126621    | \n",
       "| 1.24127488  |  0.00000000 | 2.1849030   | 5.528221    | 2.565957    | 2.096786    | 0.000000    | 0.000000    | 2.668632    | 2.419031    | 0.0000000   | 2.74064     | 0.000000    | 2.126621    | \n",
       "| 0.75019950  |  0.00000000 | 1.5539684   | 5.528221    | 0.000000    | 0.000000    | 2.021797    | 0.000000    | 0.000000    | 2.419031    | 2.8718227   | 0.00000     | 0.000000    | 0.000000    | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "    ApplicantIncome CoapplicantIncome LoanAmount Loan_Amount_Term Male    \n",
       "1   0.9574333       0.0000000         1.7251291  5.528221         2.565957\n",
       "2   0.7501995       0.5153356         1.4955485  5.528221         2.565957\n",
       "3   0.4910754       0.0000000         0.7711422  5.528221         2.565957\n",
       "4   0.4228159       0.8058099         1.4020768  5.528221         2.565957\n",
       "5   0.9821508       0.0000000         1.6474402  5.528221         2.565957\n",
       "6   0.8867185       1.4339179         3.1196208  5.528221         2.565957\n",
       "7   0.3818930       0.5180695         1.1099774  5.528221         2.565957\n",
       "8   0.4969683       0.8557032         1.8460677  5.528221         2.565957\n",
       "9   0.6557493       0.5214868         1.9629075  5.528221         2.565957\n",
       "10  2.1019663       3.7481439         4.0777066  5.528221         2.565957\n",
       "11  0.5238137       0.2392141         0.8178781  5.528221         2.565957\n",
       "12  0.4092295       0.6287915         1.2735531  5.528221         2.565957\n",
       "13  0.5030249       2.7700998         2.3367946  5.528221         2.565957\n",
       "14  0.3033209       0.9705260         1.3319729  5.528221         2.565957\n",
       "15  0.2126356       0.3711237         0.1986275  1.842740         2.565957\n",
       "16  0.8102744       0.0000000         1.4604966  5.528221         2.565957\n",
       "17  0.5886357       0.0000000         1.1683973  3.685480         2.565957\n",
       "18  0.5745582       0.0000000         0.8879820  5.528221         0.000000\n",
       "19  0.7999618       0.0000000         1.5539684  5.528221         2.565957\n",
       "20  0.4255987       1.1960707         1.3436569  5.232267         2.565957\n",
       "21  1.2538792       0.0000000         1.2151332  5.528221         2.565957\n",
       "22  0.9747846       1.9222565         3.6804515  5.528221         2.565957\n",
       "23  0.4255987       0.6530546         1.3553409  5.528221         2.565957\n",
       "24  0.5508229       0.6551050         1.3086050  5.528221         2.328079\n",
       "25  0.6084424       0.9995734         1.7642799  5.528221         2.565957\n",
       "26  1.5648936       0.0000000         2.2316388  5.528221         2.565957\n",
       "27  0.4581733       0.7699278         1.4254447  5.528221         2.565957\n",
       "28  0.6917615       0.3554039         1.2852370  5.528221         2.565957\n",
       "29  0.2360436       0.0000000         0.4089391  5.528221         2.565957\n",
       "30  0.6138442       0.7118329         1.4020768  5.528221         0.000000\n",
       "... ...             ...               ...        ...              ...     \n",
       "585 0.45620903       0.65510502       1.7058601  5.528221         2.565957\n",
       "586 0.70109196       1.02520348       2.0096434  1.289918         2.565957\n",
       "587 0.37600005       0.52011990       1.2151332  5.528221         2.565957\n",
       "588 0.35439274       0.00000000       0.8178781  5.528221         0.000000\n",
       "589 0.77753603       0.00000000       1.0982935  5.528221         1.584591\n",
       "590 0.44622383       0.00000000       1.2385011  5.528221         2.565957\n",
       "591 0.49107539       1.16736502       0.6543025  2.764110         2.565957\n",
       "592 0.98215077       0.00000000       2.3952145  3.685480         2.565957\n",
       "593 1.53166413       0.00000000       3.4117201  5.528221         1.514971\n",
       "594 0.63168664       1.12772382       1.6591242  2.764110         2.565957\n",
       "595 2.63871174       0.00000000       3.0378330  5.528221         2.565957\n",
       "596 0.62743065       0.00000000       1.2852370  5.528221         2.565957\n",
       "597 1.04484473       0.34173449       2.1849030  5.528221         2.565957\n",
       "598 0.48894739       0.00000000       1.0281896  5.528221         2.565957\n",
       "599 1.63086136       0.00000000       2.1031151  5.528221         2.565957\n",
       "600 0.94613858       0.00000000       2.2433228  5.528221         2.565957\n",
       "601 0.06809579      14.23905108       4.0893906  2.764110         0.000000\n",
       "602 0.47372406       0.95412270       1.8110158  5.528221         2.565957\n",
       "603 0.93353431       0.00000000       1.4955485  5.528221         2.565957\n",
       "604 0.60173104       1.46980005       2.0096434  5.528221         2.565957\n",
       "605 1.96430155       0.00000000       5.7952506  5.528221         0.000000\n",
       "606 0.39286031       1.29859107       1.6721692  2.764110         2.565957\n",
       "607 0.55655210       0.85433623       2.0213273  5.528221         2.565957\n",
       "608 0.65263919       0.48218737       1.8343838  5.528221         2.565957\n",
       "609 0.52905188       0.66638226       1.2618691  5.528221         2.565957\n",
       "610 0.47470621       0.00000000       0.8295621  5.528221         0.000000\n",
       "611 0.67211851       0.00000000       0.4673589  2.764110         2.565957\n",
       "612 1.32132017       0.08201628       2.9560452  5.528221         2.565957\n",
       "613 1.24127488       0.00000000       2.1849030  5.528221         2.565957\n",
       "614 0.75019950       0.00000000       1.5539684  5.528221         0.000000\n",
       "    Marital  Dependents.0 Dependents.1 Dependents.2 Graduate SelfEmployed\n",
       "1   0.000000 2.021797     0.000000     0.000000     2.419031 0.0000000   \n",
       "2   2.096786 0.000000     2.658188     0.000000     2.419031 0.0000000   \n",
       "3   2.096786 2.021797     0.000000     0.000000     2.419031 2.8718227   \n",
       "4   2.096786 2.021797     0.000000     0.000000     0.000000 0.0000000   \n",
       "5   0.000000 2.021797     0.000000     0.000000     2.419031 0.0000000   \n",
       "6   2.096786 0.000000     0.000000     2.668632     2.419031 2.8718227   \n",
       "7   2.096786 2.021797     0.000000     0.000000     0.000000 0.0000000   \n",
       "8   2.096786 0.000000     0.000000     0.000000     2.419031 0.0000000   \n",
       "9   2.096786 0.000000     0.000000     2.668632     2.419031 0.0000000   \n",
       "10  2.096786 0.000000     2.658188     0.000000     2.419031 0.0000000   \n",
       "11  2.096786 0.000000     0.000000     2.668632     2.419031 0.0000000   \n",
       "12  2.096786 0.000000     0.000000     2.668632     2.419031 0.1046593   \n",
       "13  2.096786 0.000000     0.000000     2.668632     2.419031 0.0000000   \n",
       "14  0.000000 2.021797     0.000000     0.000000     2.419031 0.0000000   \n",
       "15  2.096786 0.000000     0.000000     2.668632     2.419031 0.0000000   \n",
       "16  0.000000 2.021797     0.000000     0.000000     2.419031 0.0000000   \n",
       "17  0.000000 0.000000     2.658188     0.000000     0.000000 0.0000000   \n",
       "18  0.000000 2.021797     0.000000     0.000000     2.419031 0.0000000   \n",
       "19  2.096786 2.021797     0.000000     0.000000     0.000000 0.0000000   \n",
       "20  2.096786 2.021797     0.000000     0.000000     2.419031 0.3102668   \n",
       "21  2.096786 2.021797     0.000000     0.000000     0.000000 0.0000000   \n",
       "22  2.096786 0.000000     2.658188     0.000000     2.419031 0.0000000   \n",
       "23  2.096786 2.021797     0.000000     0.000000     0.000000 0.0000000   \n",
       "24  2.096786 0.000000     0.000000     2.668632     0.000000 0.0000000   \n",
       "25  2.096786 0.000000     2.658188     0.000000     2.419031 0.4672000   \n",
       "26  2.096786 2.021797     0.000000     0.000000     2.419031 2.8718227   \n",
       "27  2.096786 2.021797     0.000000     0.000000     2.419031 0.0000000   \n",
       "28  2.096786 0.000000     0.000000     2.668632     0.000000 0.0000000   \n",
       "29  0.000000 2.021797     0.000000     0.000000     0.000000 0.0000000   \n",
       "30  0.000000 0.000000     0.000000     2.668632     2.419031 0.1046593   \n",
       "... ...      ...          ...          ...          ...      ...         \n",
       "585 2.096786 0.000000     2.658188     0.000000     2.419031 0.0000000   \n",
       "586 2.096786 0.000000     2.658188     0.000000     2.419031 0.0000000   \n",
       "587 2.096786 2.021797     0.000000     0.000000     2.419031 0.0000000   \n",
       "588 0.000000 2.021797     0.000000     0.000000     0.000000 0.0000000   \n",
       "589 0.000000 2.021797     0.000000     0.000000     2.419031 0.0000000   \n",
       "590 2.096786 0.000000     0.000000     2.668632     2.419031 2.8718227   \n",
       "591 2.096786 2.021797     0.000000     0.000000     2.419031 0.0000000   \n",
       "592 2.096786 0.000000     0.000000     2.668632     2.419031 2.8718227   \n",
       "593 0.000000 0.000000     0.000000     0.000000     2.419031 2.8718227   \n",
       "594 2.096786 2.021797     0.000000     0.000000     2.419031 0.0000000   \n",
       "595 2.096786 2.021797     0.000000     0.000000     2.419031 2.8718227   \n",
       "596 0.000000 2.021797     0.000000     0.000000     0.000000 0.0000000   \n",
       "597 2.096786 0.000000     0.000000     2.668632     0.000000 2.8718227   \n",
       "598 0.000000 1.967588     0.000000     0.000000     2.419031 0.0000000   \n",
       "599 2.096786 2.021797     0.000000     0.000000     2.419031 2.8718227   \n",
       "600 2.096786 0.000000     0.000000     2.668632     2.419031 0.0000000   \n",
       "601 0.000000 0.000000     0.000000     0.000000     2.419031 1.3759056   \n",
       "602 2.096786 2.021797     0.000000     0.000000     0.000000 0.1009888   \n",
       "603 2.096786 0.000000     0.000000     0.000000     2.419031 0.0000000   \n",
       "604 0.000000 2.021797     0.000000     0.000000     2.419031 0.0000000   \n",
       "605 2.096786 0.000000     2.658188     0.000000     2.419031 0.0000000   \n",
       "606 2.096786 2.021797     0.000000     0.000000     0.000000 0.0000000   \n",
       "607 2.096786 0.000000     2.658188     0.000000     2.419031 0.0000000   \n",
       "608 2.096786 0.000000     0.000000     2.668632     0.000000 0.0000000   \n",
       "609 2.096786 2.021797     0.000000     0.000000     2.419031 0.0000000   \n",
       "610 0.000000 2.021797     0.000000     0.000000     2.419031 0.0000000   \n",
       "611 2.096786 0.000000     0.000000     0.000000     2.419031 0.0000000   \n",
       "612 2.096786 0.000000     2.658188     0.000000     2.419031 0.0000000   \n",
       "613 2.096786 0.000000     0.000000     2.668632     2.419031 0.0000000   \n",
       "614 0.000000 2.021797     0.000000     0.000000     2.419031 2.8718227   \n",
       "    CreditHistory Rural    Urban   \n",
       "1   2.740640      0.000000 2.126621\n",
       "2   2.740640      2.198585 0.000000\n",
       "3   2.740640      0.000000 2.126621\n",
       "4   2.740640      0.000000 2.126621\n",
       "5   2.740640      0.000000 2.126621\n",
       "6   2.740640      0.000000 2.126621\n",
       "7   2.740640      0.000000 2.126621\n",
       "8   0.000000      0.000000 0.000000\n",
       "9   2.740640      0.000000 2.126621\n",
       "10  2.740640      0.000000 0.000000\n",
       "11  2.740640      0.000000 2.126621\n",
       "12  2.740640      0.000000 2.126621\n",
       "13  2.740640      0.000000 2.126621\n",
       "14  2.740640      2.198585 0.000000\n",
       "15  2.740640      0.000000 2.126621\n",
       "16  2.740640      0.000000 2.126621\n",
       "17  2.062293      0.000000 2.126621\n",
       "18  0.000000      0.000000 2.126621\n",
       "19  2.740640      2.198585 0.000000\n",
       "20  2.740640      0.000000 2.126621\n",
       "21  0.000000      0.000000 2.126621\n",
       "22  2.740640      0.000000 2.126621\n",
       "23  0.000000      0.000000 0.000000\n",
       "24  0.000000      2.198585 0.000000\n",
       "25  2.515760      0.000000 0.000000\n",
       "26  2.740640      0.000000 0.000000\n",
       "27  2.740640      0.000000 0.000000\n",
       "28  2.740640      0.000000 2.126621\n",
       "29  2.740640      0.000000 2.126621\n",
       "30  2.740640      0.000000 0.000000\n",
       "... ...           ...      ...     \n",
       "585 0.00000       2.198585 0.000000\n",
       "586 2.74064       2.198585 0.000000\n",
       "587 2.74064       0.000000 2.126621\n",
       "588 2.74064       0.000000 0.000000\n",
       "589 2.74064       0.000000 0.000000\n",
       "590 0.00000       0.000000 0.000000\n",
       "591 2.74064       0.000000 0.000000\n",
       "592 2.74064       0.000000 0.000000\n",
       "593 2.74064       0.000000 0.000000\n",
       "594 2.74064       2.198585 0.000000\n",
       "595 2.74064       0.000000 2.126621\n",
       "596 2.74064       2.198585 0.000000\n",
       "597 2.74064       2.198585 0.000000\n",
       "598 0.00000       0.000000 0.000000\n",
       "599 2.74064       2.198585 0.000000\n",
       "600 2.74064       0.000000 2.126621\n",
       "601 1.47864       0.000000 2.126621\n",
       "602 2.74064       2.198585 0.000000\n",
       "603 2.74064       0.000000 2.126621\n",
       "604 2.74064       2.198585 0.000000\n",
       "605 2.74064       0.000000 0.000000\n",
       "606 2.74064       0.000000 2.126621\n",
       "607 2.74064       0.000000 0.000000\n",
       "608 2.74064       2.198585 0.000000\n",
       "609 2.74064       2.198585 0.000000\n",
       "610 2.74064       2.198585 0.000000\n",
       "611 2.74064       2.198585 0.000000\n",
       "612 2.74064       0.000000 2.126621\n",
       "613 2.74064       0.000000 2.126621\n",
       "614 0.00000       0.000000 0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processparameters <-preProcess(X,method =c('scale','bagImpute'))\n",
    "preprocessedX <-predict(processparameters,X)\n",
    "preprocessedX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "Different models will be trained. \n",
    "\n",
    "__Linear Discriminant Analysis__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear Discriminant Analysis \n",
       "\n",
       "614 samples\n",
       " 14 predictor\n",
       "  2 classes: 'N', 'Y' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (5 fold, repeated 3 times) \n",
       "Summary of sample sizes: 491, 491, 492, 491, 491, 491, ... \n",
       "Resampling results:\n",
       "\n",
       "  Accuracy   Kappa   \n",
       "  0.8083902  0.481057\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lda<-train(preprocessedX, df[,13],method='lda',trControl=trainControl(method='repeatedcv',number=5,repeats=3))\n",
    "lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Logistic Regression__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# weights:  16 (15 variable)\n",
      "initial  value 340.335266 \n",
      "iter  10 value 215.988439\n",
      "iter  20 value 210.429277\n",
      "final  value 210.429223 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 340.335266 \n",
      "iter  10 value 216.378379\n",
      "iter  20 value 210.980643\n",
      "final  value 210.980470 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 340.335266 \n",
      "iter  10 value 215.988839\n",
      "iter  20 value 210.429883\n",
      "final  value 210.429828 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 340.335266 \n",
      "iter  10 value 222.048433\n",
      "iter  20 value 219.163299\n",
      "final  value 219.163027 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 340.335266 \n",
      "iter  10 value 222.401477\n",
      "iter  20 value 219.545569\n",
      "final  value 219.545229 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 340.335266 \n",
      "iter  10 value 222.048789\n",
      "iter  20 value 219.163711\n",
      "final  value 219.163439 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 340.335266 \n",
      "iter  10 value 235.926487\n",
      "iter  20 value 230.999721\n",
      "final  value 230.999420 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 340.335266 \n",
      "iter  10 value 236.635422\n",
      "iter  20 value 231.641071\n",
      "final  value 231.640886 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 340.335266 \n",
      "iter  10 value 235.927228\n",
      "iter  20 value 231.000443\n",
      "final  value 231.000142 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 341.028413 \n",
      "iter  10 value 229.948552\n",
      "iter  20 value 224.248395\n",
      "final  value 224.248315 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 341.028413 \n",
      "iter  10 value 230.435198\n",
      "iter  20 value 224.794293\n",
      "final  value 224.793919 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 341.028413 \n",
      "iter  10 value 229.949058\n",
      "iter  20 value 224.249001\n",
      "final  value 224.248921 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 340.335266 \n",
      "iter  10 value 243.097959\n",
      "iter  20 value 234.702893\n",
      "final  value 234.702416 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 340.335266 \n",
      "iter  10 value 243.549653\n",
      "iter  20 value 235.105010\n",
      "final  value 235.104642 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 340.335266 \n",
      "iter  10 value 243.098424\n",
      "iter  20 value 234.703337\n",
      "final  value 234.702862 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 341.028413 \n",
      "iter  10 value 228.343101\n",
      "iter  20 value 221.735830\n",
      "final  value 221.735816 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 341.028413 \n",
      "iter  10 value 228.750107\n",
      "iter  20 value 222.327154\n",
      "final  value 222.327146 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 341.028413 \n",
      "iter  10 value 228.343522\n",
      "iter  20 value 221.736491\n",
      "final  value 221.736477 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 340.335266 \n",
      "iter  10 value 235.095730\n",
      "iter  20 value 226.146431\n",
      "iter  20 value 226.146429\n",
      "iter  20 value 226.146429\n",
      "final  value 226.146429 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 340.335266 \n",
      "iter  10 value 235.238258\n",
      "iter  20 value 226.625991\n",
      "final  value 226.625936 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 340.335266 \n",
      "iter  10 value 235.095878\n",
      "iter  20 value 226.146959\n",
      "iter  20 value 226.146958\n",
      "iter  20 value 226.146958\n",
      "final  value 226.146958 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 340.335266 \n",
      "iter  10 value 229.933822\n",
      "iter  20 value 224.317464\n",
      "final  value 224.317430 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 340.335266 \n",
      "iter  10 value 230.522789\n",
      "iter  20 value 224.936347\n",
      "final  value 224.936329 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 340.335266 \n",
      "iter  10 value 229.934443\n",
      "iter  20 value 224.318165\n",
      "final  value 224.318130 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 339.642118 \n",
      "iter  10 value 230.929539\n",
      "iter  20 value 224.064539\n",
      "final  value 224.064522 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 339.642118 \n",
      "iter  10 value 231.475177\n",
      "iter  20 value 224.763895\n",
      "final  value 224.763803 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 339.642118 \n",
      "iter  10 value 230.930116\n",
      "iter  20 value 224.065326\n",
      "final  value 224.065309 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 341.028413 \n",
      "iter  10 value 230.546880\n",
      "iter  20 value 225.564257\n",
      "final  value 225.564237 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 341.028413 \n",
      "iter  10 value 230.765904\n",
      "iter  20 value 225.843759\n",
      "final  value 225.843753 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 341.028413 \n",
      "iter  10 value 230.547100\n",
      "iter  20 value 225.564546\n",
      "final  value 225.564526 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 340.335266 \n",
      "iter  10 value 223.067159\n",
      "iter  20 value 217.980800\n",
      "final  value 217.980443 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 340.335266 \n",
      "iter  10 value 223.502119\n",
      "iter  20 value 218.562130\n",
      "final  value 218.561989 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 340.335266 \n",
      "iter  10 value 223.067611\n",
      "iter  20 value 217.981454\n",
      "final  value 217.981097 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 340.335266 \n",
      "iter  10 value 234.990000\n",
      "iter  20 value 223.543643\n",
      "final  value 223.542240 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 340.335266 \n",
      "iter  10 value 235.686928\n",
      "iter  20 value 224.135534\n",
      "final  value 224.133829 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 340.335266 \n",
      "iter  10 value 234.990725\n",
      "iter  20 value 223.544306\n",
      "final  value 223.542902 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 341.028413 \n",
      "iter  10 value 249.838723\n",
      "iter  20 value 223.693365\n",
      "final  value 223.509980 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 341.028413 \n",
      "iter  10 value 230.576984\n",
      "iter  20 value 224.077468\n",
      "final  value 224.077435 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 341.028413 \n",
      "iter  10 value 249.838471\n",
      "iter  20 value 223.693705\n",
      "final  value 223.510617 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 340.335266 \n",
      "iter  10 value 228.158434\n",
      "iter  20 value 223.791905\n",
      "final  value 223.791900 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 340.335266 \n",
      "iter  10 value 228.388320\n",
      "iter  20 value 224.061809\n",
      "iter  20 value 224.061807\n",
      "iter  20 value 224.061807\n",
      "final  value 224.061807 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 340.335266 \n",
      "iter  10 value 228.158664\n",
      "iter  20 value 223.792184\n",
      "final  value 223.792180 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 340.335266 \n",
      "iter  10 value 238.792846\n",
      "iter  20 value 231.365598\n",
      "final  value 231.365553 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 340.335266 \n",
      "iter  10 value 239.349364\n",
      "iter  20 value 232.040194\n",
      "final  value 232.040007 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 340.335266 \n",
      "iter  10 value 238.793435\n",
      "iter  20 value 231.366357\n",
      "final  value 231.366311 \n",
      "converged\n",
      "# weights:  16 (15 variable)\n",
      "initial  value 425.592369 \n",
      "iter  10 value 291.239981\n",
      "iter  20 value 282.472592\n",
      "final  value 282.472527 \n",
      "converged\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Penalized Multinomial Regression \n",
       "\n",
       "614 samples\n",
       " 14 predictor\n",
       "  2 classes: 'N', 'Y' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (5 fold, repeated 3 times) \n",
       "Summary of sample sizes: 491, 491, 491, 492, 491, 492, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  decay  Accuracy   Kappa    \n",
       "  0e+00  0.8007800  0.4646392\n",
       "  1e-04  0.8007800  0.4646392\n",
       "  1e-01  0.8013221  0.4657627\n",
       "\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final value used for the model was decay = 0.1."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logistic<-train(preprocessedX, df[,13],method='multinom',trControl=trainControl(method='repeatedcv',number=5,repeats=3))\n",
    "logistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Support Vector Machine__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Support Vector Machines with Linear Kernel \n",
       "\n",
       "614 samples\n",
       " 14 predictor\n",
       "  2 classes: 'N', 'Y' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (5 fold, repeated 3 times) \n",
       "Summary of sample sizes: 492, 491, 492, 490, 491, 491, ... \n",
       "Resampling results:\n",
       "\n",
       "  Accuracy   Kappa    \n",
       "  0.8094905  0.4797852\n",
       "\n",
       "Tuning parameter 'C' was held constant at a value of 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SVMLin<-train(preprocessedX, df[,13],method='svmLinear',trControl=trainControl(method='repeatedcv',number=5,repeats=3))\n",
    "SVMLin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Support Vector Machines with Radial Basis Function Kernel \n",
       "\n",
       "614 samples\n",
       " 14 predictor\n",
       "  2 classes: 'N', 'Y' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (5 fold, repeated 3 times) \n",
       "Summary of sample sizes: 491, 491, 491, 492, 491, 492, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  C     Accuracy   Kappa    \n",
       "  0.25  0.8072400  0.4705199\n",
       "  0.50  0.8088661  0.4758871\n",
       "  1.00  0.8072400  0.4747079\n",
       "\n",
       "Tuning parameter 'sigma' was held constant at a value of 0.06199227\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final values used for the model were sigma = 0.06199227 and C = 0.5."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SVMRad<-train(preprocessedX, df[,13],method='svmRadial',trControl=trainControl(method='repeatedcv',number=5,repeats=3))\n",
    "SVMRad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__k Nearest Neighbors__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "k-Nearest Neighbors \n",
       "\n",
       "614 samples\n",
       " 14 predictor\n",
       "  2 classes: 'N', 'Y' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (5 fold, repeated 3 times) \n",
       "Summary of sample sizes: 491, 491, 491, 492, 491, 491, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  k  Accuracy   Kappa    \n",
       "  5  0.7839600  0.4198568\n",
       "  7  0.7850175  0.4120866\n",
       "  9  0.7850264  0.4088475\n",
       "\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final value used for the model was k = 9."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kNN<-train(preprocessedX, df[,13],method='knn',trControl=trainControl(method='repeatedcv',number=5,repeats=3))\n",
    "kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "k-Nearest Neighbors \n",
       "\n",
       "614 samples\n",
       " 14 predictor\n",
       "  2 classes: 'N', 'Y' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (5 fold, repeated 3 times) \n",
       "Summary of sample sizes: 491, 492, 491, 492, 490, 492, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  kmax  Accuracy   Kappa    \n",
       "  5     0.7579762  0.3902270\n",
       "  7     0.7731707  0.4101879\n",
       "  9     0.7818520  0.4244742\n",
       "\n",
       "Tuning parameter 'distance' was held constant at a value of 2\n",
       "Tuning\n",
       " parameter 'kernel' was held constant at a value of optimal\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final values used for the model were kmax = 9, distance = 2 and kernel\n",
       " = optimal."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kkNN<-train(preprocessedX, df[,13],method='kknn',trControl=trainControl(method='repeatedcv',number=5,repeats=3))\n",
    "kkNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Naive Bayes__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 31\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 43\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 1\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 3\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 4\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 5\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 6\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 8\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 15\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 17\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 18\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 20\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 23\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 27\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 28\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 30\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 31\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 33\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 35\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 39\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 42\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 43\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 46\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 47\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 52\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 54\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 55\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 56\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 57\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 60\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 62\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 66\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 67\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 69\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 70\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 71\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 72\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 74\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 77\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 80\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 81\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 82\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 83\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 84\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 86\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 87\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 89\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 94\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 97\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 98\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 99\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 102\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 103\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 105\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 106\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 109\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 112\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 114\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 115\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 117\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 119\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 121\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 122\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 2\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 3\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 4\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 8\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 9\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 10\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 12\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 15\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 16\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 17\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 23\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 25\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 26\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 29\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 30\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 31\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 32\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 37\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 43\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 45\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 48\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 49\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 50\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 52\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 54\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 57\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 58\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 59\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 62\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 63\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 64\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 65\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 68\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 71\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 72\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 73\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 76\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 77\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 81\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 84\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 85\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 88\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 89\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 90\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 91\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 92\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 93\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 94\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 96\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 98\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 99\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 101\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 103\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 106\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 107\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 108\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 110\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 112\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 117\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 121\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 123\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 18\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 123\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 1\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 3\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 6\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 12\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 13\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 16\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 18\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 19\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 28\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 30\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 34\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 36\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 37\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 38\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 41\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 42\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 43\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 44\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 45\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 47\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 54\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 55\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 56\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 58\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 63\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 66\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 69\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 72\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 74\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 79\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 81\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 82\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 83\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 85\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 86\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 87\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 89\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 91\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 95\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 98\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 102\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 103\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 106\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 107\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 110\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 111\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 116\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 117\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 118\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 119\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 122\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 123\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 2\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 5\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 8\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 10\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 13\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 14\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 17\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 21\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 23\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 29\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 31\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 33\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 35\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 36\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 39\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 40\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 43\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 45\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 46\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 47\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 48\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 50\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 54\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 56\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 60\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 61\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 66\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 71\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 76\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 78\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 79\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 82\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 87\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 88\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 92\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 93\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 94\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 95\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 96\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 99\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 100\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 101\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 103\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 104\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 105\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 107\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 108\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 111\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 113\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 114\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 115\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 117\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 120\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 121\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 7\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 11\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 13\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 16\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 17\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 20\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 21\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 25\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 26\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 28\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 33\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 35\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 36\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 38\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 39\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 40\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 46\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 51\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 53\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 58\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 59\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 60\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 61\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 65\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 66\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 75\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 77\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 84\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 85\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 86\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 88\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 91\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 93\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 95\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 97\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 98\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 104\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 105\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 106\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 108\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 112\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 114\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 116\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 118\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 119\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 120\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 122\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 124\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 26\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 1\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 2\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 3\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 4\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 8\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 9\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 11\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 12\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 13\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 16\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 17\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 18\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 21\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 22\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 24\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 26\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 29\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 31\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 36\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 39\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 40\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 43\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 44\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 45\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 46\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 47\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 49\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 52\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 58\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 59\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 62\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 66\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 67\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 73\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 76\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 77\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 78\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 79\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 80\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 81\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 82\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 84\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 85\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 86\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 88\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 92\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 93\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 96\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 97\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 98\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 99\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 102\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 105\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 107\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 109\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 110\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 111\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 114\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 115\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 116\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 119\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 121\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 122\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 122\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 4\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 7\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 8\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 9\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 14\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 16\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 19\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 26\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 27\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 29\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 31\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 32\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 33\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 38\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 40\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 42\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 48\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 50\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 51\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 52\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 55\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 59\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 60\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 61\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 63\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 64\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 67\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 68\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 69\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 72\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 75\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 76\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 80\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 81\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 85\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 90\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 92\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 93\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 95\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 96\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 100\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 101\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 102\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 108\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 110\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 111\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 114\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 115\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 116\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 117\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 119\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 120\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 121\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 122\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 123\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 29\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 2\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 6\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 7\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 8\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 9\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 10\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 15\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 17\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 18\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 19\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 21\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 22\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 26\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 28\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 29\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 32\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 34\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 40\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 41\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 42\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 43\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 45\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 47\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 49\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 51\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 53\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 54\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 55\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 56\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 60\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 61\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 66\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 67\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 69\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 70\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 72\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 73\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 74\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 76\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 85\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 86\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 90\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 92\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 93\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 94\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 97\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 99\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 100\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 101\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 102\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 103\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 106\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 107\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 108\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 110\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 113\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 115\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 120\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 121\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 2\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 2\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 7\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 8\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 11\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 14\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 16\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 17\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 24\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 29\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 30\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 34\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 38\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 39\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 40\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 41\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 44\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 45\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 49\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 51\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 52\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 54\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 55\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 57\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 58\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 61\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 62\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 65\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 67\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 68\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 70\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 72\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 75\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 77\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 78\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 81\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 82\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 83\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 85\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 86\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 89\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 90\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 91\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 94\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 97\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 100\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 102\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 103\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 105\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 106\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 107\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 108\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 111\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 117\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 119\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 122\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 27\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 5\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 12\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 13\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 16\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 18\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 20\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 23\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 24\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 27\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 29\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 30\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 34\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 35\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 39\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 40\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 42\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 43\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 46\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 47\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 49\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 53\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 54\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 59\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 63\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 67\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 71\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 73\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 76\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 78\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 79\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 81\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 83\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 86\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 90\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 91\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 93\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 94\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 95\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 96\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 99\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 100\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 102\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 105\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 106\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 108\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 109\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 111\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 112\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 116\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 119\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 121\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 123\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 124\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 3\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 4\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 5\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 6\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 8\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 12\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 14\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 15\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 16\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 17\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 18\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 20\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 24\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 26\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 28\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 29\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 30\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 31\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 36\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 38\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 40\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 43\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 45\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 46\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 47\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 48\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 49\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 50\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 51\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 55\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 58\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 60\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 61\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 62\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 63\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 65\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 66\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 73\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 78\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 80\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 81\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 82\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 83\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 84\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 88\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 89\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 92\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 93\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 94\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 95\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 96\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 98\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 99\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 101\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 106\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 109\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 110\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 111\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 112\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 113\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 114\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 116\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 117\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 120\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 121\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 123\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 3\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 5\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 6\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 7\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 8\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 9\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 10\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 21\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 24\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 25\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 26\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 30\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 34\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 37\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 39\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 41\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 43\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 46\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 47\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 51\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 54\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 59\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 62\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 64\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 71\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 74\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 76\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 78\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 81\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 84\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 87\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 88\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 89\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 96\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 97\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 98\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 99\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 100\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 101\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 102\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 104\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 106\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 107\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 108\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 109\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 114\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 115\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 118\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 119\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 120\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 121\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 123\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 33\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 1\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 3\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 4\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 6\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 7\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 10\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 11\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 12\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 14\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 15\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 19\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 23\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 24\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 26\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 32\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 33\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 35\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 38\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 40\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 41\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 42\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 44\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 45\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 46\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 47\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 48\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 49\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 51\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 53\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 55\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 56\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 57\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 61\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 65\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 67\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 68\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 69\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 75\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 77\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 79\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 83\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 85\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 86\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 92\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 94\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 96\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 97\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 99\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 100\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 101\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 103\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 104\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 105\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 106\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 107\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 108\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 109\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 113\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 114\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 119\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 121\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 8\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 10\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 12\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 18\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 19\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 21\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 22\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 25\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 26\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 28\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 29\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 30\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 33\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 36\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 37\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 45\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 46\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 49\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 50\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 52\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 53\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 56\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 60\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 64\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 66\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 72\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 73\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 74\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 75\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 77\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 79\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 84\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 88\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 89\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 91\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 94\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 96\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 97\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 100\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 101\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 102\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 104\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 106\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 112\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 113\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 116\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 121\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 122\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 27\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 28\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 34\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 120\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 2\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 8\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 12\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 13\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 14\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 20\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 23\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 24\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 27\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 28\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 30\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 33\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 34\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 37\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 42\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 43\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 45\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 49\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 50\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 57\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 59\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 61\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 62\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 64\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 65\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 66\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 68\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 70\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 71\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 73\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 76\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 77\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 79\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 81\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 82\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 83\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 85\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 88\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 89\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 91\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 94\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 97\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 98\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 99\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 105\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 106\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 109\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 110\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 111\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 112\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 114\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 116\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 118\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 119\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 120\"Warning message in FUN(X[[i]], ...):\n",
      "\"Numerical 0 probability for all classes with observation 122\""
     ]
    },
    {
     "data": {
      "text/plain": [
       "Naive Bayes \n",
       "\n",
       "614 samples\n",
       " 14 predictor\n",
       "  2 classes: 'N', 'Y' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (5 fold, repeated 3 times) \n",
       "Summary of sample sizes: 492, 491, 491, 492, 490, 492, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  usekernel  Accuracy   Kappa     \n",
       "  FALSE      0.7915134  0.44943975\n",
       "   TRUE      0.7024625  0.08540328\n",
       "\n",
       "Tuning parameter 'fL' was held constant at a value of 0\n",
       "Tuning\n",
       " parameter 'adjust' was held constant at a value of 1\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final values used for the model were fL = 0, usekernel = FALSE and adjust\n",
       " = 1."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb<-train(preprocessedX, df[,13],method='nb',trControl=trainControl(method='repeatedcv',number=5,repeats=3))\n",
    "nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Decision Tree__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CART \n",
       "\n",
       "614 samples\n",
       " 14 predictor\n",
       "  2 classes: 'N', 'Y' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (5 fold, repeated 3 times) \n",
       "Summary of sample sizes: 492, 490, 492, 491, 491, 491, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  cp           Accuracy   Kappa    \n",
       "  0.005208333  0.7621580  0.3964007\n",
       "  0.007812500  0.7811948  0.4296773\n",
       "  0.395833333  0.7324045  0.1880315\n",
       "\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final value used for the model was cp = 0.0078125."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tree<-train(preprocessedX, df[,13],method='rpart',trControl=trainControl(method='repeatedcv',number=5,repeats=3))\n",
    "tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Random Forest__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Random Forest \n",
       "\n",
       "614 samples\n",
       " 14 predictor\n",
       "  2 classes: 'N', 'Y' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (5 fold, repeated 3 times) \n",
       "Summary of sample sizes: 491, 491, 491, 491, 492, 492, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  mtry  Accuracy   Kappa    \n",
       "   2    0.8110690  0.4892170\n",
       "   8    0.7882649  0.4507972\n",
       "  14    0.7801302  0.4349723\n",
       "\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final value used for the model was mtry = 2."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf<-train(preprocessedX, df[,13],method='rf',trControl=trainControl(method='repeatedcv',number=5,repeats=3))\n",
    "rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Gradient Boosted Trees__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1900             nan     0.1000    0.0240\n",
      "     2        1.1524             nan     0.1000    0.0188\n",
      "     3        1.1223             nan     0.1000    0.0134\n",
      "     4        1.0979             nan     0.1000    0.0107\n",
      "     5        1.0805             nan     0.1000    0.0100\n",
      "     6        1.0660             nan     0.1000    0.0084\n",
      "     7        1.0502             nan     0.1000    0.0057\n",
      "     8        1.0384             nan     0.1000    0.0048\n",
      "     9        1.0306             nan     0.1000    0.0040\n",
      "    10        1.0231             nan     0.1000    0.0030\n",
      "    20        0.9884             nan     0.1000   -0.0001\n",
      "    40        0.9533             nan     0.1000    0.0004\n",
      "    60        0.9332             nan     0.1000   -0.0011\n",
      "    80        0.9158             nan     0.1000   -0.0008\n",
      "   100        0.9005             nan     0.1000   -0.0007\n",
      "   120        0.8894             nan     0.1000   -0.0005\n",
      "   140        0.8796             nan     0.1000   -0.0011\n",
      "   150        0.8755             nan     0.1000   -0.0013\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1903             nan     0.1000    0.0244\n",
      "     2        1.1514             nan     0.1000    0.0180\n",
      "     3        1.1186             nan     0.1000    0.0126\n",
      "     4        1.0934             nan     0.1000    0.0123\n",
      "     5        1.0690             nan     0.1000    0.0075\n",
      "     6        1.0513             nan     0.1000    0.0080\n",
      "     7        1.0362             nan     0.1000    0.0064\n",
      "     8        1.0239             nan     0.1000    0.0038\n",
      "     9        1.0135             nan     0.1000    0.0046\n",
      "    10        1.0036             nan     0.1000    0.0029\n",
      "    20        0.9519             nan     0.1000   -0.0047\n",
      "    40        0.8948             nan     0.1000   -0.0008\n",
      "    60        0.8594             nan     0.1000   -0.0019\n",
      "    80        0.8266             nan     0.1000   -0.0012\n",
      "   100        0.7993             nan     0.1000   -0.0020\n",
      "   120        0.7767             nan     0.1000   -0.0017\n",
      "   140        0.7600             nan     0.1000   -0.0016\n",
      "   150        0.7520             nan     0.1000   -0.0016\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1882             nan     0.1000    0.0274\n",
      "     2        1.1407             nan     0.1000    0.0209\n",
      "     3        1.1127             nan     0.1000    0.0125\n",
      "     4        1.0854             nan     0.1000    0.0123\n",
      "     5        1.0585             nan     0.1000    0.0100\n",
      "     6        1.0418             nan     0.1000    0.0068\n",
      "     7        1.0272             nan     0.1000    0.0023\n",
      "     8        1.0147             nan     0.1000    0.0041\n",
      "     9        1.0021             nan     0.1000   -0.0018\n",
      "    10        0.9885             nan     0.1000    0.0026\n",
      "    20        0.9312             nan     0.1000   -0.0024\n",
      "    40        0.8523             nan     0.1000    0.0001\n",
      "    60        0.8055             nan     0.1000   -0.0020\n",
      "    80        0.7603             nan     0.1000   -0.0023\n",
      "   100        0.7220             nan     0.1000   -0.0024\n",
      "   120        0.6899             nan     0.1000   -0.0031\n",
      "   140        0.6641             nan     0.1000   -0.0009\n",
      "   150        0.6491             nan     0.1000   -0.0017\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1841             nan     0.1000    0.0319\n",
      "     2        1.1349             nan     0.1000    0.0226\n",
      "     3        1.0949             nan     0.1000    0.0179\n",
      "     4        1.0648             nan     0.1000    0.0173\n",
      "     5        1.0408             nan     0.1000    0.0112\n",
      "     6        1.0192             nan     0.1000    0.0079\n",
      "     7        1.0036             nan     0.1000    0.0067\n",
      "     8        0.9921             nan     0.1000    0.0049\n",
      "     9        0.9777             nan     0.1000    0.0056\n",
      "    10        0.9693             nan     0.1000    0.0049\n",
      "    20        0.9247             nan     0.1000   -0.0002\n",
      "    40        0.8868             nan     0.1000   -0.0011\n",
      "    60        0.8690             nan     0.1000   -0.0017\n",
      "    80        0.8505             nan     0.1000   -0.0009\n",
      "   100        0.8379             nan     0.1000   -0.0008\n",
      "   120        0.8268             nan     0.1000   -0.0004\n",
      "   140        0.8155             nan     0.1000   -0.0007\n",
      "   150        0.8104             nan     0.1000   -0.0004\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1775             nan     0.1000    0.0320\n",
      "     2        1.1300             nan     0.1000    0.0230\n",
      "     3        1.0915             nan     0.1000    0.0175\n",
      "     4        1.0555             nan     0.1000    0.0135\n",
      "     5        1.0262             nan     0.1000    0.0099\n",
      "     6        1.0054             nan     0.1000    0.0099\n",
      "     7        0.9885             nan     0.1000    0.0051\n",
      "     8        0.9742             nan     0.1000    0.0077\n",
      "     9        0.9615             nan     0.1000    0.0062\n",
      "    10        0.9502             nan     0.1000    0.0032\n",
      "    20        0.8879             nan     0.1000    0.0005\n",
      "    40        0.8325             nan     0.1000   -0.0024\n",
      "    60        0.7945             nan     0.1000   -0.0048\n",
      "    80        0.7637             nan     0.1000   -0.0006\n",
      "   100        0.7424             nan     0.1000   -0.0025\n",
      "   120        0.7229             nan     0.1000   -0.0024\n",
      "   140        0.7015             nan     0.1000   -0.0027\n",
      "   150        0.6912             nan     0.1000   -0.0011\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1731             nan     0.1000    0.0300\n",
      "     2        1.1232             nan     0.1000    0.0207\n",
      "     3        1.0804             nan     0.1000    0.0156\n",
      "     4        1.0503             nan     0.1000    0.0129\n",
      "     5        1.0236             nan     0.1000    0.0123\n",
      "     6        0.9999             nan     0.1000    0.0102\n",
      "     7        0.9810             nan     0.1000    0.0030\n",
      "     8        0.9647             nan     0.1000    0.0058\n",
      "     9        0.9484             nan     0.1000    0.0051\n",
      "    10        0.9330             nan     0.1000    0.0043\n",
      "    20        0.8571             nan     0.1000   -0.0015\n",
      "    40        0.7862             nan     0.1000   -0.0026\n",
      "    60        0.7343             nan     0.1000   -0.0035\n",
      "    80        0.6896             nan     0.1000   -0.0027\n",
      "   100        0.6573             nan     0.1000   -0.0024\n",
      "   120        0.6276             nan     0.1000   -0.0010\n",
      "   140        0.5977             nan     0.1000   -0.0029\n",
      "   150        0.5849             nan     0.1000   -0.0020\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1878             nan     0.1000    0.0302\n",
      "     2        1.1463             nan     0.1000    0.0244\n",
      "     3        1.1080             nan     0.1000    0.0195\n",
      "     4        1.0776             nan     0.1000    0.0142\n",
      "     5        1.0545             nan     0.1000    0.0095\n",
      "     6        1.0347             nan     0.1000    0.0100\n",
      "     7        1.0166             nan     0.1000    0.0084\n",
      "     8        1.0046             nan     0.1000    0.0060\n",
      "     9        0.9925             nan     0.1000    0.0053\n",
      "    10        0.9795             nan     0.1000    0.0027\n",
      "    20        0.9433             nan     0.1000   -0.0013\n",
      "    40        0.9073             nan     0.1000   -0.0008\n",
      "    60        0.8835             nan     0.1000   -0.0009\n",
      "    80        0.8678             nan     0.1000   -0.0013\n",
      "   100        0.8526             nan     0.1000   -0.0018\n",
      "   120        0.8397             nan     0.1000   -0.0007\n",
      "   140        0.8296             nan     0.1000    0.0000\n",
      "   150        0.8245             nan     0.1000   -0.0003\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1782             nan     0.1000    0.0327\n",
      "     2        1.1296             nan     0.1000    0.0249\n",
      "     3        1.0925             nan     0.1000    0.0158\n",
      "     4        1.0617             nan     0.1000    0.0114\n",
      "     5        1.0348             nan     0.1000    0.0114\n",
      "     6        1.0156             nan     0.1000    0.0109\n",
      "     7        0.9962             nan     0.1000    0.0087\n",
      "     8        0.9813             nan     0.1000    0.0068\n",
      "     9        0.9670             nan     0.1000    0.0050\n",
      "    10        0.9568             nan     0.1000    0.0039\n",
      "    20        0.9040             nan     0.1000    0.0007\n",
      "    40        0.8491             nan     0.1000   -0.0011\n",
      "    60        0.8145             nan     0.1000   -0.0025\n",
      "    80        0.7861             nan     0.1000   -0.0022\n",
      "   100        0.7615             nan     0.1000   -0.0026\n",
      "   120        0.7407             nan     0.1000   -0.0014\n",
      "   140        0.7260             nan     0.1000   -0.0030\n",
      "   150        0.7164             nan     0.1000   -0.0022\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1806             nan     0.1000    0.0277\n",
      "     2        1.1358             nan     0.1000    0.0205\n",
      "     3        1.0979             nan     0.1000    0.0186\n",
      "     4        1.0622             nan     0.1000    0.0151\n",
      "     5        1.0348             nan     0.1000    0.0108\n",
      "     6        1.0124             nan     0.1000    0.0090\n",
      "     7        0.9936             nan     0.1000    0.0072\n",
      "     8        0.9749             nan     0.1000    0.0075\n",
      "     9        0.9587             nan     0.1000    0.0045\n",
      "    10        0.9461             nan     0.1000    0.0038\n",
      "    20        0.8718             nan     0.1000   -0.0009\n",
      "    40        0.8030             nan     0.1000    0.0002\n",
      "    60        0.7564             nan     0.1000   -0.0059\n",
      "    80        0.7239             nan     0.1000   -0.0021\n",
      "   100        0.6914             nan     0.1000   -0.0022\n",
      "   120        0.6619             nan     0.1000   -0.0026\n",
      "   140        0.6339             nan     0.1000   -0.0020\n",
      "   150        0.6239             nan     0.1000   -0.0021\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1912             nan     0.1000    0.0233\n",
      "     2        1.1527             nan     0.1000    0.0199\n",
      "     3        1.1241             nan     0.1000    0.0140\n",
      "     4        1.0985             nan     0.1000    0.0118\n",
      "     5        1.0776             nan     0.1000    0.0127\n",
      "     6        1.0633             nan     0.1000    0.0063\n",
      "     7        1.0458             nan     0.1000    0.0075\n",
      "     8        1.0359             nan     0.1000    0.0052\n",
      "     9        1.0279             nan     0.1000    0.0040\n",
      "    10        1.0214             nan     0.1000    0.0032\n",
      "    20        0.9837             nan     0.1000   -0.0021\n",
      "    40        0.9497             nan     0.1000   -0.0005\n",
      "    60        0.9273             nan     0.1000   -0.0005\n",
      "    80        0.9094             nan     0.1000    0.0002\n",
      "   100        0.8971             nan     0.1000   -0.0005\n",
      "   120        0.8882             nan     0.1000   -0.0003\n",
      "   140        0.8766             nan     0.1000   -0.0011\n",
      "   150        0.8711             nan     0.1000   -0.0009\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1829             nan     0.1000    0.0282\n",
      "     2        1.1385             nan     0.1000    0.0171\n",
      "     3        1.1080             nan     0.1000    0.0165\n",
      "     4        1.0818             nan     0.1000    0.0103\n",
      "     5        1.0605             nan     0.1000    0.0076\n",
      "     6        1.0428             nan     0.1000    0.0063\n",
      "     7        1.0276             nan     0.1000    0.0067\n",
      "     8        1.0156             nan     0.1000    0.0050\n",
      "     9        1.0048             nan     0.1000    0.0037\n",
      "    10        0.9969             nan     0.1000    0.0031\n",
      "    20        0.9415             nan     0.1000   -0.0014\n",
      "    40        0.8884             nan     0.1000   -0.0002\n",
      "    60        0.8442             nan     0.1000   -0.0015\n",
      "    80        0.8147             nan     0.1000   -0.0018\n",
      "   100        0.7854             nan     0.1000   -0.0017\n",
      "   120        0.7580             nan     0.1000   -0.0013\n",
      "   140        0.7362             nan     0.1000   -0.0021\n",
      "   150        0.7287             nan     0.1000   -0.0019\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1870             nan     0.1000    0.0284\n",
      "     2        1.1396             nan     0.1000    0.0202\n",
      "     3        1.1067             nan     0.1000    0.0120\n",
      "     4        1.0744             nan     0.1000    0.0106\n",
      "     5        1.0551             nan     0.1000    0.0083\n",
      "     6        1.0361             nan     0.1000    0.0068\n",
      "     7        1.0147             nan     0.1000    0.0072\n",
      "     8        0.9971             nan     0.1000    0.0054\n",
      "     9        0.9855             nan     0.1000    0.0044\n",
      "    10        0.9730             nan     0.1000    0.0025\n",
      "    20        0.9144             nan     0.1000   -0.0010\n",
      "    40        0.8357             nan     0.1000   -0.0023\n",
      "    60        0.7877             nan     0.1000   -0.0018\n",
      "    80        0.7413             nan     0.1000   -0.0015\n",
      "   100        0.7068             nan     0.1000   -0.0023\n",
      "   120        0.6792             nan     0.1000   -0.0025\n",
      "   140        0.6480             nan     0.1000   -0.0022\n",
      "   150        0.6350             nan     0.1000   -0.0018\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1862             nan     0.1000    0.0269\n",
      "     2        1.1456             nan     0.1000    0.0183\n",
      "     3        1.1124             nan     0.1000    0.0141\n",
      "     4        1.0897             nan     0.1000    0.0109\n",
      "     5        1.0705             nan     0.1000    0.0101\n",
      "     6        1.0534             nan     0.1000    0.0076\n",
      "     7        1.0395             nan     0.1000    0.0051\n",
      "     8        1.0259             nan     0.1000    0.0059\n",
      "     9        1.0172             nan     0.1000    0.0045\n",
      "    10        1.0094             nan     0.1000    0.0042\n",
      "    20        0.9663             nan     0.1000   -0.0015\n",
      "    40        0.9354             nan     0.1000    0.0004\n",
      "    60        0.9147             nan     0.1000   -0.0021\n",
      "    80        0.9007             nan     0.1000   -0.0018\n",
      "   100        0.8808             nan     0.1000   -0.0005\n",
      "   120        0.8679             nan     0.1000   -0.0000\n",
      "   140        0.8558             nan     0.1000   -0.0007\n",
      "   150        0.8485             nan     0.1000   -0.0005\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1824             nan     0.1000    0.0284\n",
      "     2        1.1406             nan     0.1000    0.0184\n",
      "     3        1.1051             nan     0.1000    0.0172\n",
      "     4        1.0767             nan     0.1000    0.0131\n",
      "     5        1.0547             nan     0.1000    0.0094\n",
      "     6        1.0343             nan     0.1000    0.0064\n",
      "     7        1.0222             nan     0.1000    0.0051\n",
      "     8        1.0078             nan     0.1000    0.0042\n",
      "     9        0.9957             nan     0.1000    0.0046\n",
      "    10        0.9868             nan     0.1000    0.0020\n",
      "    20        0.9301             nan     0.1000    0.0012\n",
      "    40        0.8764             nan     0.1000   -0.0019\n",
      "    60        0.8340             nan     0.1000   -0.0031\n",
      "    80        0.7920             nan     0.1000   -0.0021\n",
      "   100        0.7610             nan     0.1000   -0.0016\n",
      "   120        0.7435             nan     0.1000   -0.0014\n",
      "   140        0.7241             nan     0.1000   -0.0028\n",
      "   150        0.7156             nan     0.1000   -0.0011\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1786             nan     0.1000    0.0289\n",
      "     2        1.1318             nan     0.1000    0.0208\n",
      "     3        1.0971             nan     0.1000    0.0160\n",
      "     4        1.0719             nan     0.1000    0.0114\n",
      "     5        1.0451             nan     0.1000    0.0110\n",
      "     6        1.0262             nan     0.1000    0.0068\n",
      "     7        1.0079             nan     0.1000    0.0085\n",
      "     8        0.9919             nan     0.1000    0.0058\n",
      "     9        0.9805             nan     0.1000    0.0039\n",
      "    10        0.9700             nan     0.1000    0.0040\n",
      "    20        0.8982             nan     0.1000   -0.0002\n",
      "    40        0.8295             nan     0.1000   -0.0029\n",
      "    60        0.7764             nan     0.1000   -0.0009\n",
      "    80        0.7293             nan     0.1000   -0.0035\n",
      "   100        0.6964             nan     0.1000   -0.0015\n",
      "   120        0.6626             nan     0.1000   -0.0007\n",
      "   140        0.6348             nan     0.1000   -0.0034\n",
      "   150        0.6181             nan     0.1000   -0.0014\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1838             nan     0.1000    0.0275\n",
      "     2        1.1414             nan     0.1000    0.0194\n",
      "     3        1.1068             nan     0.1000    0.0160\n",
      "     4        1.0778             nan     0.1000    0.0111\n",
      "     5        1.0590             nan     0.1000    0.0106\n",
      "     6        1.0401             nan     0.1000    0.0080\n",
      "     7        1.0224             nan     0.1000    0.0074\n",
      "     8        1.0093             nan     0.1000    0.0056\n",
      "     9        0.9983             nan     0.1000    0.0028\n",
      "    10        0.9904             nan     0.1000    0.0029\n",
      "    20        0.9428             nan     0.1000    0.0018\n",
      "    40        0.9038             nan     0.1000   -0.0014\n",
      "    60        0.8853             nan     0.1000   -0.0005\n",
      "    80        0.8697             nan     0.1000   -0.0003\n",
      "   100        0.8551             nan     0.1000   -0.0008\n",
      "   120        0.8429             nan     0.1000   -0.0017\n",
      "   140        0.8343             nan     0.1000   -0.0024\n",
      "   150        0.8301             nan     0.1000   -0.0009\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1782             nan     0.1000    0.0304\n",
      "     2        1.1315             nan     0.1000    0.0213\n",
      "     3        1.0912             nan     0.1000    0.0188\n",
      "     4        1.0614             nan     0.1000    0.0135\n",
      "     5        1.0371             nan     0.1000    0.0097\n",
      "     6        1.0186             nan     0.1000    0.0075\n",
      "     7        1.0012             nan     0.1000    0.0079\n",
      "     8        0.9860             nan     0.1000    0.0055\n",
      "     9        0.9711             nan     0.1000    0.0057\n",
      "    10        0.9603             nan     0.1000    0.0019\n",
      "    20        0.9011             nan     0.1000    0.0015\n",
      "    40        0.8538             nan     0.1000   -0.0008\n",
      "    60        0.8173             nan     0.1000   -0.0009\n",
      "    80        0.7877             nan     0.1000   -0.0011\n",
      "   100        0.7612             nan     0.1000   -0.0004\n",
      "   120        0.7401             nan     0.1000   -0.0017\n",
      "   140        0.7131             nan     0.1000   -0.0034\n",
      "   150        0.7059             nan     0.1000   -0.0028\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1812             nan     0.1000    0.0301\n",
      "     2        1.1305             nan     0.1000    0.0255\n",
      "     3        1.0934             nan     0.1000    0.0199\n",
      "     4        1.0591             nan     0.1000    0.0133\n",
      "     5        1.0335             nan     0.1000    0.0125\n",
      "     6        1.0113             nan     0.1000    0.0092\n",
      "     7        0.9912             nan     0.1000    0.0071\n",
      "     8        0.9733             nan     0.1000    0.0055\n",
      "     9        0.9578             nan     0.1000    0.0048\n",
      "    10        0.9448             nan     0.1000    0.0037\n",
      "    20        0.8663             nan     0.1000   -0.0008\n",
      "    40        0.7962             nan     0.1000   -0.0039\n",
      "    60        0.7369             nan     0.1000   -0.0009\n",
      "    80        0.6982             nan     0.1000   -0.0013\n",
      "   100        0.6682             nan     0.1000   -0.0028\n",
      "   120        0.6371             nan     0.1000   -0.0015\n",
      "   140        0.6110             nan     0.1000   -0.0017\n",
      "   150        0.6017             nan     0.1000   -0.0022\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1915             nan     0.1000    0.0274\n",
      "     2        1.1533             nan     0.1000    0.0188\n",
      "     3        1.1226             nan     0.1000    0.0148\n",
      "     4        1.0970             nan     0.1000    0.0120\n",
      "     5        1.0756             nan     0.1000    0.0105\n",
      "     6        1.0609             nan     0.1000    0.0071\n",
      "     7        1.0481             nan     0.1000    0.0070\n",
      "     8        1.0337             nan     0.1000    0.0056\n",
      "     9        1.0247             nan     0.1000    0.0046\n",
      "    10        1.0169             nan     0.1000    0.0037\n",
      "    20        0.9766             nan     0.1000   -0.0007\n",
      "    40        0.9422             nan     0.1000   -0.0019\n",
      "    60        0.9241             nan     0.1000   -0.0006\n",
      "    80        0.9111             nan     0.1000   -0.0011\n",
      "   100        0.8996             nan     0.1000   -0.0008\n",
      "   120        0.8896             nan     0.1000   -0.0013\n",
      "   140        0.8809             nan     0.1000   -0.0017\n",
      "   150        0.8772             nan     0.1000   -0.0016\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1896             nan     0.1000    0.0282\n",
      "     2        1.1459             nan     0.1000    0.0194\n",
      "     3        1.1120             nan     0.1000    0.0139\n",
      "     4        1.0842             nan     0.1000    0.0102\n",
      "     5        1.0617             nan     0.1000    0.0086\n",
      "     6        1.0446             nan     0.1000    0.0080\n",
      "     7        1.0281             nan     0.1000    0.0079\n",
      "     8        1.0160             nan     0.1000    0.0057\n",
      "     9        1.0058             nan     0.1000    0.0034\n",
      "    10        0.9930             nan     0.1000    0.0052\n",
      "    20        0.9426             nan     0.1000    0.0003\n",
      "    40        0.8969             nan     0.1000   -0.0014\n",
      "    60        0.8624             nan     0.1000   -0.0017\n",
      "    80        0.8255             nan     0.1000   -0.0009\n",
      "   100        0.8002             nan     0.1000   -0.0004\n",
      "   120        0.7768             nan     0.1000   -0.0014\n",
      "   140        0.7603             nan     0.1000   -0.0017\n",
      "   150        0.7513             nan     0.1000   -0.0029\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1826             nan     0.1000    0.0251\n",
      "     2        1.1400             nan     0.1000    0.0211\n",
      "     3        1.0975             nan     0.1000    0.0142\n",
      "     4        1.0720             nan     0.1000    0.0100\n",
      "     5        1.0490             nan     0.1000    0.0041\n",
      "     6        1.0297             nan     0.1000    0.0061\n",
      "     7        1.0099             nan     0.1000    0.0076\n",
      "     8        0.9952             nan     0.1000    0.0040\n",
      "     9        0.9827             nan     0.1000    0.0015\n",
      "    10        0.9689             nan     0.1000    0.0046\n",
      "    20        0.9082             nan     0.1000    0.0007\n",
      "    40        0.8491             nan     0.1000   -0.0012\n",
      "    60        0.7940             nan     0.1000   -0.0014\n",
      "    80        0.7594             nan     0.1000   -0.0025\n",
      "   100        0.7204             nan     0.1000   -0.0023\n",
      "   120        0.6949             nan     0.1000   -0.0031\n",
      "   140        0.6694             nan     0.1000   -0.0013\n",
      "   150        0.6622             nan     0.1000   -0.0016\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1844             nan     0.1000    0.0264\n",
      "     2        1.1418             nan     0.1000    0.0200\n",
      "     3        1.1107             nan     0.1000    0.0167\n",
      "     4        1.0850             nan     0.1000    0.0120\n",
      "     5        1.0611             nan     0.1000    0.0089\n",
      "     6        1.0462             nan     0.1000    0.0095\n",
      "     7        1.0360             nan     0.1000    0.0040\n",
      "     8        1.0227             nan     0.1000    0.0073\n",
      "     9        1.0130             nan     0.1000    0.0055\n",
      "    10        1.0064             nan     0.1000    0.0038\n",
      "    20        0.9652             nan     0.1000    0.0009\n",
      "    40        0.9304             nan     0.1000    0.0001\n",
      "    60        0.9100             nan     0.1000   -0.0011\n",
      "    80        0.8928             nan     0.1000   -0.0014\n",
      "   100        0.8780             nan     0.1000   -0.0006\n",
      "   120        0.8645             nan     0.1000   -0.0015\n",
      "   140        0.8563             nan     0.1000   -0.0011\n",
      "   150        0.8513             nan     0.1000   -0.0001\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1865             nan     0.1000    0.0267\n",
      "     2        1.1419             nan     0.1000    0.0144\n",
      "     3        1.1080             nan     0.1000    0.0163\n",
      "     4        1.0795             nan     0.1000    0.0111\n",
      "     5        1.0583             nan     0.1000    0.0087\n",
      "     6        1.0416             nan     0.1000    0.0077\n",
      "     7        1.0247             nan     0.1000    0.0080\n",
      "     8        1.0116             nan     0.1000    0.0032\n",
      "     9        1.0035             nan     0.1000    0.0028\n",
      "    10        0.9930             nan     0.1000    0.0037\n",
      "    20        0.9349             nan     0.1000    0.0001\n",
      "    40        0.8824             nan     0.1000   -0.0009\n",
      "    60        0.8455             nan     0.1000   -0.0014\n",
      "    80        0.8082             nan     0.1000   -0.0031\n",
      "   100        0.7862             nan     0.1000   -0.0020\n",
      "   120        0.7599             nan     0.1000   -0.0013\n",
      "   140        0.7379             nan     0.1000   -0.0051\n",
      "   150        0.7295             nan     0.1000   -0.0016\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1854             nan     0.1000    0.0232\n",
      "     2        1.1364             nan     0.1000    0.0165\n",
      "     3        1.0971             nan     0.1000    0.0154\n",
      "     4        1.0669             nan     0.1000    0.0112\n",
      "     5        1.0412             nan     0.1000    0.0089\n",
      "     6        1.0219             nan     0.1000    0.0065\n",
      "     7        1.0027             nan     0.1000    0.0067\n",
      "     8        0.9840             nan     0.1000    0.0063\n",
      "     9        0.9692             nan     0.1000    0.0042\n",
      "    10        0.9584             nan     0.1000    0.0029\n",
      "    20        0.8924             nan     0.1000   -0.0026\n",
      "    40        0.8253             nan     0.1000   -0.0041\n",
      "    60        0.7683             nan     0.1000   -0.0006\n",
      "    80        0.7286             nan     0.1000   -0.0014\n",
      "   100        0.6868             nan     0.1000   -0.0020\n",
      "   120        0.6494             nan     0.1000   -0.0016\n",
      "   140        0.6234             nan     0.1000   -0.0030\n",
      "   150        0.6113             nan     0.1000   -0.0015\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1809             nan     0.1000    0.0260\n",
      "     2        1.1407             nan     0.1000    0.0252\n",
      "     3        1.1035             nan     0.1000    0.0170\n",
      "     4        1.0749             nan     0.1000    0.0124\n",
      "     5        1.0509             nan     0.1000    0.0127\n",
      "     6        1.0339             nan     0.1000    0.0092\n",
      "     7        1.0201             nan     0.1000    0.0074\n",
      "     8        1.0056             nan     0.1000    0.0055\n",
      "     9        0.9946             nan     0.1000    0.0047\n",
      "    10        0.9857             nan     0.1000    0.0037\n",
      "    20        0.9421             nan     0.1000   -0.0001\n",
      "    40        0.9127             nan     0.1000   -0.0005\n",
      "    60        0.8931             nan     0.1000   -0.0005\n",
      "    80        0.8712             nan     0.1000   -0.0005\n",
      "   100        0.8511             nan     0.1000   -0.0000\n",
      "   120        0.8406             nan     0.1000   -0.0012\n",
      "   140        0.8304             nan     0.1000   -0.0011\n",
      "   150        0.8242             nan     0.1000   -0.0020\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1793             nan     0.1000    0.0286\n",
      "     2        1.1393             nan     0.1000    0.0201\n",
      "     3        1.0993             nan     0.1000    0.0175\n",
      "     4        1.0693             nan     0.1000    0.0112\n",
      "     5        1.0456             nan     0.1000    0.0099\n",
      "     6        1.0264             nan     0.1000    0.0089\n",
      "     7        1.0079             nan     0.1000    0.0071\n",
      "     8        0.9910             nan     0.1000    0.0047\n",
      "     9        0.9814             nan     0.1000    0.0032\n",
      "    10        0.9703             nan     0.1000    0.0042\n",
      "    20        0.9108             nan     0.1000   -0.0021\n",
      "    40        0.8482             nan     0.1000   -0.0022\n",
      "    60        0.8061             nan     0.1000   -0.0015\n",
      "    80        0.7718             nan     0.1000   -0.0015\n",
      "   100        0.7484             nan     0.1000   -0.0005\n",
      "   120        0.7208             nan     0.1000   -0.0019\n",
      "   140        0.7001             nan     0.1000   -0.0004\n",
      "   150        0.6880             nan     0.1000   -0.0019\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1793             nan     0.1000    0.0297\n",
      "     2        1.1318             nan     0.1000    0.0209\n",
      "     3        1.0886             nan     0.1000    0.0190\n",
      "     4        1.0560             nan     0.1000    0.0116\n",
      "     5        1.0327             nan     0.1000    0.0101\n",
      "     6        1.0124             nan     0.1000    0.0071\n",
      "     7        0.9957             nan     0.1000    0.0075\n",
      "     8        0.9798             nan     0.1000    0.0041\n",
      "     9        0.9676             nan     0.1000    0.0035\n",
      "    10        0.9520             nan     0.1000    0.0035\n",
      "    20        0.8833             nan     0.1000    0.0008\n",
      "    40        0.8075             nan     0.1000   -0.0026\n",
      "    60        0.7558             nan     0.1000   -0.0007\n",
      "    80        0.7143             nan     0.1000   -0.0034\n",
      "   100        0.6708             nan     0.1000   -0.0012\n",
      "   120        0.6379             nan     0.1000   -0.0026\n",
      "   140        0.6107             nan     0.1000   -0.0030\n",
      "   150        0.6006             nan     0.1000   -0.0026\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1877             nan     0.1000    0.0269\n",
      "     2        1.1487             nan     0.1000    0.0196\n",
      "     3        1.1171             nan     0.1000    0.0131\n",
      "     4        1.0927             nan     0.1000    0.0121\n",
      "     5        1.0752             nan     0.1000    0.0100\n",
      "     6        1.0568             nan     0.1000    0.0055\n",
      "     7        1.0443             nan     0.1000    0.0067\n",
      "     8        1.0389             nan     0.1000   -0.0006\n",
      "     9        1.0280             nan     0.1000    0.0053\n",
      "    10        1.0193             nan     0.1000    0.0044\n",
      "    20        0.9735             nan     0.1000    0.0005\n",
      "    40        0.9378             nan     0.1000    0.0004\n",
      "    60        0.9161             nan     0.1000   -0.0013\n",
      "    80        0.9013             nan     0.1000   -0.0007\n",
      "   100        0.8864             nan     0.1000   -0.0016\n",
      "   120        0.8756             nan     0.1000   -0.0019\n",
      "   140        0.8641             nan     0.1000   -0.0012\n",
      "   150        0.8569             nan     0.1000   -0.0012\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1860             nan     0.1000    0.0260\n",
      "     2        1.1400             nan     0.1000    0.0197\n",
      "     3        1.1097             nan     0.1000    0.0151\n",
      "     4        1.0800             nan     0.1000    0.0101\n",
      "     5        1.0575             nan     0.1000    0.0090\n",
      "     6        1.0392             nan     0.1000    0.0066\n",
      "     7        1.0240             nan     0.1000    0.0054\n",
      "     8        1.0122             nan     0.1000    0.0026\n",
      "     9        1.0034             nan     0.1000    0.0034\n",
      "    10        0.9928             nan     0.1000    0.0038\n",
      "    20        0.9443             nan     0.1000   -0.0004\n",
      "    40        0.8871             nan     0.1000    0.0010\n",
      "    60        0.8499             nan     0.1000   -0.0006\n",
      "    80        0.8118             nan     0.1000   -0.0014\n",
      "   100        0.7846             nan     0.1000   -0.0014\n",
      "   120        0.7583             nan     0.1000   -0.0016\n",
      "   140        0.7352             nan     0.1000   -0.0019\n",
      "   150        0.7252             nan     0.1000   -0.0025\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1819             nan     0.1000    0.0246\n",
      "     2        1.1445             nan     0.1000    0.0204\n",
      "     3        1.1057             nan     0.1000    0.0164\n",
      "     4        1.0734             nan     0.1000    0.0116\n",
      "     5        1.0502             nan     0.1000    0.0085\n",
      "     6        1.0301             nan     0.1000    0.0084\n",
      "     7        1.0110             nan     0.1000    0.0064\n",
      "     8        0.9976             nan     0.1000    0.0031\n",
      "     9        0.9825             nan     0.1000    0.0058\n",
      "    10        0.9709             nan     0.1000    0.0037\n",
      "    20        0.9055             nan     0.1000    0.0003\n",
      "    40        0.8301             nan     0.1000   -0.0006\n",
      "    60        0.7746             nan     0.1000   -0.0005\n",
      "    80        0.7273             nan     0.1000   -0.0020\n",
      "   100        0.6927             nan     0.1000   -0.0021\n",
      "   120        0.6636             nan     0.1000   -0.0025\n",
      "   140        0.6410             nan     0.1000   -0.0035\n",
      "   150        0.6289             nan     0.1000   -0.0008\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1901             nan     0.1000    0.0303\n",
      "     2        1.1486             nan     0.1000    0.0182\n",
      "     3        1.1154             nan     0.1000    0.0122\n",
      "     4        1.0899             nan     0.1000    0.0116\n",
      "     5        1.0708             nan     0.1000    0.0102\n",
      "     6        1.0521             nan     0.1000    0.0063\n",
      "     7        1.0411             nan     0.1000    0.0064\n",
      "     8        1.0299             nan     0.1000    0.0050\n",
      "     9        1.0205             nan     0.1000    0.0030\n",
      "    10        1.0135             nan     0.1000    0.0033\n",
      "    20        0.9770             nan     0.1000   -0.0009\n",
      "    40        0.9440             nan     0.1000    0.0004\n",
      "    60        0.9253             nan     0.1000   -0.0005\n",
      "    80        0.9068             nan     0.1000   -0.0007\n",
      "   100        0.8968             nan     0.1000   -0.0006\n",
      "   120        0.8843             nan     0.1000   -0.0012\n",
      "   140        0.8742             nan     0.1000   -0.0002\n",
      "   150        0.8704             nan     0.1000   -0.0013\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1836             nan     0.1000    0.0266\n",
      "     2        1.1443             nan     0.1000    0.0201\n",
      "     3        1.1153             nan     0.1000    0.0154\n",
      "     4        1.0896             nan     0.1000    0.0113\n",
      "     5        1.0701             nan     0.1000    0.0086\n",
      "     6        1.0521             nan     0.1000    0.0072\n",
      "     7        1.0374             nan     0.1000    0.0061\n",
      "     8        1.0272             nan     0.1000    0.0055\n",
      "     9        1.0139             nan     0.1000    0.0040\n",
      "    10        1.0007             nan     0.1000    0.0044\n",
      "    20        0.9511             nan     0.1000   -0.0021\n",
      "    40        0.9024             nan     0.1000   -0.0015\n",
      "    60        0.8595             nan     0.1000    0.0000\n",
      "    80        0.8249             nan     0.1000   -0.0012\n",
      "   100        0.7948             nan     0.1000   -0.0018\n",
      "   120        0.7729             nan     0.1000   -0.0017\n",
      "   140        0.7496             nan     0.1000   -0.0034\n",
      "   150        0.7364             nan     0.1000   -0.0018\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1874             nan     0.1000    0.0244\n",
      "     2        1.1425             nan     0.1000    0.0208\n",
      "     3        1.1113             nan     0.1000    0.0161\n",
      "     4        1.0843             nan     0.1000    0.0119\n",
      "     5        1.0591             nan     0.1000    0.0102\n",
      "     6        1.0370             nan     0.1000    0.0080\n",
      "     7        1.0225             nan     0.1000    0.0038\n",
      "     8        1.0053             nan     0.1000    0.0069\n",
      "     9        0.9981             nan     0.1000    0.0023\n",
      "    10        0.9840             nan     0.1000    0.0027\n",
      "    20        0.9127             nan     0.1000   -0.0002\n",
      "    40        0.8359             nan     0.1000   -0.0015\n",
      "    60        0.7823             nan     0.1000   -0.0014\n",
      "    80        0.7388             nan     0.1000   -0.0016\n",
      "   100        0.6973             nan     0.1000   -0.0034\n",
      "   120        0.6648             nan     0.1000   -0.0013\n",
      "   140        0.6377             nan     0.1000   -0.0015\n",
      "   150        0.6259             nan     0.1000   -0.0032\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1914             nan     0.1000    0.0232\n",
      "     2        1.1542             nan     0.1000    0.0188\n",
      "     3        1.1239             nan     0.1000    0.0156\n",
      "     4        1.1032             nan     0.1000    0.0125\n",
      "     5        1.0826             nan     0.1000    0.0125\n",
      "     6        1.0672             nan     0.1000    0.0077\n",
      "     7        1.0527             nan     0.1000    0.0075\n",
      "     8        1.0417             nan     0.1000    0.0042\n",
      "     9        1.0305             nan     0.1000    0.0051\n",
      "    10        1.0194             nan     0.1000    0.0032\n",
      "    20        0.9812             nan     0.1000    0.0009\n",
      "    40        0.9399             nan     0.1000   -0.0024\n",
      "    60        0.9140             nan     0.1000   -0.0014\n",
      "    80        0.8941             nan     0.1000   -0.0004\n",
      "   100        0.8760             nan     0.1000   -0.0001\n",
      "   120        0.8672             nan     0.1000   -0.0008\n",
      "   140        0.8564             nan     0.1000   -0.0010\n",
      "   150        0.8510             nan     0.1000   -0.0013\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1903             nan     0.1000    0.0249\n",
      "     2        1.1467             nan     0.1000    0.0233\n",
      "     3        1.1125             nan     0.1000    0.0153\n",
      "     4        1.0859             nan     0.1000    0.0101\n",
      "     5        1.0654             nan     0.1000    0.0106\n",
      "     6        1.0463             nan     0.1000    0.0078\n",
      "     7        1.0298             nan     0.1000    0.0069\n",
      "     8        1.0143             nan     0.1000    0.0015\n",
      "     9        1.0034             nan     0.1000    0.0049\n",
      "    10        0.9945             nan     0.1000    0.0033\n",
      "    20        0.9396             nan     0.1000    0.0004\n",
      "    40        0.8802             nan     0.1000    0.0001\n",
      "    60        0.8373             nan     0.1000    0.0004\n",
      "    80        0.8112             nan     0.1000   -0.0021\n",
      "   100        0.7772             nan     0.1000   -0.0009\n",
      "   120        0.7540             nan     0.1000   -0.0020\n",
      "   140        0.7328             nan     0.1000   -0.0033\n",
      "   150        0.7240             nan     0.1000   -0.0012\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1864             nan     0.1000    0.0278\n",
      "     2        1.1415             nan     0.1000    0.0199\n",
      "     3        1.1087             nan     0.1000    0.0135\n",
      "     4        1.0793             nan     0.1000    0.0136\n",
      "     5        1.0543             nan     0.1000    0.0077\n",
      "     6        1.0345             nan     0.1000    0.0065\n",
      "     7        1.0178             nan     0.1000    0.0066\n",
      "     8        1.0020             nan     0.1000    0.0038\n",
      "     9        0.9882             nan     0.1000    0.0046\n",
      "    10        0.9760             nan     0.1000    0.0052\n",
      "    20        0.9020             nan     0.1000   -0.0010\n",
      "    40        0.8297             nan     0.1000   -0.0014\n",
      "    60        0.7746             nan     0.1000   -0.0016\n",
      "    80        0.7334             nan     0.1000   -0.0043\n",
      "   100        0.6987             nan     0.1000   -0.0023\n",
      "   120        0.6699             nan     0.1000   -0.0034\n",
      "   140        0.6434             nan     0.1000   -0.0008\n",
      "   150        0.6272             nan     0.1000   -0.0002\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1845             nan     0.1000    0.0262\n",
      "     2        1.1402             nan     0.1000    0.0182\n",
      "     3        1.1074             nan     0.1000    0.0176\n",
      "     4        1.0814             nan     0.1000    0.0112\n",
      "     5        1.0618             nan     0.1000    0.0088\n",
      "     6        1.0458             nan     0.1000    0.0074\n",
      "     7        1.0290             nan     0.1000    0.0077\n",
      "     8        1.0177             nan     0.1000    0.0062\n",
      "     9        1.0080             nan     0.1000    0.0039\n",
      "    10        0.9999             nan     0.1000    0.0038\n",
      "    20        0.9643             nan     0.1000   -0.0009\n",
      "    40        0.9337             nan     0.1000   -0.0007\n",
      "    60        0.9120             nan     0.1000   -0.0019\n",
      "    80        0.9000             nan     0.1000   -0.0010\n",
      "   100        0.8857             nan     0.1000   -0.0010\n",
      "   120        0.8755             nan     0.1000   -0.0012\n",
      "   140        0.8625             nan     0.1000   -0.0017\n",
      "   150        0.8568             nan     0.1000   -0.0018\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1844             nan     0.1000    0.0287\n",
      "     2        1.1466             nan     0.1000    0.0199\n",
      "     3        1.1071             nan     0.1000    0.0146\n",
      "     4        1.0787             nan     0.1000    0.0133\n",
      "     5        1.0556             nan     0.1000    0.0057\n",
      "     6        1.0356             nan     0.1000    0.0089\n",
      "     7        1.0154             nan     0.1000    0.0074\n",
      "     8        1.0023             nan     0.1000    0.0061\n",
      "     9        0.9895             nan     0.1000    0.0042\n",
      "    10        0.9782             nan     0.1000    0.0043\n",
      "    20        0.9216             nan     0.1000    0.0011\n",
      "    40        0.8676             nan     0.1000   -0.0012\n",
      "    60        0.8331             nan     0.1000   -0.0006\n",
      "    80        0.8084             nan     0.1000   -0.0008\n",
      "   100        0.7825             nan     0.1000   -0.0032\n",
      "   120        0.7546             nan     0.1000   -0.0014\n",
      "   140        0.7331             nan     0.1000   -0.0010\n",
      "   150        0.7247             nan     0.1000   -0.0014\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1747             nan     0.1000    0.0261\n",
      "     2        1.1283             nan     0.1000    0.0207\n",
      "     3        1.0894             nan     0.1000    0.0160\n",
      "     4        1.0603             nan     0.1000    0.0103\n",
      "     5        1.0341             nan     0.1000    0.0100\n",
      "     6        1.0156             nan     0.1000    0.0051\n",
      "     7        0.9965             nan     0.1000    0.0075\n",
      "     8        0.9827             nan     0.1000    0.0048\n",
      "     9        0.9700             nan     0.1000    0.0031\n",
      "    10        0.9583             nan     0.1000    0.0032\n",
      "    20        0.8887             nan     0.1000   -0.0009\n",
      "    40        0.8135             nan     0.1000   -0.0038\n",
      "    60        0.7658             nan     0.1000   -0.0004\n",
      "    80        0.7294             nan     0.1000   -0.0031\n",
      "   100        0.6919             nan     0.1000   -0.0036\n",
      "   120        0.6593             nan     0.1000   -0.0016\n",
      "   140        0.6330             nan     0.1000   -0.0014\n",
      "   150        0.6187             nan     0.1000   -0.0005\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1805             nan     0.1000    0.0370\n",
      "     2        1.1333             nan     0.1000    0.0244\n",
      "     3        1.0979             nan     0.1000    0.0207\n",
      "     4        1.0701             nan     0.1000    0.0132\n",
      "     5        1.0446             nan     0.1000    0.0104\n",
      "     6        1.0248             nan     0.1000    0.0094\n",
      "     7        1.0087             nan     0.1000    0.0074\n",
      "     8        0.9937             nan     0.1000    0.0053\n",
      "     9        0.9823             nan     0.1000    0.0041\n",
      "    10        0.9731             nan     0.1000    0.0047\n",
      "    20        0.9281             nan     0.1000   -0.0007\n",
      "    40        0.8863             nan     0.1000    0.0004\n",
      "    60        0.8622             nan     0.1000   -0.0019\n",
      "    80        0.8456             nan     0.1000   -0.0010\n",
      "   100        0.8339             nan     0.1000   -0.0006\n",
      "   120        0.8227             nan     0.1000   -0.0012\n",
      "   140        0.8150             nan     0.1000   -0.0016\n",
      "   150        0.8088             nan     0.1000   -0.0012\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1758             nan     0.1000    0.0249\n",
      "     2        1.1249             nan     0.1000    0.0220\n",
      "     3        1.0873             nan     0.1000    0.0201\n",
      "     4        1.0546             nan     0.1000    0.0156\n",
      "     5        1.0316             nan     0.1000    0.0100\n",
      "     6        1.0101             nan     0.1000    0.0090\n",
      "     7        0.9934             nan     0.1000    0.0089\n",
      "     8        0.9783             nan     0.1000    0.0054\n",
      "     9        0.9623             nan     0.1000    0.0063\n",
      "    10        0.9495             nan     0.1000    0.0036\n",
      "    20        0.8870             nan     0.1000    0.0007\n",
      "    40        0.8260             nan     0.1000   -0.0009\n",
      "    60        0.7973             nan     0.1000   -0.0014\n",
      "    80        0.7704             nan     0.1000   -0.0007\n",
      "   100        0.7495             nan     0.1000   -0.0006\n",
      "   120        0.7307             nan     0.1000   -0.0022\n",
      "   140        0.7171             nan     0.1000   -0.0028\n",
      "   150        0.7066             nan     0.1000   -0.0015\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1819             nan     0.1000    0.0301\n",
      "     2        1.1310             nan     0.1000    0.0219\n",
      "     3        1.0879             nan     0.1000    0.0195\n",
      "     4        1.0540             nan     0.1000    0.0186\n",
      "     5        1.0264             nan     0.1000    0.0123\n",
      "     6        1.0009             nan     0.1000    0.0112\n",
      "     7        0.9840             nan     0.1000    0.0070\n",
      "     8        0.9658             nan     0.1000    0.0067\n",
      "     9        0.9533             nan     0.1000    0.0041\n",
      "    10        0.9401             nan     0.1000    0.0045\n",
      "    20        0.8637             nan     0.1000    0.0004\n",
      "    40        0.7937             nan     0.1000   -0.0029\n",
      "    60        0.7430             nan     0.1000   -0.0008\n",
      "    80        0.7022             nan     0.1000   -0.0020\n",
      "   100        0.6647             nan     0.1000   -0.0012\n",
      "   120        0.6317             nan     0.1000   -0.0011\n",
      "   140        0.6011             nan     0.1000   -0.0029\n",
      "   150        0.5887             nan     0.1000   -0.0014\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1842             nan     0.1000    0.0355\n",
      "     2        1.1439             nan     0.1000    0.0244\n",
      "     3        1.1083             nan     0.1000    0.0146\n",
      "     4        1.0839             nan     0.1000    0.0112\n",
      "     5        1.0610             nan     0.1000    0.0115\n",
      "     6        1.0414             nan     0.1000    0.0086\n",
      "     7        1.0273             nan     0.1000    0.0078\n",
      "     8        1.0184             nan     0.1000    0.0053\n",
      "     9        1.0064             nan     0.1000    0.0048\n",
      "    10        0.9968             nan     0.1000    0.0043\n",
      "    20        0.9567             nan     0.1000   -0.0003\n",
      "    40        0.9179             nan     0.1000    0.0000\n",
      "    60        0.8930             nan     0.1000   -0.0008\n",
      "    80        0.8730             nan     0.1000   -0.0030\n",
      "   100        0.8610             nan     0.1000   -0.0013\n",
      "   120        0.8497             nan     0.1000   -0.0012\n",
      "   140        0.8417             nan     0.1000   -0.0005\n",
      "   150        0.8355             nan     0.1000   -0.0008\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1805             nan     0.1000    0.0241\n",
      "     2        1.1315             nan     0.1000    0.0181\n",
      "     3        1.1038             nan     0.1000    0.0148\n",
      "     4        1.0763             nan     0.1000    0.0132\n",
      "     5        1.0490             nan     0.1000    0.0128\n",
      "     6        1.0336             nan     0.1000    0.0071\n",
      "     7        1.0143             nan     0.1000    0.0073\n",
      "     8        0.9997             nan     0.1000    0.0067\n",
      "     9        0.9866             nan     0.1000    0.0025\n",
      "    10        0.9773             nan     0.1000    0.0044\n",
      "    20        0.9134             nan     0.1000   -0.0001\n",
      "    40        0.8624             nan     0.1000   -0.0021\n",
      "    60        0.8273             nan     0.1000   -0.0023\n",
      "    80        0.7959             nan     0.1000   -0.0021\n",
      "   100        0.7671             nan     0.1000    0.0001\n",
      "   120        0.7452             nan     0.1000   -0.0009\n",
      "   140        0.7236             nan     0.1000   -0.0028\n",
      "   150        0.7184             nan     0.1000   -0.0028\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1840             nan     0.1000    0.0298\n",
      "     2        1.1342             nan     0.1000    0.0186\n",
      "     3        1.0936             nan     0.1000    0.0194\n",
      "     4        1.0609             nan     0.1000    0.0141\n",
      "     5        1.0349             nan     0.1000    0.0082\n",
      "     6        1.0173             nan     0.1000    0.0067\n",
      "     7        0.9960             nan     0.1000    0.0082\n",
      "     8        0.9812             nan     0.1000    0.0063\n",
      "     9        0.9649             nan     0.1000    0.0038\n",
      "    10        0.9505             nan     0.1000    0.0039\n",
      "    20        0.8749             nan     0.1000   -0.0005\n",
      "    40        0.7964             nan     0.1000    0.0008\n",
      "    60        0.7430             nan     0.1000   -0.0028\n",
      "    80        0.7019             nan     0.1000   -0.0014\n",
      "   100        0.6606             nan     0.1000   -0.0026\n",
      "   120        0.6271             nan     0.1000   -0.0017\n",
      "   140        0.6042             nan     0.1000   -0.0029\n",
      "   150        0.5954             nan     0.1000   -0.0019\n",
      "\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1857             nan     0.1000    0.0261\n",
      "     2        1.1434             nan     0.1000    0.0217\n",
      "     3        1.1117             nan     0.1000    0.0174\n",
      "     4        1.0852             nan     0.1000    0.0131\n",
      "     5        1.0662             nan     0.1000    0.0096\n",
      "     6        1.0480             nan     0.1000    0.0083\n",
      "     7        1.0323             nan     0.1000    0.0074\n",
      "     8        1.0210             nan     0.1000    0.0057\n",
      "     9        1.0110             nan     0.1000    0.0034\n",
      "    10        1.0011             nan     0.1000    0.0043\n",
      "    20        0.9607             nan     0.1000    0.0015\n",
      "    40        0.9275             nan     0.1000   -0.0006\n",
      "    60        0.9060             nan     0.1000   -0.0010\n",
      "    80        0.8958             nan     0.1000   -0.0016\n",
      "   100        0.8836             nan     0.1000   -0.0020\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Stochastic Gradient Boosting \n",
       "\n",
       "614 samples\n",
       " 14 predictor\n",
       "  2 classes: 'N', 'Y' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (5 fold, repeated 3 times) \n",
       "Summary of sample sizes: 492, 491, 490, 492, 491, 491, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  interaction.depth  n.trees  Accuracy   Kappa    \n",
       "  1                   50      0.8073558  0.4771215\n",
       "  1                  100      0.8095327  0.4832626\n",
       "  1                  150      0.8068401  0.4797073\n",
       "  2                   50      0.8089863  0.4807597\n",
       "  2                  100      0.8008250  0.4671640\n",
       "  2                  150      0.7861418  0.4408175\n",
       "  3                   50      0.8030021  0.4702420\n",
       "  3                  100      0.7942986  0.4607435\n",
       "  3                  150      0.7867016  0.4498793\n",
       "\n",
       "Tuning parameter 'shrinkage' was held constant at a value of 0.1\n",
       "\n",
       "Tuning parameter 'n.minobsinnode' was held constant at a value of 10\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final values used for the model were n.trees = 100, interaction.depth =\n",
       " 1, shrinkage = 0.1 and n.minobsinnode = 10."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gbm<-train(preprocessedX, df[,13],method='gbm',trControl=trainControl(method='repeatedcv',number=5,repeats=3))\n",
    "gbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance\n",
    "\n",
    "The best performing algorithms were linear discriminant analysis, linear support vector machine, random forest, and gradient boosted trees.\n",
    "\n",
    "First, the LDA model will be examined and improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Call:\n",
       "lda(x, y)\n",
       "\n",
       "Prior probabilities of groups:\n",
       "        N         Y \n",
       "0.3127036 0.6872964 \n",
       "\n",
       "Group means:\n",
       "  ApplicantIncome CoapplicantIncome LoanAmount Loan_Amount_Term     Male\n",
       "N       0.8914783         0.6417115   1.762903         5.276322 2.062734\n",
       "Y       0.8813279         0.5141451   1.685500         5.238056 2.099051\n",
       "   Marital Dependents.0 Dependents.1 Dependents.2 Graduate SelfEmployed\n",
       "N 1.234046     1.175798    0.4984103    0.3474781 1.763877    0.4167675\n",
       "Y 1.426332     1.177636    0.4157356    0.4806067 1.948982    0.3959870\n",
       "  CreditHistory     Rural     Urban\n",
       "N      1.540925 0.7901165 0.7642545\n",
       "Y      2.652009 0.5730909 0.6702384\n",
       "\n",
       "Coefficients of linear discriminants:\n",
       "                            LD1\n",
       "ApplicantIncome    0.0377631380\n",
       "CoapplicantIncome -0.1035320497\n",
       "LoanAmount        -0.0957850594\n",
       "Loan_Amount_Term  -0.0523607171\n",
       "Male              -0.0189169539\n",
       "Marital            0.2056612969\n",
       "Dependents.0       0.0099260027\n",
       "Dependents.1      -0.1077564446\n",
       "Dependents.2       0.0590280913\n",
       "Graduate           0.1027340830\n",
       "SelfEmployed      -0.0001677648\n",
       "CreditHistory      1.1712580027\n",
       "Rural             -0.2769010514\n",
       "Urban             -0.1909541960"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lda$finalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEUAAABNTU1oaGh8fHyM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\nSUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "par(mar=c(4,8,2,2))\n",
    "barplot(coef(lda$finalModel)[,1],horiz=TRUE,las=2,cex.names=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model coefficients are consistent with the observations made during preliminary data visualization. The credit history has the largest influence on the loan approval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction   N   Y\n",
       "         N  85  10\n",
       "         Y 107 412\n",
       "                                          \n",
       "               Accuracy : 0.8094          \n",
       "                 95% CI : (0.7761, 0.8398)\n",
       "    No Information Rate : 0.6873          \n",
       "    P-Value [Acc > NIR] : 6.062e-12       \n",
       "                                          \n",
       "                  Kappa : 0.4859          \n",
       " Mcnemar's Test P-Value : < 2.2e-16       \n",
       "                                          \n",
       "            Sensitivity : 0.9763          \n",
       "            Specificity : 0.4427          \n",
       "         Pos Pred Value : 0.7938          \n",
       "         Neg Pred Value : 0.8947          \n",
       "             Prevalence : 0.6873          \n",
       "         Detection Rate : 0.6710          \n",
       "   Detection Prevalence : 0.8453          \n",
       "      Balanced Accuracy : 0.7095          \n",
       "                                          \n",
       "       'Positive' Class : Y               \n",
       "                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictTrain <- predict(lda,preprocessedX)\n",
    "confusionMatrix(predictTrain, df[,13],positive='Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is similar to the accuracy from cross validation, therefore the model is not overfitted.\n",
    "\n",
    "Because the data is unbalanced, (most of the loans were approved), there are more false positives than false negatives. The model will tend to predict more loans being approved than actually are approved. However, the false positive rate is still much higher than the false negative rate, indicating there may be some other problem with the model.\n",
    "\n",
    "Normality is an assumption of LDA. The Box Cox transformation may be performed to improve the LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear Discriminant Analysis \n",
       "\n",
       "614 samples\n",
       " 14 predictor\n",
       "  2 classes: 'N', 'Y' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (5 fold, repeated 3 times) \n",
       "Summary of sample sizes: 491, 491, 491, 491, 492, 492, ... \n",
       "Resampling results:\n",
       "\n",
       "  Accuracy   Kappa    \n",
       "  0.8121314  0.4901704\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction   N   Y\n",
       "         N  86   9\n",
       "         Y 106 413\n",
       "                                          \n",
       "               Accuracy : 0.8127          \n",
       "                 95% CI : (0.7796, 0.8428)\n",
       "    No Information Rate : 0.6873          \n",
       "    P-Value [Acc > NIR] : 1.565e-12       \n",
       "                                          \n",
       "                  Kappa : 0.4947          \n",
       " Mcnemar's Test P-Value : < 2.2e-16       \n",
       "                                          \n",
       "            Sensitivity : 0.9787          \n",
       "            Specificity : 0.4479          \n",
       "         Pos Pred Value : 0.7958          \n",
       "         Neg Pred Value : 0.9053          \n",
       "             Prevalence : 0.6873          \n",
       "         Detection Rate : 0.6726          \n",
       "   Detection Prevalence : 0.8453          \n",
       "      Balanced Accuracy : 0.7133          \n",
       "                                          \n",
       "       'Positive' Class : Y               \n",
       "                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processparameters2 <-preProcess(X,method =c('scale','bagImpute','BoxCox'))\n",
    "preprocessedX2 <-predict(processparameters2,X)\n",
    "lda<-train(preprocessedX2, df[,13],method='lda',trControl=trainControl(method='repeatedcv',number=5,repeats=3))\n",
    "lda\n",
    "predictTrain <- predict(lda,preprocessedX2)\n",
    "confusionMatrix(predictTrain, df[,13],positive='Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the performance of the LDA model is comparable if not slightly better than that of the other models. Because of its simplicity and lower tendency to overfit, the LDA model will be chosen as the final model.\n",
    "\n",
    "Note that the number of false positives is still very high. This may be partially due to imbalance, as mentioned before. However, it is likely mostly due to the nature of the data itself because the results for kNN (which is not sensitive to class imbalance) are even worse than the LDA model, in terms of Cohen's kappa. This indicates that modeling based on neighbors leads to predictions that are not much more accurate compared to random assignment, which implies that samples that are close to each other in feature space may belong to a different class. It suggests the nature of loan approval is subjective, and that similar people may be approved or denied arbitrarily. Because there are more false positives, it suggests that some may have biases against certain groups of people that causes them to deny loans that otherwise would be approved in the majority of cases. Overall, since it seems the low specificity is not mainly due to imbalance, no oversampling or subsampling will be performed to balance the data, and the current LDA model will be used as is.\n",
    "\n",
    "The test data set is pre-processed and the prediction is generated below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     Loan_ID       Gender    Married   Dependents        Education  \n",
       " LP001015:  1   Female: 70   No :134   0   :200   Graduate    :283  \n",
       " LP001022:  1   Male  :286   Yes:233   1   : 58   Not Graduate: 84  \n",
       " LP001031:  1   NA's  : 11             2   : 59                     \n",
       " LP001035:  1                          3+  : 40                     \n",
       " LP001051:  1                          NA's: 10                     \n",
       " LP001054:  1                                                       \n",
       " (Other) :361                                                       \n",
       " Self_Employed ApplicantIncome CoapplicantIncome   LoanAmount   \n",
       " No  :307      Min.   :    0   Min.   :    0     Min.   : 28.0  \n",
       " Yes : 37      1st Qu.: 2864   1st Qu.:    0     1st Qu.:100.2  \n",
       " NA's: 23      Median : 3786   Median : 1025     Median :125.0  \n",
       "               Mean   : 4806   Mean   : 1570     Mean   :136.1  \n",
       "               3rd Qu.: 5060   3rd Qu.: 2430     3rd Qu.:158.0  \n",
       "               Max.   :72529   Max.   :24000     Max.   :550.0  \n",
       "                                                 NA's   :5      \n",
       " Loan_Amount_Term Credit_History   Property_Area\n",
       " Min.   :  6.0    0   : 59       Rural    :111  \n",
       " 1st Qu.:360.0    1   :279       Semiurban:116  \n",
       " Median :360.0    NA's: 29       Urban    :140  \n",
       " Mean   :342.5                                  \n",
       " 3rd Qu.:360.0                                  \n",
       " Max.   :480.0                                  \n",
       " NA's   :6                                      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$lvls):\n",
      "\"variable 'Loan_Status' is not a factor\"Warning message in predict.BoxCoxTrans(object$bc[[i]], newdata[, i]):\n",
      "\"newdata should have values > 0\""
     ]
    }
   ],
   "source": [
    "test <- read.csv('C:/Datasets/LoanTest.csv',na.strings='')\n",
    "test$Credit_History<-as.factor(test$Credit_History)\n",
    "summary(test)\n",
    "test$Loan_Status <- 0\n",
    "testX <- createdummy(test)\n",
    "preprocessedtestX <-predict(processparameters2,testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "results <- data.frame(test[,1],predict(lda,preprocessedtestX))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
